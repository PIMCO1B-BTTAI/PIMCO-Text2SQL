{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/virounikamina/Desktop/PIMCO-Text2SQL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import compare_csv\n",
    "import openai as OpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz.distance.Levenshtein import distance\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "import chat_prompt\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"/d/GithubRepos/PIMCO-Text2SQL\"))\n",
    "din_modules_path = os.path.join(current_dir, 'chatgpt_api')\n",
    "sys.path.append(din_modules_path)\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"OpenAI API key not configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Key Analysis:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Table: REGISTRANT\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 3591\n",
      "Unique values: 3591\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: REGISTRANT.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: FUND_REPORTED_INFO\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 3591\n",
      "Unique values: 3591\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Column: SERIES_ID\n",
      "Total rows: 3591\n",
      "Unique values: 3369\n",
      "Foreign Key: FUND_REPORTED_INFO.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: INTEREST_RATE_RISK\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 5350\n",
      "Unique values: 1576\n",
      "Column: INTEREST_RATE_RISK_ID\n",
      "Total rows: 5350\n",
      "Unique values: 5350\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: INTEREST_RATE_RISK.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: BORROWER\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 15258\n",
      "Unique values: 1549\n",
      "Column: BORROWER_ID\n",
      "Total rows: 15258\n",
      "Unique values: 15258\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: BORROWER.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: MONTHLY_TOTAL_RETURN\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 10024\n",
      "Unique values: 3591\n",
      "Column: MONTHLY_TOTAL_RETURN_ID\n",
      "Total rows: 10024\n",
      "Unique values: 10024\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Column: CLASS_ID\n",
      "Total rows: 10024\n",
      "Unique values: 9700\n",
      "Foreign Key: MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: MONTHLY_RETURN_CAT_INSTRUMENT\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 77164\n",
      "Unique values: 2724\n",
      "Foreign Key: MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: EXPLANATORY_NOTE\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 4225\n",
      "Unique values: 1876\n",
      "Column: EXPLANATORY_NOTE_ID\n",
      "Total rows: 4225\n",
      "Unique values: 4225\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: EXPLANATORY_NOTE.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: DEBT_SECURITY\n",
      "Column: HOLDING_ID\n",
      "Total rows: 3523\n",
      "Unique values: 3523\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: DEBT_SECURITY.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: CONVERTIBLE_SECURITY_CURRENCY\n",
      "Column: HOLDING_ID\n",
      "Total rows: 32\n",
      "Unique values: 32\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Column: CONVERTIBLE_SECURITY_ID\n",
      "Total rows: 32\n",
      "Unique values: 32\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: REPURCHASE_AGREEMENT\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: REPURCHASE_COUNTERPARTY\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: REPURCHASE_COUNTERPARTY_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: REPURCHASE_COLLATERAL\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: REPURCHASE_COLLATERAL_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: DERIVATIVE_COUNTERPARTY\n",
      "Column: HOLDING_ID\n",
      "Total rows: 13\n",
      "Unique values: 13\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Column: DERIVATIVE_COUNTERPARTY_ID\n",
      "Total rows: 13\n",
      "Unique values: 13\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: DERIVATIVE_COUNTERPARTY.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: DESC_REF_INDEX_BASKET\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: INDEX_IDENTIFIER\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: DESC_REF_INDEX_COMPONENT\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: DESC_REF_INDEX_COMPONENT_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: OTHER_IDENTIFIER\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: FUT_FWD_NONFOREIGNCUR_CONTRACT\n",
      "Column: HOLDING_ID\n",
      "Total rows: 6\n",
      "Unique values: 6\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: FWD_FOREIGNCUR_CONTRACT_SWAP\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: NONFOREIGN_EXCHANGE_SWAP\n",
      "Column: HOLDING_ID\n",
      "Total rows: 2\n",
      "Unique values: 2\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: FLOATING_RATE_RESET_TENOR\n",
      "Column: HOLDING_ID\n",
      "Total rows: 2\n",
      "Unique values: 2\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Column: RATE_RESET_TENOR_ID\n",
      "Total rows: 2\n",
      "Unique values: 2\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: FLOATING_RATE_RESET_TENOR.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: OTHER_DERIV\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: OTHER_DERIV_NOTIONAL_AMOUNT\n",
      "Column: HOLDING_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "Column: OTHER_DERIV_NOTIONAL_AMOUNT_ID\n",
      "Total rows: 0\n",
      "Unique values: 0\n",
      "\n",
      "Table: FUND_REPORTED_HOLDING\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 9055\n",
      "Unique values: 3591\n",
      "Column: HOLDING_ID\n",
      "Total rows: 9055\n",
      "Unique values: 9055\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: FUND_REPORTED_HOLDING.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "Foreign Key: FUND_REPORTED_HOLDING.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: SUBMISSION\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 3591\n",
      "Unique values: 3591\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: SUBMISSION.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: BORROW_AGGREGATE\n",
      "Column: ACCESSION_NUMBER\n",
      "Total rows: 488\n",
      "Unique values: 461\n",
      "Column: BORROW_AGGREGATE_ID\n",
      "Total rows: 488\n",
      "Unique values: 488\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: BORROW_AGGREGATE.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\n",
      "\n",
      "Table: IDENTIFIERS\n",
      "Column: HOLDING_ID\n",
      "Total rows: 12041\n",
      "Unique values: 9055\n",
      "Column: IDENTIFIERS_ID\n",
      "Total rows: 12041\n",
      "Unique values: 12041\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: IDENTIFIERS.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: DEBT_SECURITY_REF_INSTRUMENT\n",
      "Column: HOLDING_ID\n",
      "Total rows: 56\n",
      "Unique values: 32\n",
      "Column: DEBT_SECURITY_REF_ID\n",
      "Total rows: 56\n",
      "Unique values: 56\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: SWAPTION_OPTION_WARNT_DERIV\n",
      "Column: HOLDING_ID\n",
      "Total rows: 5\n",
      "Unique values: 5\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: DESC_REF_OTHER\n",
      "Column: HOLDING_ID\n",
      "Total rows: 31\n",
      "Unique values: 11\n",
      "Column: DESC_REF_OTHER_ID\n",
      "Total rows: 31\n",
      "Unique values: 31\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: DESC_REF_OTHER.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n",
      "\n",
      "Table: SECURITIES_LENDING\n",
      "Column: HOLDING_ID\n",
      "Total rows: 9055\n",
      "Unique values: 9055\n",
      ">>> Potential PRIMARY KEY <<<\n",
      "Foreign Key: SECURITIES_LENDING.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\n"
     ]
    }
   ],
   "source": [
    "def explore_keys():\n",
    "    \"\"\"Explore potential primary and foreign keys in the database\"\"\"\n",
    "    import sqlite3\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('sqlite/nport.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    print(\"Database Key Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Analyze each table\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "\n",
    "        # Get column info\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Get sample count for potential key columns\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            # Check if column name contains potential key indicators\n",
    "            if any(key_term in col_name.lower() for key_term in ['_id', 'accession', 'number']):\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) total_rows, \n",
    "                           COUNT(DISTINCT {col_name}) unique_values \n",
    "                    FROM {table_name}\n",
    "                    WHERE {col_name} IS NOT NULL\n",
    "                \"\"\")\n",
    "                stats = cursor.fetchone()\n",
    "                print(f\"Column: {col_name}\")\n",
    "                print(f\"Total rows: {stats[0]}\")\n",
    "                print(f\"Unique values: {stats[1]}\")\n",
    "                \n",
    "                # If unique values equals total rows, likely a key\n",
    "                if stats[0] == stats[1] and stats[0] > 0:\n",
    "                    print(\">>> Potential PRIMARY KEY <<<\")\n",
    "\n",
    "        # Look for foreign key relationships\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            if col_name == 'ACCESSION_NUMBER':\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM {table_name} t1\n",
    "                    WHERE EXISTS (\n",
    "                        SELECT 1 FROM FUND_REPORTED_INFO t2 \n",
    "                        WHERE t1.ACCESSION_NUMBER = t2.ACCESSION_NUMBER\n",
    "                    )\n",
    "                \"\"\")\n",
    "                match_count = cursor.fetchone()[0]\n",
    "                if match_count > 0:\n",
    "                    print(f\"Foreign Key: {table_name}.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\")\n",
    "            \n",
    "            elif col_name == 'HOLDING_ID':\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM {table_name} t1\n",
    "                    WHERE EXISTS (\n",
    "                        SELECT 1 FROM FUND_REPORTED_HOLDING t2 \n",
    "                        WHERE t1.HOLDING_ID = t2.HOLDING_ID\n",
    "                    )\n",
    "                \"\"\")\n",
    "                match_count = cursor.fetchone()[0]\n",
    "                if match_count > 0:\n",
    "                    print(f\"Foreign Key: {table_name}.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Run the analysis\n",
    "explore_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "1. \"List the top 5 registrants by total net assets, including their CIK and country.\"\n",
    "   SQL: \n",
    "   WITH FundAssets AS (\n",
    "       SELECT R.CIK, R.REGISTRANT_NAME, R.COUNTRY, F.NET_ASSETS\n",
    "       FROM REGISTRANT R\n",
    "       JOIN FUND_REPORTED_INFO F ON R.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT CIK, REGISTRANT_NAME, COUNTRY, NET_ASSETS\n",
    "   FROM FundAssets\n",
    "   ORDER BY NET_ASSETS DESC\n",
    "   LIMIT 5;\n",
    "\n",
    "2. \"Find all holdings with a fair value level of Level 1 and their corresponding fund names.\"\n",
    "   SQL: \n",
    "   WITH HoldingsCTE AS (\n",
    "       SELECT H.HOLDING_ID, H.ISSUER_NAME, H.FAIR_VALUE_LEVEL, F.SERIES_NAME\n",
    "       FROM FUND_REPORTED_HOLDING H\n",
    "       JOIN FUND_REPORTED_INFO F ON H.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "       WHERE H.FAIR_VALUE_LEVEL = 'Level 1'\n",
    "   )\n",
    "   SELECT HOLDING_ID, ISSUER_NAME, SERIES_NAME\n",
    "   FROM HoldingsCTE;\n",
    "\n",
    "3. \"Calculate the total collateral amount for repurchase agreements grouped by counterparty.\"\n",
    "   SQL: \n",
    "   WITH CollateralCTE AS (\n",
    "    SELECT RCP.NAME AS Counterparty_Name, SUM(RC.COLLATERAL_AMOUNT) AS Total_Collateral\n",
    "    FROM REPURCHASE_COLLATERAL RC\n",
    "    JOIN REPURCHASE_COUNTERPARTY RCP ON RC.HOLDING_ID = RCP.HOLDING_ID\n",
    "    GROUP BY RCP.NAME\n",
    "   )\n",
    "   SELECT Counterparty_Name, Total_Collateral\n",
    "   FROM CollateralCTE\n",
    "   ORDER BY Total_Collateral DESC;\n",
    "\n",
    "4. \"Locate funds that have both securities lending activities and repurchase agreements.\"\n",
    "   SQL: \n",
    "   WITH SecuritiesLending AS (\n",
    "       SELECT ACCESSION_NUMBER\n",
    "       FROM SECURITIES_LENDING\n",
    "       WHERE IS_LOAN_BY_FUND = 'Y'\n",
    "   ),\n",
    "   RepurchaseAgreements AS (\n",
    "       SELECT ACCESSION_NUMBER\n",
    "       FROM REPURCHASE_AGREEMENT\n",
    "   )\n",
    "   SELECT F.SERIES_NAME\n",
    "   FROM FUND_REPORTED_INFO F\n",
    "   WHERE F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM SecuritiesLending)\n",
    "     AND F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM RepurchaseAgreements);\n",
    "\n",
    "5. \"Find borrowers who have borrowed more than $5,000,000, including their names and LEIs.\"\n",
    "   SQL: \n",
    "   WITH BorrowedAmounts AS (\n",
    "       SELECT BORROWER_ID, SUM(AGGREGATE_VALUE) AS Total_Borrowed\n",
    "       FROM BORROWER\n",
    "       GROUP BY BORROWER_ID\n",
    "       HAVING SUM(AGGREGATE_VALUE) > 5000000\n",
    "   )\n",
    "   SELECT B.NAME, B.LEI, BA.Total_Borrowed\n",
    "   FROM BORROWER B\n",
    "   JOIN BorrowedAmounts BA ON B.BORROWER_ID = BA.BORROWER_ID;\n",
    "\n",
    "6. \"List all derivative counterparties along with the number of derivative instruments they are involved in.\"\n",
    "   SQL: \n",
    "   WITH CounterpartyCounts AS (\n",
    "       SELECT DC.DERIVATIVE_COUNTERPARTY_NAME, COUNT(*) AS Instrument_Count\n",
    "       FROM DERIVATIVE_COUNTERPARTY DC\n",
    "       JOIN FUND_REPORTED_HOLDING H ON DC.HOLDING_ID = H.HOLDING_ID\n",
    "       JOIN DEBT_SECURITY D ON H.HOLDING_ID = D.HOLDING_ID\n",
    "       GROUP BY DC.DERIVATIVE_COUNTERPARTY_NAME\n",
    "   )\n",
    "   SELECT DERIVATIVE_COUNTERPARTY_NAME, Instrument_Count\n",
    "   FROM CounterpartyCounts\n",
    "   ORDER BY Instrument_Count DESC;\n",
    "\n",
    "7. \"Compute the average annualized rate for debt securities grouped by coupon type.\"\n",
    "   SQL: \n",
    "   WITH RateAverages AS (\n",
    "       SELECT DS.COUPON_TYPE, AVG(DS.ANNUALIZED_RATE) AS Avg_Annualized_Rate\n",
    "       FROM DEBT_SECURITY DS\n",
    "       WHERE DS.ANNUALIZED_RATE IS NOT NULL\n",
    "       GROUP BY DS.COUPON_TYPE\n",
    "   )\n",
    "   SELECT COUPON_TYPE, Avg_Annualized_Rate\n",
    "   FROM RateAverages\n",
    "   ORDER BY Avg_Annualized_Rate DESC;\n",
    "\n",
    "8. \"Get funds that have experienced a net decrease in assets over the last three reporting periods.\"\n",
    "   SQL: \n",
    "   WITH AssetChanges AS (\n",
    "       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\n",
    "              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\n",
    "       FROM FUND_REPORTED_INFO F\n",
    "       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT DISTINCT AC.SERIES_NAME\n",
    "   FROM AssetChanges AC\n",
    "   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\n",
    "     AND AC.Previous_Period_Assets IS NOT NULL;\n",
    "\n",
    "9. \"Identify issuers with more than three different securities holdings, including their names and CUSIPs.\"\n",
    "   SQL: \n",
    "   WITH IssuerHoldings AS (\n",
    "       SELECT H.ISSUER_NAME, H.ISSUER_CUSIP, COUNT(DISTINCT H.HOLDING_ID) AS Holding_Count\n",
    "       FROM FUND_REPORTED_HOLDING H\n",
    "       GROUP BY H.ISSUER_NAME, H.ISSUER_CUSIP\n",
    "       HAVING COUNT(DISTINCT H.HOLDING_ID) > 3\n",
    "   )\n",
    "   SELECT ISSUER_NAME, ISSUER_CUSIP, Holding_Count\n",
    "   FROM IssuerHoldings\n",
    "   ORDER BY Holding_Count DESC;\n",
    "\n",
    "10. \"Calculate the total notional amount of derivatives per currency and identify the top 3 currencies by notional amount.\"\n",
    "    SQL: \n",
    "    WITH NotionalSums AS (\n",
    "        SELECT ODNA.CURRENCY_CODE, SUM(ODNA.NOTIONAL_AMOUNT) AS Total_Notional\n",
    "        FROM OTHER_DERIV_NOTIONAL_AMOUNT ODNA\n",
    "        GROUP BY ODNA.CURRENCY_CODE\n",
    "    )\n",
    "    SELECT CURRENCY_CODE, Total_Notional\n",
    "    FROM NotionalSums\n",
    "    ORDER BY Total_Notional DESC\n",
    "    LIMIT 3;\n",
    "\n",
    "11. \"List funds with liquidation preferences exceeding their net assets.\"\n",
    "    SQL: \n",
    "    WITH FundPreferences AS (\n",
    "        SELECT F.SERIES_NAME, F.LIQUIDATION_PREFERENCE, F.NET_ASSETS\n",
    "        FROM FUND_REPORTED_INFO F\n",
    "    )\n",
    "    SELECT SERIES_NAME, LIQUIDATION_PREFERENCE, NET_ASSETS\n",
    "    FROM FundPreferences\n",
    "    WHERE LIQUIDATION_PREFERENCE > NET_ASSETS;\n",
    "\n",
    "12. \"Find all convertible securities that are contingent and have a conversion ratio above 1.5.\"\n",
    "    SQL: \n",
    "    WITH ConvertibleCTE AS (\n",
    "        SELECT DS.HOLDING_ID, CSC.CONVERSION_RATIO\n",
    "        FROM DEBT_SECURITY DS\n",
    "        JOIN CONVERTIBLE_SECURITY_CURRENCY CSC ON DS.HOLDING_ID = CSC.HOLDING_ID\n",
    "        WHERE DS.IS_CONVTIBLE_CONTINGENT = 'Y' AND CSC.CONVERSION_RATIO > 1.5\n",
    "    )\n",
    "    SELECT HOLDING_ID, CONVERSION_RATIO\n",
    "    FROM ConvertibleCTE;\n",
    "\n",
    "13. \"Analyze the distribution of asset categories within the top 10 largest funds by total assets.\"\n",
    "    SQL: \n",
    "    WITH TopFunds AS (\n",
    "        SELECT SERIES_NAME, ACCESSION_NUMBER\n",
    "        FROM FUND_REPORTED_INFO\n",
    "        ORDER BY TOTAL_ASSETS DESC\n",
    "        LIMIT 10\n",
    "    ),\n",
    "    AssetDistribution AS (\n",
    "        SELECT H.ASSET_CAT, COUNT(*) AS Category_Count\n",
    "        FROM FUND_REPORTED_HOLDING H\n",
    "        JOIN TopFunds T ON H.ACCESSION_NUMBER = T.ACCESSION_NUMBER\n",
    "        GROUP BY H.ASSET_CAT\n",
    "    )\n",
    "    SELECT ASSET_CAT, Category_Count\n",
    "    FROM AssetDistribution\n",
    "    ORDER BY Category_Count DESC;\n",
    "    \n",
    "14. \"Find the top 10 funds with the highest average monthly returns in the past quarter.\"\n",
    "   SQL: \n",
    "   WITH AvgMonthlyReturn AS (\n",
    "       SELECT ACCESSION_NUMBER, \n",
    "              (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\n",
    "       FROM MONTHLY_TOTAL_RETURN\n",
    "   )\n",
    "   SELECT F.SERIES_NAME, A.ACCESSION_NUMBER, A.Avg_Return\n",
    "   FROM AvgMonthlyReturn A\n",
    "   JOIN FUND_REPORTED_INFO F ON A.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "   ORDER BY A.Avg_Return DESC\n",
    "   LIMIT 10;\n",
    "\n",
    "15. \"Compare the latest net asset values of the top 5 performing funds.\"\n",
    "    SQL: \n",
    "    WITH TopPerformingFunds AS (\n",
    "        SELECT \n",
    "            ACCESSION_NUMBER, \n",
    "            (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\n",
    "        FROM \n",
    "            MONTHLY_TOTAL_RETURN\n",
    "        ORDER BY \n",
    "            Avg_Return DESC\n",
    "        LIMIT 5\n",
    "    )\n",
    "    SELECT \n",
    "        FR.SERIES_NAME, \n",
    "        FR.NET_ASSETS, \n",
    "        TP.Avg_Return\n",
    "    FROM \n",
    "        TopPerformingFunds TP\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO FR ON TP.ACCESSION_NUMBER = FR.ACCESSION_NUMBER;\n",
    "\n",
    "16. \"Calculate the overall average return across all funds for the most recent month.\"\n",
    "    SQL: \n",
    "    WITH LatestReturns AS (\n",
    "        SELECT \n",
    "            M.ACCESSION_NUMBER, \n",
    "            M.MONTHLY_TOTAL_RETURN1\n",
    "        FROM \n",
    "            MONTHLY_TOTAL_RETURN M\n",
    "        JOIN \n",
    "            SUBMISSION S ON M.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "        WHERE \n",
    "            S.REPORT_DATE = (SELECT MAX(REPORT_DATE) FROM SUBMISSION)\n",
    "    )\n",
    "    SELECT \n",
    "        AVG(MONTHLY_TOTAL_RETURN1) AS Average_Return\n",
    "    FROM \n",
    "        LatestReturns;\n",
    "\n",
    "17. \"Find the interest rate risk for each fund and identify those with the highest risk scores.\"\n",
    "    SQL: \n",
    "    WITH InterestRiskScores AS (\n",
    "        SELECT \n",
    "            IR.ACCESSION_NUMBER, \n",
    "            -- Calculating composite risk score by summing absolute values of DV01 and DV100 columns\n",
    "            (ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV100 AS FLOAT))\n",
    "            ) AS Composite_Risk_Score\n",
    "        FROM \n",
    "            INTEREST_RATE_RISK IR\n",
    "    )\n",
    "    SELECT \n",
    "        FR.SERIES_NAME, \n",
    "        FR.ACCESSION_NUMBER, \n",
    "        IRS.Composite_Risk_Score\n",
    "    FROM \n",
    "        InterestRiskScores IRS\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO FR ON IRS.ACCESSION_NUMBER = FR.ACCESSION_NUMBER\n",
    "    ORDER BY \n",
    "        IRS.Composite_Risk_Score DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "18. \"Analyze the composition of fund portfolios by categorizing assets and their total values.\"\n",
    "    SQL: \n",
    "    WITH PortfolioComposition AS (\n",
    "    SELECT \n",
    "        ACCESSION_NUMBER, \n",
    "        ASSET_CAT, \n",
    "        SUM(CAST(CURRENCY_VALUE AS FLOAT)) AS Total_Value\n",
    "    FROM \n",
    "        FUND_REPORTED_HOLDING\n",
    "    GROUP BY \n",
    "        ACCESSION_NUMBER, \n",
    "        ASSET_CAT\n",
    "    )\n",
    "    SELECT \n",
    "        F.SERIES_NAME, \n",
    "        PC.ASSET_CAT, \n",
    "        PC.Total_Value\n",
    "    FROM \n",
    "        PortfolioComposition PC\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO F ON PC.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "    ORDER BY \n",
    "        F.SERIES_NAME, \n",
    "        PC.Total_Value DESC;\n",
    "\n",
    "19. \"Identify the most common asset categories across all fund portfolios.\"\n",
    "    SQL: \n",
    "    WITH AssetCounts AS (\n",
    "        SELECT ASSET_CAT, COUNT(*) AS Count\n",
    "        FROM FUND_REPORTED_HOLDING\n",
    "        GROUP BY ASSET_CAT\n",
    "    )\n",
    "    SELECT ASSET_CAT, Count\n",
    "    FROM AssetCounts\n",
    "    ORDER BY Count DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "20. \"Retrieve funds that have experienced a net decrease in assets over the last three reporting periods.\"\n",
    "   SQL: \n",
    "   WITH AssetChanges AS (\n",
    "       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\n",
    "              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\n",
    "       FROM FUND_REPORTED_Iimport os\n",
    "print(os.getcwd())NFO F\n",
    "       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT DISTINCT AC.SERIES_NAME\n",
    "   FROM AssetChanges AC\n",
    "   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\n",
    "     AND AC.Previous_Period_Assets IS NOT NULL;\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\"\\s*(.*?)\\s*\"\\s*SQL:\\s*(WITH.*?;)(?=\\n\\s*\\d+|$)'\n",
    "matches = re.findall(pattern, text, re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: ['WITH FundAssets AS (\\n       SELECT R.CIK, R.REGISTRANT_NAME, R.COUNTRY, F.NET_ASSETS\\n       FROM REGISTRANT R\\n       JOIN FUND_REPORTED_INFO F ON R.ACCESSION_NUMBER = F.ACCESSION_NUMBER\\n   )\\n   SELECT CIK, REGISTRANT_NAME, COUNTRY, NET_ASSETS\\n   FROM FundAssets\\n   ORDER BY NET_ASSETS DESC\\n   LIMIT 5;', \"WITH HoldingsCTE AS (\\n       SELECT H.HOLDING_ID, H.ISSUER_NAME, H.FAIR_VALUE_LEVEL, F.SERIES_NAME\\n       FROM FUND_REPORTED_HOLDING H\\n       JOIN FUND_REPORTED_INFO F ON H.ACCESSION_NUMBER = F.ACCESSION_NUMBER\\n       WHERE H.FAIR_VALUE_LEVEL = 'Level 1'\\n   )\\n   SELECT HOLDING_ID, ISSUER_NAME, SERIES_NAME\\n   FROM HoldingsCTE;\", 'WITH CollateralCTE AS (\\n    SELECT RCP.NAME AS Counterparty_Name, SUM(RC.COLLATERAL_AMOUNT) AS Total_Collateral\\n    FROM REPURCHASE_COLLATERAL RC\\n    JOIN REPURCHASE_COUNTERPARTY RCP ON RC.HOLDING_ID = RCP.HOLDING_ID\\n    GROUP BY RCP.NAME\\n   )\\n   SELECT Counterparty_Name, Total_Collateral\\n   FROM CollateralCTE\\n   ORDER BY Total_Collateral DESC;', \"WITH SecuritiesLending AS (\\n       SELECT ACCESSION_NUMBER\\n       FROM SECURITIES_LENDING\\n       WHERE IS_LOAN_BY_FUND = 'Y'\\n   ),\\n   RepurchaseAgreements AS (\\n       SELECT ACCESSION_NUMBER\\n       FROM REPURCHASE_AGREEMENT\\n   )\\n   SELECT F.SERIES_NAME\\n   FROM FUND_REPORTED_INFO F\\n   WHERE F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM SecuritiesLending)\\n     AND F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM RepurchaseAgreements);\", 'WITH BorrowedAmounts AS (\\n       SELECT BORROWER_ID, SUM(AGGREGATE_VALUE) AS Total_Borrowed\\n       FROM BORROWER\\n       GROUP BY BORROWER_ID\\n       HAVING SUM(AGGREGATE_VALUE) > 5000000\\n   )\\n   SELECT B.NAME, B.LEI, BA.Total_Borrowed\\n   FROM BORROWER B\\n   JOIN BorrowedAmounts BA ON B.BORROWER_ID = BA.BORROWER_ID;', 'WITH CounterpartyCounts AS (\\n       SELECT DC.DERIVATIVE_COUNTERPARTY_NAME, COUNT(*) AS Instrument_Count\\n       FROM DERIVATIVE_COUNTERPARTY DC\\n       JOIN FUND_REPORTED_HOLDING H ON DC.HOLDING_ID = H.HOLDING_ID\\n       JOIN DEBT_SECURITY D ON H.HOLDING_ID = D.HOLDING_ID\\n       GROUP BY DC.DERIVATIVE_COUNTERPARTY_NAME\\n   )\\n   SELECT DERIVATIVE_COUNTERPARTY_NAME, Instrument_Count\\n   FROM CounterpartyCounts\\n   ORDER BY Instrument_Count DESC;', 'WITH RateAverages AS (\\n       SELECT DS.COUPON_TYPE, AVG(DS.ANNUALIZED_RATE) AS Avg_Annualized_Rate\\n       FROM DEBT_SECURITY DS\\n       WHERE DS.ANNUALIZED_RATE IS NOT NULL\\n       GROUP BY DS.COUPON_TYPE\\n   )\\n   SELECT COUPON_TYPE, Avg_Annualized_Rate\\n   FROM RateAverages\\n   ORDER BY Avg_Annualized_Rate DESC;', 'WITH AssetChanges AS (\\n       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\\n              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\\n       FROM FUND_REPORTED_INFO F\\n       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\\n   )\\n   SELECT DISTINCT AC.SERIES_NAME\\n   FROM AssetChanges AC\\n   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\\n     AND AC.Previous_Period_Assets IS NOT NULL;', 'WITH IssuerHoldings AS (\\n       SELECT H.ISSUER_NAME, H.ISSUER_CUSIP, COUNT(DISTINCT H.HOLDING_ID) AS Holding_Count\\n       FROM FUND_REPORTED_HOLDING H\\n       GROUP BY H.ISSUER_NAME, H.ISSUER_CUSIP\\n       HAVING COUNT(DISTINCT H.HOLDING_ID) > 3\\n   )\\n   SELECT ISSUER_NAME, ISSUER_CUSIP, Holding_Count\\n   FROM IssuerHoldings\\n   ORDER BY Holding_Count DESC;', 'WITH NotionalSums AS (\\n        SELECT ODNA.CURRENCY_CODE, SUM(ODNA.NOTIONAL_AMOUNT) AS Total_Notional\\n        FROM OTHER_DERIV_NOTIONAL_AMOUNT ODNA\\n        GROUP BY ODNA.CURRENCY_CODE\\n    )\\n    SELECT CURRENCY_CODE, Total_Notional\\n    FROM NotionalSums\\n    ORDER BY Total_Notional DESC\\n    LIMIT 3;', 'WITH FundPreferences AS (\\n        SELECT F.SERIES_NAME, F.LIQUIDATION_PREFERENCE, F.NET_ASSETS\\n        FROM FUND_REPORTED_INFO F\\n    )\\n    SELECT SERIES_NAME, LIQUIDATION_PREFERENCE, NET_ASSETS\\n    FROM FundPreferences\\n    WHERE LIQUIDATION_PREFERENCE > NET_ASSETS;', \"WITH ConvertibleCTE AS (\\n        SELECT DS.HOLDING_ID, CSC.CONVERSION_RATIO\\n        FROM DEBT_SECURITY DS\\n        JOIN CONVERTIBLE_SECURITY_CURRENCY CSC ON DS.HOLDING_ID = CSC.HOLDING_ID\\n        WHERE DS.IS_CONVTIBLE_CONTINGENT = 'Y' AND CSC.CONVERSION_RATIO > 1.5\\n    )\\n    SELECT HOLDING_ID, CONVERSION_RATIO\\n    FROM ConvertibleCTE;\", 'WITH TopFunds AS (\\n        SELECT SERIES_NAME, ACCESSION_NUMBER\\n        FROM FUND_REPORTED_INFO\\n        ORDER BY TOTAL_ASSETS DESC\\n        LIMIT 10\\n    ),\\n    AssetDistribution AS (\\n        SELECT H.ASSET_CAT, COUNT(*) AS Category_Count\\n        FROM FUND_REPORTED_HOLDING H\\n        JOIN TopFunds T ON H.ACCESSION_NUMBER = T.ACCESSION_NUMBER\\n        GROUP BY H.ASSET_CAT\\n    )\\n    SELECT ASSET_CAT, Category_Count\\n    FROM AssetDistribution\\n    ORDER BY Category_Count DESC;', 'WITH AvgMonthlyReturn AS (\\n       SELECT ACCESSION_NUMBER, \\n              (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\\n       FROM MONTHLY_TOTAL_RETURN\\n   )\\n   SELECT F.SERIES_NAME, A.ACCESSION_NUMBER, A.Avg_Return\\n   FROM AvgMonthlyReturn A\\n   JOIN FUND_REPORTED_INFO F ON A.ACCESSION_NUMBER = F.ACCESSION_NUMBER\\n   ORDER BY A.Avg_Return DESC\\n   LIMIT 10;', 'WITH TopPerformingFunds AS (\\n        SELECT \\n            ACCESSION_NUMBER, \\n            (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\\n        FROM \\n            MONTHLY_TOTAL_RETURN\\n        ORDER BY \\n            Avg_Return DESC\\n        LIMIT 5\\n    )\\n    SELECT \\n        FR.SERIES_NAME, \\n        FR.NET_ASSETS, \\n        TP.Avg_Return\\n    FROM \\n        TopPerformingFunds TP\\n    JOIN \\n        FUND_REPORTED_INFO FR ON TP.ACCESSION_NUMBER = FR.ACCESSION_NUMBER;', 'WITH LatestReturns AS (\\n        SELECT \\n            M.ACCESSION_NUMBER, \\n            M.MONTHLY_TOTAL_RETURN1\\n        FROM \\n            MONTHLY_TOTAL_RETURN M\\n        JOIN \\n            SUBMISSION S ON M.ACCESSION_NUMBER = S.ACCESSION_NUMBER\\n        WHERE \\n            S.REPORT_DATE = (SELECT MAX(REPORT_DATE) FROM SUBMISSION)\\n    )\\n    SELECT \\n        AVG(MONTHLY_TOTAL_RETURN1) AS Average_Return\\n    FROM \\n        LatestReturns;', 'WITH InterestRiskScores AS (\\n        SELECT \\n            IR.ACCESSION_NUMBER, \\n            -- Calculating composite risk score by summing absolute values of DV01 and DV100 columns\\n            (ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV01 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV01 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV01 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV01 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV01 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV100 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV100 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV100 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV100 AS FLOAT)) +\\n            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV100 AS FLOAT))\\n            ) AS Composite_Risk_Score\\n        FROM \\n            INTEREST_RATE_RISK IR\\n    )\\n    SELECT \\n        FR.SERIES_NAME, \\n        FR.ACCESSION_NUMBER, \\n        IRS.Composite_Risk_Score\\n    FROM \\n        InterestRiskScores IRS\\n    JOIN \\n        FUND_REPORTED_INFO FR ON IRS.ACCESSION_NUMBER = FR.ACCESSION_NUMBER\\n    ORDER BY \\n        IRS.Composite_Risk_Score DESC\\n    LIMIT 5;', 'WITH PortfolioComposition AS (\\n    SELECT \\n        ACCESSION_NUMBER, \\n        ASSET_CAT, \\n        SUM(CAST(CURRENCY_VALUE AS FLOAT)) AS Total_Value\\n    FROM \\n        FUND_REPORTED_HOLDING\\n    GROUP BY \\n        ACCESSION_NUMBER, \\n        ASSET_CAT\\n    )\\n    SELECT \\n        F.SERIES_NAME, \\n        PC.ASSET_CAT, \\n        PC.Total_Value\\n    FROM \\n        PortfolioComposition PC\\n    JOIN \\n        FUND_REPORTED_INFO F ON PC.ACCESSION_NUMBER = F.ACCESSION_NUMBER\\n    ORDER BY \\n        F.SERIES_NAME, \\n        PC.Total_Value DESC;', 'WITH AssetCounts AS (\\n        SELECT ASSET_CAT, COUNT(*) AS Count\\n        FROM FUND_REPORTED_HOLDING\\n        GROUP BY ASSET_CAT\\n    )\\n    SELECT ASSET_CAT, Count\\n    FROM AssetCounts\\n    ORDER BY Count DESC\\n    LIMIT 5;', 'WITH AssetChanges AS (\\n       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\\n              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\\n       FROM FUND_REPORTED_Iimport os\\nprint(os.getcwd())NFO F\\n       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\\n   )\\n   SELECT DISTINCT AC.SERIES_NAME\\n   FROM AssetChanges AC\\n   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\\n     AND AC.Previous_Period_Assets IS NOT NULL;']\n",
      "SQL Statements: ['List the top 5 registrants by total net assets, including their CIK and country.', 'Find all holdings with a fair value level of Level 1 and their corresponding fund names.', 'Calculate the total collateral amount for repurchase agreements grouped by counterparty.', 'Locate funds that have both securities lending activities and repurchase agreements.', 'Find borrowers who have borrowed more than $5,000,000, including their names and LEIs.', 'List all derivative counterparties along with the number of derivative instruments they are involved in.', 'Compute the average annualized rate for debt securities grouped by coupon type.', 'Get funds that have experienced a net decrease in assets over the last three reporting periods.', 'Identify issuers with more than three different securities holdings, including their names and CUSIPs.', 'Calculate the total notional amount of derivatives per currency and identify the top 3 currencies by notional amount.', 'List funds with liquidation preferences exceeding their net assets.', 'Find all convertible securities that are contingent and have a conversion ratio above 1.5.', 'Analyze the distribution of asset categories within the top 10 largest funds by total assets.', 'Find the top 10 funds with the highest average monthly returns in the past quarter.', 'Compare the latest net asset values of the top 5 performing funds.', 'Calculate the overall average return across all funds for the most recent month.', 'Find the interest rate risk for each fund and identify those with the highest risk scores.', 'Analyze the composition of fund portfolios by categorizing assets and their total values.', 'Identify the most common asset categories across all fund portfolios.', 'Retrieve funds that have experienced a net decrease in assets over the last three reporting periods.']\n"
     ]
    }
   ],
   "source": [
    "ground_truth_query = [match[1] for match in matches]\n",
    "llm_query = [match[0] for match in matches]\n",
    "print(\"Queries:\", ground_truth_query)\n",
    "print(\"SQL Statements:\", llm_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "Are the following queries the same?\n",
      "i =  0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "while len(results) < 20:\n",
    "    results.append(None)\n",
    "print(len(results))\n",
    "print(len(llm_query))\n",
    "\n",
    "print(\"Are the following queries the same?\")\n",
    "i=0\n",
    "print(\"i = \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:compare_csv:Executing LLM query\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8000\n",
      "ERROR:compare_csv:Unexpected error in process_query: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /din-query (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "ename": "HTTPException",
     "evalue": "500: Unexpected error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /din-query (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connection.py:441\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1048\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m \n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connection.py:279\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connection.py:214\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /din-query (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/compare_csv.py:23\u001b[0m, in \u001b[0;36mcompare_csv_din\u001b[0;34m(ground_truth_query, llm_query)\u001b[0m\n\u001b[1;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting LLM query\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:8000/din-query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_query\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /din-query (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mlen\u001b[39m(llm_query):\n\u001b[0;32m----> 2\u001b[0m     results[i]\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mcompare_csv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_csv_din\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_query\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mllm_query\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      3\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi = \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/compare_csv.py:36\u001b[0m, in \u001b[0;36mcompare_csv_din\u001b[0;34m(ground_truth_query, llm_query)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     35\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error in process_query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPException(\n\u001b[1;32m     37\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     38\u001b[0m         detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m## direct query to the database\u001b[39;00m\n\u001b[1;32m     44\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting grouth truth query...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mHTTPException\u001b[0m: 500: Unexpected error: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /din-query (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x177606cd0>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "if i<len(llm_query):\n",
    "    results[i]=(str(i)+'. '+ str(compare_csv.compare_csv_din(ground_truth_query[i],llm_query[i])))\n",
    "i+=1 #1\n",
    "print(\"i = \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\\nYour task is to analyze questions about financial data and extract key components that will help in database queries.\\n\\nObjective: Break down the given question into essential components that will help formulate a database query.\\n\\nInstructions:\\n1. Read the question carefully to identify:\\n- Individual keywords that map to database columns or values\\n- Technical terms related to financial data\\n- Named entities (companies, funds, locations)\\n- Numerical thresholds or values\\n\\n2. For each identified element, categorize it as:\\n- keywords: Individual significant words that might match database columns\\n- keyphrases: Multi-word expressions that represent single concepts\\n- named_entities: Specific names of companies, funds, or locations\\n- numerical_values: Any numbers, amounts, or thresholds\\n\\n3. Return a JSON object with these categories.'}, {'role': 'user', 'content': '\\nExample Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\\n{\\n    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\\n    \"keyphrases\": [\"PIMCO funds\"],\\n    \"named_entities\": [\"PIMCO\", \"California\"],\\n    \"numerical_values\": [\"2020\", \"2023\"]\\n}\\n\\nExample Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\\n{\\n    \"keywords\": [\"funds\", \"assets\", \"managed\"],\\n    \"keyphrases\": [\"total assets\"],\\n    \"named_entities\": [\"BlackRock\", \"New York\"],\\n    \"numerical_values\": [\"1 billion\"]\\n}\\nQuestion: \"Show me all funds with total assets over 1 billion\"\\n\\nExtract the key components and return as JSON.'}], 'model': 'gpt-4o', 'response_format': {'type': 'json_object'}}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x175d4c5d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x176c41ac0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1758a7dd0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading schema file: chatgpt_api/schema.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 22:21:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'682'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999576'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'12ms'), (b'x-request-id', b'req_7dcf1ce36e8148c5d746150fa21771dd'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=dCrUxsCUBtI4sv1Y36_ojva4IXHLmpqUoOcuUjn_D90-1732141286-1.0.1.1-BZcPWLtciGmwt9QD74AeCTXc.dd93Z8IfFLMVyyvW0W.7S4dply3oY0F9Ph4VQQKCE76yatxfgmGwibTv07oNA; path=/; expires=Wed, 20-Nov-24 22:51:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=OgvaDOmJrQcNUu_ugekdLYGXq6zKbhgvAyJx7yP9juk-1732141286553-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5bd53c0a973161-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 20 Nov 2024 22:21:26 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-annfuni26pdtuawdwdj6zorw'), ('openai-processing-ms', '682'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '2000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '1999576'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '12ms'), ('x-request-id', 'req_7dcf1ce36e8148c5d746150fa21771dd'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=dCrUxsCUBtI4sv1Y36_ojva4IXHLmpqUoOcuUjn_D90-1732141286-1.0.1.1-BZcPWLtciGmwt9QD74AeCTXc.dd93Z8IfFLMVyyvW0W.7S4dply3oY0F9Ph4VQQKCE76yatxfgmGwibTv07oNA; path=/; expires=Wed, 20-Nov-24 22:51:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=OgvaDOmJrQcNUu_ugekdLYGXq6zKbhgvAyJx7yP9juk-1732141286553-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8e5bd53c0a973161-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_7dcf1ce36e8148c5d746150fa21771dd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Finding matches for 'fund'\n",
      "Found 279 matches for 'fund':\n",
      "  securities_lending.is_loan_by_fund: 1.0000\n",
      "  registrant.accession_number: 0.9000\n",
      "  registrant.cik: 0.9000\n",
      "  registrant.registrant_name: 0.9000\n",
      "  registrant.file_num: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for 'asset'\n",
      "Found 7 matches for 'asset':\n",
      "  fund_reported_holding.asset_cat: 1.0000\n",
      "  monthly_return_cat_instrument.asset_cat: 1.0000\n",
      "  fund_reported_info.total_assets: 0.9000\n",
      "  fund_reported_info.net_assets: 0.9000\n",
      "  fund_reported_info.assets_attrbt_to_misc_security: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for '1'\n",
      "Found 0 matches for '1':\n",
      "\n",
      "DEBUG: Finding matches for 'billion'\n",
      "Found 2 matches for 'billion':\n",
      "  submission.filing_date: 0.6154\n",
      "  submission.is_last_filing: 0.6154\n",
      "\n",
      "DEBUG: Finding matches for 'total'\n",
      "Found 279 matches for 'total':\n",
      "  fund_reported_info.total_assets: 1.0000\n",
      "  fund_reported_info.total_liabilities: 1.0000\n",
      "  monthly_total_return.monthly_total_return_id: 1.0000\n",
      "  monthly_total_return.monthly_total_return1: 1.0000\n",
      "  monthly_total_return.monthly_total_return2: 1.0000\n",
      "\n",
      "Processed Schema Links:\n",
      "Table Columns: ['securities_lending.is_loan_by_fund', 'fund_reported_holding.asset_cat', 'fund_reported_info.total_assets']\n",
      "Primary Keys: ['FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID']\n",
      "Foreign Keys: ['REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID']\n",
      "Schema Links: {'table_columns': ['securities_lending.is_loan_by_fund', 'fund_reported_holding.asset_cat', 'fund_reported_info.total_assets'], 'primary_keys': ['FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID'], 'foreign_keys': ['REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID']}\n"
     ]
    }
   ],
   "source": [
    "############################################ VALUE RETRIEVAL AND SCHEMA LINKING\n",
    "class PSLsh:\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class ValueRetrieval:\n",
    "    financial_terms = {\n",
    "            'total': ['total', 'sum', 'aggregate', 'combined'],\n",
    "            'assets': ['asset', 'holdings', 'investments', 'securities'],\n",
    "            'liabilities': ['liability', 'debt', 'obligations'],\n",
    "            'net': ['net', 'pure', 'adjusted'],\n",
    "            'fund': ['fund', 'portfolio', 'investment vehicle'],\n",
    "            'return': ['return', 'yield', 'profit', 'gain'],\n",
    "            'monthly': ['monthly', 'month', 'monthly basis'],\n",
    "            'rate': ['rate', 'percentage', 'ratio'],\n",
    "            'risk': ['risk', 'exposure', 'vulnerability']\n",
    "        }\n",
    "    def __init__(self, schema_path: str = 'chatgpt_api/schema.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load schema\n",
    "        print(\"DEBUG: Loading schema file:\", schema_path)\n",
    "        with open(schema_path, 'r') as f:\n",
    "            self.schema = json.load(f)\n",
    "\n",
    "        # Initialize lemmatizer and stop words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Build column name index\n",
    "        self.column_index = self._build_column_index()\n",
    "        \n",
    "        # Build common financial terms dictionary\n",
    "        \n",
    "\n",
    "        # Build vectorizer and LSH for backup matching\n",
    "        self.build_vectorizer_and_lsh(seed=lsh_seed)\n",
    "        \n",
    "        # Get schema relationships\n",
    "        self.primary_keys, self.foreign_keys = self.discover_schema_relationships()\n",
    "\n",
    "    def _build_column_index(self) -> Dict:\n",
    "        \"\"\"Build an index of all columns with their metadata.\"\"\"\n",
    "        column_index = {}\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        \n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                \n",
    "                # Store both the full qualified name and column properties\n",
    "                qualified_name = f\"{table_name}.{column_name}\"\n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': column_name,\n",
    "                    'type': column.get('type', ''),\n",
    "                    'words': self._split_column_name(column_name),\n",
    "                    'synonyms': self._get_column_synonyms(column_name)\n",
    "                }\n",
    "                \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        # Handle both underscore and camel case\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        synonyms = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.financial_terms:\n",
    "                synonyms.extend(self.financial_terms[word])\n",
    "                \n",
    "        return list(set(synonyms))\n",
    "\n",
    "    def build_vectorizer_and_lsh(self, seed: int):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "        self.term_list = self.get_schema_terms()\n",
    "        self.term_vectors = self.vectorizer.fit_transform(self.term_list)\n",
    "        self.lsh = PSLsh(self.term_vectors, n_planes=10, n_tables=5)\n",
    "\n",
    "    def get_schema_terms(self) -> List[str]:\n",
    "        terms = []\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            terms.append(table_name)\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                terms.append(f\"{table_name}.{column_name}\")\n",
    "        return terms\n",
    "\n",
    "    def discover_schema_relationships(self):\n",
    "        # Define our primary keys and foreign keys here\n",
    "        primary_keys = {\n",
    "            'SUBMISSION': ['ACCESSION_NUMBER'],\n",
    "            'REGISTRANT': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_INFO': ['ACCESSION_NUMBER'],\n",
    "            'INTEREST_RATE_RISK': ['ACCESSION_NUMBER', 'INTEREST_RATE_RISK_ID'],\n",
    "            'BORROWER': ['ACCESSION_NUMBER', 'BORROWER_ID'],\n",
    "            'BORROW_AGGREGATE': ['ACCESSION_NUMBER', 'BORROW_AGGREGATE_ID'],\n",
    "            'MONTHLY_TOTAL_RETURN': ['ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN_ID'],\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT': ['ACCESSION_NUMBER', 'ASSET_CAT', 'INSTRUMENT_KIND'],\n",
    "            'FUND_VAR_INFO': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_HOLDING': ['ACCESSION_NUMBER', 'HOLDING_ID'],\n",
    "            'IDENTIFIERS': ['HOLDING_ID', 'IDENTIFIERS_ID'],\n",
    "            'DEBT_SECURITY': [],  \n",
    "            'DEBT_SECURITY_REF_INSTRUMENT': ['HOLDING_ID', 'DEBT_SECURITY_REF_ID'],\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY': ['HOLDING_ID', 'CONVERTIBLE_SECURITY_ID'],\n",
    "            'REPURCHASE_AGREEMENT': ['HOLDING_ID'],\n",
    "            'REPURCHASE_COUNTERPARTY': ['HOLDING_ID', 'REPURCHASE_COUNTERPARTY_ID'],\n",
    "            'REPURCHASE_COLLATERAL': ['HOLDING_ID', 'REPURCHASE_COLLATERAL_ID'],\n",
    "            'DERIVATIVE_COUNTERPARTY': ['HOLDING_ID', 'DERIVATIVE_COUNTERPARTY_ID'],\n",
    "            'SWAPTION_OPTION_WARNT_DERIV': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_BASKET': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_COMPONENT': ['HOLDING_ID', 'DESC_REF_INDEX_COMPONENT_ID'],\n",
    "            'DESC_REF_OTHER': ['HOLDING_ID', 'DESC_REF_OTHER_ID'],\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT': ['HOLDING_ID'],\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP': ['HOLDING_ID'],\n",
    "            'NONFOREIGN_EXCHANGE_SWAP': ['HOLDING_ID'],\n",
    "            'FLOATING_RATE_RESET_TENOR': ['HOLDING_ID', 'RATE_RESET_TENOR_ID'],\n",
    "            'OTHER_DERIV': ['HOLDING_ID'],\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT': ['HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT_ID'],\n",
    "            'SECURITIES_LENDING': ['HOLDING_ID'],\n",
    "            'EXPLANATORY_NOTE': ['ACCESSION_NUMBER', 'EXPLANATORY_NOTE_ID']\n",
    "        }\n",
    "\n",
    "        foreign_keys = [\n",
    "            # ACCESSION_NUMBER relationships\n",
    "            'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "\n",
    "            # HOLDING_ID relationships\n",
    "            'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID'\n",
    "        ]\n",
    "\n",
    "        formatted_pks = []\n",
    "        for table, keys in primary_keys.items():\n",
    "            for key in keys:\n",
    "                formatted_pks.append(f\"{table}.{key}\")\n",
    "\n",
    "        return formatted_pks, foreign_keys\n",
    "\n",
    "    def find_similar_words(self, word: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Enhanced matching using multiple techniques.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        print(f\"\\nDEBUG: Finding matches for '{word}'\")\n",
    "        \n",
    "        matches = []\n",
    "        \n",
    "        # 1. Direct matching with column names and their components\n",
    "        for qualified_name, metadata in self.column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "                \n",
    "            # Check synonyms\n",
    "            if word in self.financial_terms.get(word, []):\n",
    "                matches.append((qualified_name, 0.9))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            # Fuzzy match with synonyms\n",
    "            for term, synonyms in self.financial_terms.items():\n",
    "                if term in metadata['words']:\n",
    "                    for synonym in synonyms:\n",
    "                        ratio = fuzz.ratio(word, synonym) / 100.0\n",
    "                        if ratio > score:\n",
    "                            score = ratio * 0.9  # Slightly lower weight for synonym matches\n",
    "            \n",
    "            if score > 0.6:  # Only include if similarity is above 60%\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # 2. LSH-based matching as backup\n",
    "        if len(matches) < 5:  # If we have fewer than 5 matches, try LSH\n",
    "            try:\n",
    "                word_vector = self.vectorizer.transform([word]).toarray()[0]\n",
    "                candidate_indices = self.lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = self.term_list[idx]\n",
    "                    if not any(term == m[0] for m in matches):  # Avoid duplicates\n",
    "                        candidate_vector = self.term_vectors[idx].toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:  # Only include if similarity is above 50%\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        # Remove duplicates keeping highest score and sort by score\n",
    "        unique_matches = {}\n",
    "        for term, score in matches:\n",
    "            if term not in unique_matches or score > unique_matches[term]:\n",
    "                unique_matches[term] = score\n",
    "        \n",
    "        matches = [(term, score) for term, score in unique_matches.items()]\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"Found {len(matches)} matches for '{word}':\")\n",
    "        for match, score in matches[:5]:\n",
    "            print(f\"  {match}: {score:.4f}\")\n",
    "        \n",
    "        return matches[:5] if matches else [('fund_reported_info.total_assets', 0.6)] if word in ['total', 'asset', 'assets'] else []\n",
    "\n",
    "    def extract_keywords(self, question: str) -> Dict:\n",
    "        system_prompt = \"\"\"You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\n",
    "Your task is to analyze questions about financial data and extract key components that will help in database queries.\n",
    "\n",
    "Objective: Break down the given question into essential components that will help formulate a database query.\n",
    "\n",
    "Instructions:\n",
    "1. Read the question carefully to identify:\n",
    "- Individual keywords that map to database columns or values\n",
    "- Technical terms related to financial data\n",
    "- Named entities (companies, funds, locations)\n",
    "- Numerical thresholds or values\n",
    "\n",
    "2. For each identified element, categorize it as:\n",
    "- keywords: Individual significant words that might match database columns\n",
    "- keyphrases: Multi-word expressions that represent single concepts\n",
    "- named_entities: Specific names of companies, funds, or locations\n",
    "- numerical_values: Any numbers, amounts, or thresholds\n",
    "\n",
    "3. Return a JSON object with these categories.\"\"\"\n",
    "\n",
    "        few_shot_examples = \"\"\"\n",
    "Example Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\n",
    "{\n",
    "    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\n",
    "    \"keyphrases\": [\"PIMCO funds\"],\n",
    "    \"named_entities\": [\"PIMCO\", \"California\"],\n",
    "    \"numerical_values\": [\"2020\", \"2023\"]\n",
    "}\n",
    "\n",
    "Example Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\n",
    "{\n",
    "    \"keywords\": [\"funds\", \"assets\", \"managed\"],\n",
    "    \"keyphrases\": [\"total assets\"],\n",
    "    \"named_entities\": [\"BlackRock\", \"New York\"],\n",
    "    \"numerical_values\": [\"1 billion\"]\n",
    "}\"\"\"\n",
    "\n",
    "        formatted_prompt = system_prompt\n",
    "        user_prompt = f\"Question: \\\"{question}\\\"\\n\\nExtract the key components and return as JSON.\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": formatted_prompt},\n",
    "                {\"role\": \"user\", \"content\": few_shot_examples + \"\\n\" + user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:  # Add check for empty text\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []  # Return empty list instead of None on error\n",
    "       \n",
    "       \n",
    "    def process_schema(self, question: str) -> str:\n",
    "        # Get all the processing results\n",
    "        results = self.process_question(question)\n",
    "        \n",
    "        # Organize schema links by type\n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # 1. Get main table/column matches\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                # Only take the top match if score > 0.7\n",
    "                top_match = matches[0]  # (match, score)\n",
    "                if top_match[1] > 0.7:\n",
    "                    # Handle numerical values\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        if 'billion' in word.lower():\n",
    "                            table_columns.append(f\"{top_match[0]} > 1000000000\")\n",
    "                        elif 'million' in word.lower():\n",
    "                            table_columns.append(f\"{top_match[0]} > 1000000\")\n",
    "                        else:\n",
    "                            table_columns.append(f\"{top_match[0]} > {word}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # 2. Get relevant tables\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].upper())\n",
    "        \n",
    "        # 3. Add relevant primary keys\n",
    "        for pk in results['schema_relationships']['primary_keys']:\n",
    "            table = pk.split('.')[0]\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "        \n",
    "        # 4. Add relevant foreign keys\n",
    "        for fk in results['schema_relationships']['foreign_keys']:\n",
    "            tables_in_fk = set(part.split('.')[0] for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        \n",
    "        # Format output with sections\n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys\n",
    "        }\n",
    "        \n",
    "        print(\"\\nProcessed Schema Links:\")\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "        print(\"Primary Keys:\", relevant_primary_keys)\n",
    "        print(\"Foreign Keys:\", relevant_foreign_keys)\n",
    "        \n",
    "        return str(schema_dict)\n",
    "\n",
    "\n",
    "    def process_question(self, question: str) -> Dict:\n",
    "        # Extract keywords using gpt\n",
    "        extracted_info = self.extract_keywords(question)\n",
    "\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'named_entities', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "\n",
    "        # Preprocess the words (lemmatize, remove stop words)\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "\n",
    "        # Remove duplicates\n",
    "        processed_words = list(set(processed_words))\n",
    "\n",
    "        # Find similar columns for each word\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self.find_similar_words(word)\n",
    "\n",
    "        # Combine the results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self.primary_keys,\n",
    "                \"foreign_keys\": self.foreign_keys\n",
    "            }\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    vr = ValueRetrieval(schema_path='chatgpt_api/schema.json')\n",
    "    schema_links = vr.process_schema(\"Show me all funds with total assets over 1 billion\")\n",
    "    print(\"Schema Links:\", schema_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ CLASSIFICATION\n",
    "classification_prompt = '''Q: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\n",
    "schema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\n",
    "A: Let’s think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Get the names and CIK of registrants who are located in California.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\n",
    "A: Let’s think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\n",
    "A: Let's analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\n",
    "Label: \"NESTED\"\n",
    "\n",
    "'''\n",
    "\n",
    "def classification_prompt_maker(question,relevant_schema_links):\n",
    "  instruction = \"# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\"\n",
    "  instruction += \"\\nif need nested queries: predict NESTED\\n\"\n",
    "  instruction += \"elif need JOIN and don't need nested queries: predict NON-NESTED\\n\"\n",
    "  instruction += \"elif don't need JOIN and don't need nested queries: predict EASY\\n\\n\"\n",
    "  prompt = instruction + classification_prompt + 'Q: \"' + question + '\\nrelevant_schema_links: ' + relevant_schema_links + '\\nA: Let’s think step by step.'\n",
    "  return prompt\n",
    "\n",
    "def process_question_classification(question, relevant_schema_links):\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            print(\"Attempting classification...\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(question, relevant_schema_links=relevant_schema_links)\n",
    "                }],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                temperature=0.0,\n",
    "                max_tokens=300,  # Reduced max tokens\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "                # Removed the stop sequence\n",
    "            )\n",
    "            classification = response.choices[0].message.content\n",
    "            print(\"Raw response:\", classification)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    if classification is None:\n",
    "        print(\"Failed to get classification after 3 attempts\")\n",
    "        return '\"NESTED\"'\n",
    "        \n",
    "    try:\n",
    "        predicted_class = classification.split(\"Label: \")[1].strip().strip('\"')\n",
    "        return f'\"{predicted_class}\"'\n",
    "    except:\n",
    "        print(\"Slicing error for the classification module\")\n",
    "        return '\"NESTED\"'\n",
    "\n",
    "\n",
    "############################################ SQL GENERATION\n",
    "easy_prompt = '''Q: \"Find the issuers with a balance greater than 1 million.\"\n",
    "Schema_links: [fund_reported_holding.balance]\n",
    "SQL: SELECT DISTINCT issuer_name \n",
    "      FROM fund_reported_holding \n",
    "      WHERE balance > 1000000\n",
    "'''\n",
    "\n",
    "medium_prompt = '''Q: \"Find the total upfront payments and receipts for swaps with fixed rate receipts.\"\n",
    "Schema_links: [nonforeign_exchange_swap.upfront_payment, nonforeign_exchange_swap.upfront_receipt, nonforeign_exchange_swap.fixed_rate_receipt]\n",
    "A: Let’s think step by step. For creating the SQL for the given question, we need to filter the swaps that have fixed rate receipts. Then, sum up the upfront payments and receipts. First, create an intermediate representation, then use it to construct the SQL query.\n",
    "Intermediate_representation: \n",
    "SELECT SUM(nonforeign_exchange_swap.upfront_payment) + SUM(nonforeign_exchange_swap.upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE nonforeign_exchange_swap.fixed_rate_receipt IS NOT NULL\n",
    "SQL: \n",
    "SELECT SUM(upfront_payment) + SUM(upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE fixed_rate_receipt IS NOT NULL\n",
    "'''\n",
    "\n",
    "hard_prompt = '''Q: \"Find the borrowers with aggregate value greater than $1 million and whose interest rate change at 10-year maturity for a 100 basis point change is positive.\"\n",
    "Schema_links: [borrower.aggregate_value, borrower.name, interest_rate_risk.intrst_rate_change_10yr_dv100]\n",
    "A: Let's think step by step. First, we need to filter borrowers with aggregate values greater than $1 million. Then, we need to check for interest rate changes at 10-year maturity where the change is positive. \n",
    "The SQL query for the sub-question \"What are the borrowers with aggregate value greater than $1 million and positive interest rate change at 10-year maturity for 100 basis points?\" is:\n",
    "\n",
    "Intermediate_representation: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0\n",
    "\n",
    "SQL: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0\n",
    "'''\n",
    "\n",
    "def hard_prompt_maker(question,database,schema_links,sub_questions=\"\"):\n",
    "    instruction = \"# Use the intermediate representation and the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "    if sub_questions==\"\":\n",
    "        stepping = f'''\\nA: Let's think step by step. \"{question}\" can be solved by first solving a sub-question using nested queries\".'''\n",
    "    else:\n",
    "        stepping = f'''\\nA: Let's think step by step. \"{question}\" can be solved by first solving the answer to the following sub-question \"{sub_questions}\".'''\n",
    "    prompt = instruction + hard_prompt+ chat_prompt.gpt_queries_hard+ 'Q: \"' + question + '\"' + '\\nschema_links: ' + schema_links + stepping +'\\nThe SQL query for the sub-question:\"'\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question,database,schema_links):\n",
    "    instruction = \"# Use the the schema links and Intermediate_representation to generate the SQL queries for each of the questions.\\n\"\n",
    "    prompt = instruction + medium_prompt + chat_prompt.gpt_queries_medium+ 'Q: \"' + question + '\\nSchema_links: ' + schema_links + '\\nA: Let’s think step by step.'\n",
    "    return prompt\n",
    "\n",
    "def easy_prompt_maker(question,database,schema_links):\n",
    "    instruction = \"# Use the the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "    prompt = instruction + easy_prompt + chat_prompt.gpt_queries_easy + 'Q: \"' + question + '\\nSchema_links: ' + schema_links + '\\nSQL:'\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\\nYour task is to analyze questions about financial data and extract key components that will help in database queries.\\n\\nObjective: Break down the given question into essential components that will help formulate a database query.\\n\\nInstructions:\\n1. Read the question carefully to identify:\\n- Individual keywords that map to database columns or values\\n- Technical terms related to financial data\\n- Named entities (companies, funds, locations)\\n- Numerical thresholds or values\\n\\n2. For each identified element, categorize it as:\\n- keywords: Individual significant words that might match database columns\\n- keyphrases: Multi-word expressions that represent single concepts\\n- named_entities: Specific names of companies, funds, or locations\\n- numerical_values: Any numbers, amounts, or thresholds\\n\\n3. Return a JSON object with these categories.'}, {'role': 'user', 'content': '\\nExample Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\\n{\\n    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\\n    \"keyphrases\": [\"PIMCO funds\"],\\n    \"named_entities\": [\"PIMCO\", \"California\"],\\n    \"numerical_values\": [\"2020\", \"2023\"]\\n}\\n\\nExample Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\\n{\\n    \"keywords\": [\"funds\", \"assets\", \"managed\"],\\n    \"keyphrases\": [\"total assets\"],\\n    \"named_entities\": [\"BlackRock\", \"New York\"],\\n    \"numerical_values\": [\"1 billion\"]\\n}\\nQuestion: \"Show me all funds with total assets over 1 billion\"\\n\\nExtract the key components and return as JSON.'}], 'model': 'gpt-4o', 'response_format': {'type': 'json_object'}}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x291b4bc50>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x291ad84d0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17689f590>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading schema file: chatgpt_api/schema.json\n",
      "\n",
      "==================================================\n",
      "\n",
      "Processing question: Show me all funds with total assets over 1 billion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 22:25:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'636'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999576'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'12ms'), (b'x-request-id', b'req_d29b9dc9c8b70285721ae3db5f78b15e'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ypHaAoflCFial_ftuWraWBV6pjqky50aw6OLyjm4Gi4-1732141544-1.0.1.1-nAJSeZ3W_M_jIjX12Mihq3JkD2DZy.XMvhQpUhl_1kxBtFzznLG82tP3VXg7NxDULYGlzdq3gyKI1utjPQeOFg; path=/; expires=Wed, 20-Nov-24 22:55:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=JCWcBZH0BYZl.SWhlNMG3M0V.6gUGlDlXKwhf.yvYFI-1732141544404-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5bdb87e90a2b4f-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 20 Nov 2024 22:25:44 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-annfuni26pdtuawdwdj6zorw'), ('openai-processing-ms', '636'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '2000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '1999576'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '12ms'), ('x-request-id', 'req_d29b9dc9c8b70285721ae3db5f78b15e'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=ypHaAoflCFial_ftuWraWBV6pjqky50aw6OLyjm4Gi4-1732141544-1.0.1.1-nAJSeZ3W_M_jIjX12Mihq3JkD2DZy.XMvhQpUhl_1kxBtFzznLG82tP3VXg7NxDULYGlzdq3gyKI1utjPQeOFg; path=/; expires=Wed, 20-Nov-24 22:55:44 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=JCWcBZH0BYZl.SWhlNMG3M0V.6gUGlDlXKwhf.yvYFI-1732141544404-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8e5bdb87e90a2b4f-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_d29b9dc9c8b70285721ae3db5f78b15e\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\\nif need nested queries: predict NESTED\\nelif need JOIN and don\\'t need nested queries: predict NON-NESTED\\nelif don\\'t need JOIN and don\\'t need nested queries: predict EASY\\n\\nQ: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\\nschema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\\nA: Let’s think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Get the names and CIK of registrants who are located in California.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\\nA: Let’s think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\\nA: Let\\'s analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\\nLabel: \"NESTED\"\\n\\nQ: \"Show me all funds with total assets over 1 billion\\nrelevant_schema_links: {\\'table_columns\\': [\\'securities_lending.is_loan_by_fund\\', \\'fund_reported_holding.asset_cat\\', \\'fund_reported_info.total_assets\\'], \\'primary_keys\\': [\\'FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'FUND_REPORTED_HOLDING.ACCESSION_NUMBER\\', \\'FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'SECURITIES_LENDING.HOLDING_ID\\'], \\'foreign_keys\\': [\\'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\']}\\nA: Let’s think step by step.'}], 'model': 'gpt-4', 'frequency_penalty': 0.0, 'max_tokens': 300, 'n': 1, 'presence_penalty': 0.0, 'stream': False, 'temperature': 0.0, 'top_p': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x291ae48d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x176c41910> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1768a4a10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Finding matches for 'fund'\n",
      "Found 279 matches for 'fund':\n",
      "  securities_lending.is_loan_by_fund: 1.0000\n",
      "  registrant.accession_number: 0.9000\n",
      "  registrant.cik: 0.9000\n",
      "  registrant.registrant_name: 0.9000\n",
      "  registrant.file_num: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for 'asset'\n",
      "Found 7 matches for 'asset':\n",
      "  fund_reported_holding.asset_cat: 1.0000\n",
      "  monthly_return_cat_instrument.asset_cat: 1.0000\n",
      "  fund_reported_info.total_assets: 0.9000\n",
      "  fund_reported_info.net_assets: 0.9000\n",
      "  fund_reported_info.assets_attrbt_to_misc_security: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for '1'\n",
      "Found 0 matches for '1':\n",
      "\n",
      "DEBUG: Finding matches for 'billion'\n",
      "Found 2 matches for 'billion':\n",
      "  submission.filing_date: 0.6154\n",
      "  submission.is_last_filing: 0.6154\n",
      "\n",
      "DEBUG: Finding matches for 'total'\n",
      "Found 279 matches for 'total':\n",
      "  fund_reported_info.total_assets: 1.0000\n",
      "  fund_reported_info.total_liabilities: 1.0000\n",
      "  monthly_total_return.monthly_total_return_id: 1.0000\n",
      "  monthly_total_return.monthly_total_return1: 1.0000\n",
      "  monthly_total_return.monthly_total_return2: 1.0000\n",
      "\n",
      "Processed Schema Links:\n",
      "Table Columns: ['securities_lending.is_loan_by_fund', 'fund_reported_holding.asset_cat', 'fund_reported_info.total_assets']\n",
      "Primary Keys: ['FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID']\n",
      "Foreign Keys: ['REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID']\n",
      "Attempting classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 22:25:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'4135'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'298528'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'294ms'), (b'x-request-id', b'req_fc8c648c023e1e823d155aed9e887119'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5bdb8cda9014e0-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 20 Nov 2024 22:25:48 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-annfuni26pdtuawdwdj6zorw', 'openai-processing-ms': '4135', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '300000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '298528', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '294ms', 'x-request-id': 'req_fc8c648c023e1e823d155aed9e887119', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8e5bdb8cda9014e0-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_fc8c648c023e1e823d155aed9e887119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: The SQL query for the question \"Show me all funds with total assets over 1 billion\" needs these tables = [fund_reported_info], so we don't need JOIN.\n",
      "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
      "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
      "Label: \"EASY\"\n",
      "Classification: \"EASY\"\n"
     ]
    }
   ],
   "source": [
    "# Test pipeline\n",
    "def test_pipeline():\n",
    "   vr = ValueRetrieval(schema_path='chatgpt_api/schema.json')\n",
    "   \n",
    "   test_questions = [\n",
    "       \"Show me all funds with total assets over 1 billion\",\n",
    "   ]\n",
    "\n",
    "   for question in test_questions:\n",
    "       print(\"\\n\" + \"=\"*50)\n",
    "       print(f\"\\nProcessing question: {question}\")\n",
    "       \n",
    "       # Get schema info\n",
    "       schema_info = vr.process_schema(question)\n",
    "       #print(\"\\nSchema Info:\")\n",
    "       #print(schema_info)\n",
    "       \n",
    "       # Pass to classification\n",
    "       #print(\"\\nGetting classification...\")\n",
    "       classification = process_question_classification(question, schema_info)\n",
    "       print(\"Classification:\", classification)\n",
    "\n",
    "# Run test\n",
    "test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4_generation(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        n = 1,\n",
    "        stream = False,\n",
    "        temperature=0.0,\n",
    "        max_tokens=600,\n",
    "        top_p = 1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop = [\"Q:\"]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links):\n",
    "    if '\"EASY\"' in predicted_class:\n",
    "        print(\"EASY\")\n",
    "        SQL = None\n",
    "        while SQL is None:\n",
    "            try:\n",
    "                SQL = GPT4_generation(easy_prompt_maker(question, schema_links))\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                pass\n",
    "    elif '\"NON-NESTED\"' in predicted_class:\n",
    "        print(\"NON-NESTED\")\n",
    "        SQL = None\n",
    "        while SQL is None:\n",
    "            try:\n",
    "                SQL = GPT4_generation(medium_prompt_maker(question, schema_links))\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                pass\n",
    "        try:\n",
    "            SQL = SQL.split(\"SQL: \")[1]\n",
    "        except:\n",
    "            print(\"SQL slicing error\")\n",
    "            SQL = \"SELECT\"\n",
    "    else:\n",
    "        print(\"NESTED\")\n",
    "        SQL = None\n",
    "        while SQL is None:\n",
    "            try:\n",
    "                SQL = GPT4_generation(\n",
    "                    hard_prompt_maker(question, schema_links))\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                pass\n",
    "        try:\n",
    "            SQL = SQL.split(\"SQL: \")[1]\n",
    "        except:\n",
    "            print(\"SQL slicing error\")\n",
    "            SQL = \"SELECT\"\n",
    "    print(SQL)\n",
    "    return SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
