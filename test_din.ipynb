{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/virounikamina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/virounikamina/Desktop/PIMCO-Text2SQL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import compare_csv\n",
    "import openai as OpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz.distance.Levenshtein import distance\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"/d/GithubRepos/PIMCO-Text2SQL\"))\n",
    "din_modules_path = os.path.join(current_dir, 'chatgpt_api')\n",
    "sys.path.append(din_modules_path)\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"OpenAI API key not configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_keys():\n",
    "    \"\"\"Explore potential primary and foreign keys in the database\"\"\"\n",
    "    import sqlite3\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('sqlite/nport.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    print(\"Database Key Analysis:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Analyze each table\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        print(f\"\\nTable: {table_name}\")\n",
    "\n",
    "        # Get column info\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Get sample count for potential key columns\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            # Check if column name contains potential key indicators\n",
    "            if any(key_term in col_name.lower() for key_term in ['_id', 'accession', 'number']):\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) total_rows, \n",
    "                           COUNT(DISTINCT {col_name}) unique_values \n",
    "                    FROM {table_name}\n",
    "                    WHERE {col_name} IS NOT NULL\n",
    "                \"\"\")\n",
    "                stats = cursor.fetchone()\n",
    "                print(f\"Column: {col_name}\")\n",
    "                print(f\"Total rows: {stats[0]}\")\n",
    "                print(f\"Unique values: {stats[1]}\")\n",
    "                \n",
    "                # If unique values equals total rows, likely a key\n",
    "                if stats[0] == stats[1] and stats[0] > 0:\n",
    "                    print(\">>> Potential PRIMARY KEY <<<\")\n",
    "\n",
    "        # Look for foreign key relationships\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            if col_name == 'ACCESSION_NUMBER':\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM {table_name} t1\n",
    "                    WHERE EXISTS (\n",
    "                        SELECT 1 FROM FUND_REPORTED_INFO t2 \n",
    "                        WHERE t1.ACCESSION_NUMBER = t2.ACCESSION_NUMBER\n",
    "                    )\n",
    "                \"\"\")\n",
    "                match_count = cursor.fetchone()[0]\n",
    "                if match_count > 0:\n",
    "                    print(f\"Foreign Key: {table_name}.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\")\n",
    "            \n",
    "            elif col_name == 'HOLDING_ID':\n",
    "                cursor.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM {table_name} t1\n",
    "                    WHERE EXISTS (\n",
    "                        SELECT 1 FROM FUND_REPORTED_HOLDING t2 \n",
    "                        WHERE t1.HOLDING_ID = t2.HOLDING_ID\n",
    "                    )\n",
    "                \"\"\")\n",
    "                match_count = cursor.fetchone()[0]\n",
    "                if match_count > 0:\n",
    "                    print(f\"Foreign Key: {table_name}.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "# Run the analysis\n",
    "explore_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "1. \"List the top 5 registrants by total net assets, including their CIK and country.\"\n",
    "   SQL: \n",
    "   WITH FundAssets AS (\n",
    "       SELECT R.CIK, R.REGISTRANT_NAME, R.COUNTRY, F.NET_ASSETS\n",
    "       FROM REGISTRANT R\n",
    "       JOIN FUND_REPORTED_INFO F ON R.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT CIK, REGISTRANT_NAME, COUNTRY, NET_ASSETS\n",
    "   FROM FundAssets\n",
    "   ORDER BY NET_ASSETS DESC\n",
    "   LIMIT 5;\n",
    "\n",
    "2. \"Find all holdings with a fair value level of Level 1 and their corresponding fund names.\"\n",
    "   SQL: \n",
    "   WITH HoldingsCTE AS (\n",
    "       SELECT H.HOLDING_ID, H.ISSUER_NAME, H.FAIR_VALUE_LEVEL, F.SERIES_NAME\n",
    "       FROM FUND_REPORTED_HOLDING H\n",
    "       JOIN FUND_REPORTED_INFO F ON H.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "       WHERE H.FAIR_VALUE_LEVEL = 'Level 1'\n",
    "   )\n",
    "   SELECT HOLDING_ID, ISSUER_NAME, SERIES_NAME\n",
    "   FROM HoldingsCTE;\n",
    "\n",
    "3. \"Calculate the total collateral amount for repurchase agreements grouped by counterparty.\"\n",
    "   SQL: \n",
    "   WITH CollateralCTE AS (\n",
    "    SELECT RCP.NAME AS Counterparty_Name, SUM(RC.COLLATERAL_AMOUNT) AS Total_Collateral\n",
    "    FROM REPURCHASE_COLLATERAL RC\n",
    "    JOIN REPURCHASE_COUNTERPARTY RCP ON RC.HOLDING_ID = RCP.HOLDING_ID\n",
    "    GROUP BY RCP.NAME\n",
    "   )\n",
    "   SELECT Counterparty_Name, Total_Collateral\n",
    "   FROM CollateralCTE\n",
    "   ORDER BY Total_Collateral DESC;\n",
    "\n",
    "4. \"Locate funds that have both securities lending activities and repurchase agreements.\"\n",
    "   SQL: \n",
    "   WITH SecuritiesLending AS (\n",
    "       SELECT ACCESSION_NUMBER\n",
    "       FROM SECURITIES_LENDING\n",
    "       WHERE IS_LOAN_BY_FUND = 'Y'\n",
    "   ),\n",
    "   RepurchaseAgreements AS (\n",
    "       SELECT ACCESSION_NUMBER\n",
    "       FROM REPURCHASE_AGREEMENT\n",
    "   )\n",
    "   SELECT F.SERIES_NAME\n",
    "   FROM FUND_REPORTED_INFO F\n",
    "   WHERE F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM SecuritiesLending)\n",
    "     AND F.ACCESSION_NUMBER IN (SELECT ACCESSION_NUMBER FROM RepurchaseAgreements);\n",
    "\n",
    "5. \"Find borrowers who have borrowed more than $5,000,000, including their names and LEIs.\"\n",
    "   SQL: \n",
    "   WITH BorrowedAmounts AS (\n",
    "       SELECT BORROWER_ID, SUM(AGGREGATE_VALUE) AS Total_Borrowed\n",
    "       FROM BORROWER\n",
    "       GROUP BY BORROWER_ID\n",
    "       HAVING SUM(AGGREGATE_VALUE) > 5000000\n",
    "   )\n",
    "   SELECT B.NAME, B.LEI, BA.Total_Borrowed\n",
    "   FROM BORROWER B\n",
    "   JOIN BorrowedAmounts BA ON B.BORROWER_ID = BA.BORROWER_ID;\n",
    "\n",
    "6. \"List all derivative counterparties along with the number of derivative instruments they are involved in.\"\n",
    "   SQL: \n",
    "   WITH CounterpartyCounts AS (\n",
    "       SELECT DC.DERIVATIVE_COUNTERPARTY_NAME, COUNT(*) AS Instrument_Count\n",
    "       FROM DERIVATIVE_COUNTERPARTY DC\n",
    "       JOIN FUND_REPORTED_HOLDING H ON DC.HOLDING_ID = H.HOLDING_ID\n",
    "       JOIN DEBT_SECURITY D ON H.HOLDING_ID = D.HOLDING_ID\n",
    "       GROUP BY DC.DERIVATIVE_COUNTERPARTY_NAME\n",
    "   )\n",
    "   SELECT DERIVATIVE_COUNTERPARTY_NAME, Instrument_Count\n",
    "   FROM CounterpartyCounts\n",
    "   ORDER BY Instrument_Count DESC;\n",
    "\n",
    "7. \"Compute the average annualized rate for debt securities grouped by coupon type.\"\n",
    "   SQL: \n",
    "   WITH RateAverages AS (\n",
    "       SELECT DS.COUPON_TYPE, AVG(DS.ANNUALIZED_RATE) AS Avg_Annualized_Rate\n",
    "       FROM DEBT_SECURITY DS\n",
    "       WHERE DS.ANNUALIZED_RATE IS NOT NULL\n",
    "       GROUP BY DS.COUPON_TYPE\n",
    "   )\n",
    "   SELECT COUPON_TYPE, Avg_Annualized_Rate\n",
    "   FROM RateAverages\n",
    "   ORDER BY Avg_Annualized_Rate DESC;\n",
    "\n",
    "8. \"Get funds that have experienced a net decrease in assets over the last three reporting periods.\"\n",
    "   SQL: \n",
    "   WITH AssetChanges AS (\n",
    "       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\n",
    "              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\n",
    "       FROM FUND_REPORTED_INFO F\n",
    "       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT DISTINCT AC.SERIES_NAME\n",
    "   FROM AssetChanges AC\n",
    "   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\n",
    "     AND AC.Previous_Period_Assets IS NOT NULL;\n",
    "\n",
    "9. \"Identify issuers with more than three different securities holdings, including their names and CUSIPs.\"\n",
    "   SQL: \n",
    "   WITH IssuerHoldings AS (\n",
    "       SELECT H.ISSUER_NAME, H.ISSUER_CUSIP, COUNT(DISTINCT H.HOLDING_ID) AS Holding_Count\n",
    "       FROM FUND_REPORTED_HOLDING H\n",
    "       GROUP BY H.ISSUER_NAME, H.ISSUER_CUSIP\n",
    "       HAVING COUNT(DISTINCT H.HOLDING_ID) > 3\n",
    "   )\n",
    "   SELECT ISSUER_NAME, ISSUER_CUSIP, Holding_Count\n",
    "   FROM IssuerHoldings\n",
    "   ORDER BY Holding_Count DESC;\n",
    "\n",
    "10. \"Calculate the total notional amount of derivatives per currency and identify the top 3 currencies by notional amount.\"\n",
    "    SQL: \n",
    "    WITH NotionalSums AS (\n",
    "        SELECT ODNA.CURRENCY_CODE, SUM(ODNA.NOTIONAL_AMOUNT) AS Total_Notional\n",
    "        FROM OTHER_DERIV_NOTIONAL_AMOUNT ODNA\n",
    "        GROUP BY ODNA.CURRENCY_CODE\n",
    "    )\n",
    "    SELECT CURRENCY_CODE, Total_Notional\n",
    "    FROM NotionalSums\n",
    "    ORDER BY Total_Notional DESC\n",
    "    LIMIT 3;\n",
    "\n",
    "11. \"List funds with liquidation preferences exceeding their net assets.\"\n",
    "    SQL: \n",
    "    WITH FundPreferences AS (\n",
    "        SELECT F.SERIES_NAME, F.LIQUIDATION_PREFERENCE, F.NET_ASSETS\n",
    "        FROM FUND_REPORTED_INFO F\n",
    "    )\n",
    "    SELECT SERIES_NAME, LIQUIDATION_PREFERENCE, NET_ASSETS\n",
    "    FROM FundPreferences\n",
    "    WHERE LIQUIDATION_PREFERENCE > NET_ASSETS;\n",
    "\n",
    "12. \"Find all convertible securities that are contingent and have a conversion ratio above 1.5.\"\n",
    "    SQL: \n",
    "    WITH ConvertibleCTE AS (\n",
    "        SELECT DS.HOLDING_ID, CSC.CONVERSION_RATIO\n",
    "        FROM DEBT_SECURITY DS\n",
    "        JOIN CONVERTIBLE_SECURITY_CURRENCY CSC ON DS.HOLDING_ID = CSC.HOLDING_ID\n",
    "        WHERE DS.IS_CONVTIBLE_CONTINGENT = 'Y' AND CSC.CONVERSION_RATIO > 1.5\n",
    "    )\n",
    "    SELECT HOLDING_ID, CONVERSION_RATIO\n",
    "    FROM ConvertibleCTE;\n",
    "\n",
    "13. \"Analyze the distribution of asset categories within the top 10 largest funds by total assets.\"\n",
    "    SQL: \n",
    "    WITH TopFunds AS (\n",
    "        SELECT SERIES_NAME, ACCESSION_NUMBER\n",
    "        FROM FUND_REPORTED_INFO\n",
    "        ORDER BY TOTAL_ASSETS DESC\n",
    "        LIMIT 10\n",
    "    ),\n",
    "    AssetDistribution AS (\n",
    "        SELECT H.ASSET_CAT, COUNT(*) AS Category_Count\n",
    "        FROM FUND_REPORTED_HOLDING H\n",
    "        JOIN TopFunds T ON H.ACCESSION_NUMBER = T.ACCESSION_NUMBER\n",
    "        GROUP BY H.ASSET_CAT\n",
    "    )\n",
    "    SELECT ASSET_CAT, Category_Count\n",
    "    FROM AssetDistribution\n",
    "    ORDER BY Category_Count DESC;\n",
    "    \n",
    "14. \"Find the top 10 funds with the highest average monthly returns in the past quarter.\"\n",
    "   SQL: \n",
    "   WITH AvgMonthlyReturn AS (\n",
    "       SELECT ACCESSION_NUMBER, \n",
    "              (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\n",
    "       FROM MONTHLY_TOTAL_RETURN\n",
    "   )\n",
    "   SELECT F.SERIES_NAME, A.ACCESSION_NUMBER, A.Avg_Return\n",
    "   FROM AvgMonthlyReturn A\n",
    "   JOIN FUND_REPORTED_INFO F ON A.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "   ORDER BY A.Avg_Return DESC\n",
    "   LIMIT 10;\n",
    "\n",
    "15. \"Compare the latest net asset values of the top 5 performing funds.\"\n",
    "    SQL: \n",
    "    WITH TopPerformingFunds AS (\n",
    "        SELECT \n",
    "            ACCESSION_NUMBER, \n",
    "            (MONTHLY_TOTAL_RETURN1 + MONTHLY_TOTAL_RETURN2 + MONTHLY_TOTAL_RETURN3) / 3.0 AS Avg_Return\n",
    "        FROM \n",
    "            MONTHLY_TOTAL_RETURN\n",
    "        ORDER BY \n",
    "            Avg_Return DESC\n",
    "        LIMIT 5\n",
    "    )\n",
    "    SELECT \n",
    "        FR.SERIES_NAME, \n",
    "        FR.NET_ASSETS, \n",
    "        TP.Avg_Return\n",
    "    FROM \n",
    "        TopPerformingFunds TP\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO FR ON TP.ACCESSION_NUMBER = FR.ACCESSION_NUMBER;\n",
    "\n",
    "16. \"Calculate the overall average return across all funds for the most recent month.\"\n",
    "    SQL: \n",
    "    WITH LatestReturns AS (\n",
    "        SELECT \n",
    "            M.ACCESSION_NUMBER, \n",
    "            M.MONTHLY_TOTAL_RETURN1\n",
    "        FROM \n",
    "            MONTHLY_TOTAL_RETURN M\n",
    "        JOIN \n",
    "            SUBMISSION S ON M.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "        WHERE \n",
    "            S.REPORT_DATE = (SELECT MAX(REPORT_DATE) FROM SUBMISSION)\n",
    "    )\n",
    "    SELECT \n",
    "        AVG(MONTHLY_TOTAL_RETURN1) AS Average_Return\n",
    "    FROM \n",
    "        LatestReturns;\n",
    "\n",
    "17. \"Find the interest rate risk for each fund and identify those with the highest risk scores.\"\n",
    "    SQL: \n",
    "    WITH InterestRiskScores AS (\n",
    "        SELECT \n",
    "            IR.ACCESSION_NUMBER, \n",
    "            -- Calculating composite risk score by summing absolute values of DV01 and DV100 columns\n",
    "            (ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV01 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_3MON_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_1YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_5YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_10YR_DV100 AS FLOAT)) +\n",
    "            ABS(CAST(IR.INTRST_RATE_CHANGE_30YR_DV100 AS FLOAT))\n",
    "            ) AS Composite_Risk_Score\n",
    "        FROM \n",
    "            INTEREST_RATE_RISK IR\n",
    "    )\n",
    "    SELECT \n",
    "        FR.SERIES_NAME, \n",
    "        FR.ACCESSION_NUMBER, \n",
    "        IRS.Composite_Risk_Score\n",
    "    FROM \n",
    "        InterestRiskScores IRS\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO FR ON IRS.ACCESSION_NUMBER = FR.ACCESSION_NUMBER\n",
    "    ORDER BY \n",
    "        IRS.Composite_Risk_Score DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "18. \"Analyze the composition of fund portfolios by categorizing assets and their total values.\"\n",
    "    SQL: \n",
    "    WITH PortfolioComposition AS (\n",
    "    SELECT \n",
    "        ACCESSION_NUMBER, \n",
    "        ASSET_CAT, \n",
    "        SUM(CAST(CURRENCY_VALUE AS FLOAT)) AS Total_Value\n",
    "    FROM \n",
    "        FUND_REPORTED_HOLDING\n",
    "    GROUP BY \n",
    "        ACCESSION_NUMBER, \n",
    "        ASSET_CAT\n",
    "    )\n",
    "    SELECT \n",
    "        F.SERIES_NAME, \n",
    "        PC.ASSET_CAT, \n",
    "        PC.Total_Value\n",
    "    FROM \n",
    "        PortfolioComposition PC\n",
    "    JOIN \n",
    "        FUND_REPORTED_INFO F ON PC.ACCESSION_NUMBER = F.ACCESSION_NUMBER\n",
    "    ORDER BY \n",
    "        F.SERIES_NAME, \n",
    "        PC.Total_Value DESC;\n",
    "\n",
    "19. \"Identify the most common asset categories across all fund portfolios.\"\n",
    "    SQL: \n",
    "    WITH AssetCounts AS (\n",
    "        SELECT ASSET_CAT, COUNT(*) AS Count\n",
    "        FROM FUND_REPORTED_HOLDING\n",
    "        GROUP BY ASSET_CAT\n",
    "    )\n",
    "    SELECT ASSET_CAT, Count\n",
    "    FROM AssetCounts\n",
    "    ORDER BY Count DESC\n",
    "    LIMIT 5;\n",
    "\n",
    "20. \"Retrieve funds that have experienced a net decrease in assets over the last three reporting periods.\"\n",
    "   SQL: \n",
    "   WITH AssetChanges AS (\n",
    "       SELECT F.ACCESSION_NUMBER, F.SERIES_NAME, S.REPORT_DATE, F.NET_ASSETS,\n",
    "              LAG(F.NET_ASSETS, 1) OVER (PARTITION BY F.SERIES_NAME ORDER BY S.REPORT_DATE) AS Previous_Period_Assets\n",
    "       FROM FUND_REPORTED_Iimport os\n",
    "print(os.getcwd())NFO F\n",
    "       JOIN SUBMISSION S ON F.ACCESSION_NUMBER = S.ACCESSION_NUMBER\n",
    "   )\n",
    "   SELECT DISTINCT AC.SERIES_NAME\n",
    "   FROM AssetChanges AC\n",
    "   WHERE AC.NET_ASSETS < AC.Previous_Period_Assets\n",
    "     AND AC.Previous_Period_Assets IS NOT NULL;\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\"\\s*(.*?)\\s*\"\\s*SQL:\\s*(WITH.*?;)(?=\\n\\s*\\d+|$)'\n",
    "matches = re.findall(pattern, text, re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_query = [match[1] for match in matches]\n",
    "llm_query = [match[0] for match in matches]\n",
    "print(\"Queries:\", ground_truth_query)\n",
    "print(\"SQL Statements:\", llm_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "while len(results) < 20:\n",
    "    results.append(None)\n",
    "print(len(results))\n",
    "print(len(llm_query))\n",
    "\n",
    "print(\"Are the following queries the same?\")\n",
    "i=0\n",
    "print(\"i = \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i<len(llm_query):\n",
    "    results[i]=(str(i)+'. '+ str(compare_csv.compare_csv_din(ground_truth_query[i],llm_query[i])))\n",
    "i+=1 #1\n",
    "print(\"i = \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\\nYour task is to analyze questions about financial data and extract key components that will help in database queries.\\n\\nObjective: Break down the given question into essential components that will help formulate a database query.\\n\\nInstructions:\\n1. Read the question carefully to identify:\\n- Individual keywords that map to database columns or values\\n- Technical terms related to financial data\\n- Named entities (companies, funds, locations)\\n- Numerical thresholds or values\\n\\n2. For each identified element, categorize it as:\\n- keywords: Individual significant words that might match database columns\\n- keyphrases: Multi-word expressions that represent single concepts\\n- named_entities: Specific names of companies, funds, or locations\\n- numerical_values: Any numbers, amounts, or thresholds\\n\\n3. Return a JSON object with these categories.'}, {'role': 'user', 'content': '\\nExample Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\\n{\\n    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\\n    \"keyphrases\": [\"PIMCO funds\"],\\n    \"named_entities\": [\"PIMCO\", \"California\"],\\n    \"numerical_values\": [\"2020\", \"2023\"]\\n}\\n\\nExample Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\\n{\\n    \"keywords\": [\"funds\", \"assets\", \"managed\"],\\n    \"keyphrases\": [\"total assets\"],\\n    \"named_entities\": [\"BlackRock\", \"New York\"],\\n    \"numerical_values\": [\"1 billion\"]\\n}\\nQuestion: \"Show me all funds with total asset over 1 billion\"\\n\\nExtract the key components and return as JSON.'}], 'model': 'gpt-4o', 'response_format': {'type': 'json_object'}}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1772946d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1772b2cc0> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x177294a50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading schema file: chatgpt_api/schema.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 21:43:11 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'781'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999576'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'12ms'), (b'x-request-id', b'req_207352607ee62fbe93be52f59fb2c7a0'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=LnFqub9I4MYwyU1K0aXt7x9SRvjHDA58Wf0ukGNPUj8-1732138991-1.0.1.1-efTD1Pze4.Emk4DFVF0j9kDyPTV.OsZZfrRcy6JKjseQtDNR6u9bTY6Kq.2QzTTrBDcuTnmLTbretoQ23RParw; path=/; expires=Wed, 20-Nov-24 22:13:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=tPZfiykpsFPZqDe14kmHX.S7kyDHcyUYQv3Nk7uaqZE-1732138991604-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5b9d301a2514e2-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 20 Nov 2024 21:43:11 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-annfuni26pdtuawdwdj6zorw'), ('openai-processing-ms', '781'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '2000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '1999576'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '12ms'), ('x-request-id', 'req_207352607ee62fbe93be52f59fb2c7a0'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=LnFqub9I4MYwyU1K0aXt7x9SRvjHDA58Wf0ukGNPUj8-1732138991-1.0.1.1-efTD1Pze4.Emk4DFVF0j9kDyPTV.OsZZfrRcy6JKjseQtDNR6u9bTY6Kq.2QzTTrBDcuTnmLTbretoQ23RParw; path=/; expires=Wed, 20-Nov-24 22:13:11 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=tPZfiykpsFPZqDe14kmHX.S7kyDHcyUYQv3Nk7uaqZE-1732138991604-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8e5b9d301a2514e2-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_207352607ee62fbe93be52f59fb2c7a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Finding matches for 'fund'\n",
      "Found 279 matches for 'fund':\n",
      "  securities_lending.is_loan_by_fund: 1.0000\n",
      "  registrant.accession_number: 0.9000\n",
      "  registrant.cik: 0.9000\n",
      "  registrant.registrant_name: 0.9000\n",
      "  registrant.file_num: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for 'asset'\n",
      "Found 7 matches for 'asset':\n",
      "  fund_reported_holding.asset_cat: 1.0000\n",
      "  monthly_return_cat_instrument.asset_cat: 1.0000\n",
      "  fund_reported_info.total_assets: 0.9000\n",
      "  fund_reported_info.net_assets: 0.9000\n",
      "  fund_reported_info.assets_attrbt_to_misc_security: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for '1'\n",
      "Found 0 matches for '1':\n",
      "\n",
      "DEBUG: Finding matches for 'billion'\n",
      "Found 2 matches for 'billion':\n",
      "  submission.filing_date: 0.6154\n",
      "  submission.is_last_filing: 0.6154\n",
      "\n",
      "DEBUG: Finding matches for 'total'\n",
      "Found 279 matches for 'total':\n",
      "  fund_reported_info.total_assets: 1.0000\n",
      "  fund_reported_info.total_liabilities: 1.0000\n",
      "  monthly_total_return.monthly_total_return_id: 1.0000\n",
      "  monthly_total_return.monthly_total_return1: 1.0000\n",
      "  monthly_total_return.monthly_total_return2: 1.0000\n",
      "{'extracted_info': {'keyphrases': ['total asset'],\n",
      "                    'keywords': ['funds', 'asset'],\n",
      "                    'named_entities': [],\n",
      "                    'numerical_values': ['1 billion']},\n",
      " 'processed_words': ['fund', 'asset', '1', 'billion', 'total'],\n",
      " 'question': 'Show me all funds with total asset over 1 billion',\n",
      " 'schema_relationships': {'foreign_keys': ['REGISTRANT.ACCESSION_NUMBER = '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'INTEREST_RATE_RISK.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'BORROWER.ACCESSION_NUMBER = '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'BORROW_AGGREGATE.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'FUND_VAR_INFO.ACCESSION_NUMBER = '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'FUND_REPORTED_HOLDING.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'EXPLANATORY_NOTE.ACCESSION_NUMBER '\n",
      "                                           '= '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'SUBMISSION.ACCESSION_NUMBER = '\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'IDENTIFIERS.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DEBT_SECURITY.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'REPURCHASE_AGREEMENT.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'REPURCHASE_COUNTERPARTY.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'REPURCHASE_COLLATERAL.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DERIVATIVE_COUNTERPARTY.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DESC_REF_INDEX_BASKET.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DESC_REF_INDEX_COMPONENT.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'DESC_REF_OTHER.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'FLOATING_RATE_RESET_TENOR.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'OTHER_DERIV.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID '\n",
      "                                           '= FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'SECURITIES_LENDING.HOLDING_ID = '\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID'],\n",
      "                          'primary_keys': ['SUBMISSION.ACCESSION_NUMBER',\n",
      "                                           'REGISTRANT.ACCESSION_NUMBER',\n",
      "                                           'FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
      "                                           'INTEREST_RATE_RISK.ACCESSION_NUMBER',\n",
      "                                           'INTEREST_RATE_RISK.INTEREST_RATE_RISK_ID',\n",
      "                                           'BORROWER.ACCESSION_NUMBER',\n",
      "                                           'BORROWER.BORROWER_ID',\n",
      "                                           'BORROW_AGGREGATE.ACCESSION_NUMBER',\n",
      "                                           'BORROW_AGGREGATE.BORROW_AGGREGATE_ID',\n",
      "                                           'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER',\n",
      "                                           'MONTHLY_TOTAL_RETURN.MONTHLY_TOTAL_RETURN_ID',\n",
      "                                           'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER',\n",
      "                                           'MONTHLY_RETURN_CAT_INSTRUMENT.ASSET_CAT',\n",
      "                                           'MONTHLY_RETURN_CAT_INSTRUMENT.INSTRUMENT_KIND',\n",
      "                                           'FUND_VAR_INFO.ACCESSION_NUMBER',\n",
      "                                           'FUND_REPORTED_HOLDING.ACCESSION_NUMBER',\n",
      "                                           'FUND_REPORTED_HOLDING.HOLDING_ID',\n",
      "                                           'IDENTIFIERS.HOLDING_ID',\n",
      "                                           'IDENTIFIERS.IDENTIFIERS_ID',\n",
      "                                           'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID',\n",
      "                                           'DEBT_SECURITY_REF_INSTRUMENT.DEBT_SECURITY_REF_ID',\n",
      "                                           'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID',\n",
      "                                           'CONVERTIBLE_SECURITY_CURRENCY.CONVERTIBLE_SECURITY_ID',\n",
      "                                           'REPURCHASE_AGREEMENT.HOLDING_ID',\n",
      "                                           'REPURCHASE_COUNTERPARTY.HOLDING_ID',\n",
      "                                           'REPURCHASE_COUNTERPARTY.REPURCHASE_COUNTERPARTY_ID',\n",
      "                                           'REPURCHASE_COLLATERAL.HOLDING_ID',\n",
      "                                           'REPURCHASE_COLLATERAL.REPURCHASE_COLLATERAL_ID',\n",
      "                                           'DERIVATIVE_COUNTERPARTY.HOLDING_ID',\n",
      "                                           'DERIVATIVE_COUNTERPARTY.DERIVATIVE_COUNTERPARTY_ID',\n",
      "                                           'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID',\n",
      "                                           'DESC_REF_INDEX_BASKET.HOLDING_ID',\n",
      "                                           'DESC_REF_INDEX_COMPONENT.HOLDING_ID',\n",
      "                                           'DESC_REF_INDEX_COMPONENT.DESC_REF_INDEX_COMPONENT_ID',\n",
      "                                           'DESC_REF_OTHER.HOLDING_ID',\n",
      "                                           'DESC_REF_OTHER.DESC_REF_OTHER_ID',\n",
      "                                           'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID',\n",
      "                                           'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID',\n",
      "                                           'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID',\n",
      "                                           'FLOATING_RATE_RESET_TENOR.HOLDING_ID',\n",
      "                                           'FLOATING_RATE_RESET_TENOR.RATE_RESET_TENOR_ID',\n",
      "                                           'OTHER_DERIV.HOLDING_ID',\n",
      "                                           'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID',\n",
      "                                           'OTHER_DERIV_NOTIONAL_AMOUNT.OTHER_DERIV_NOTIONAL_AMOUNT_ID',\n",
      "                                           'SECURITIES_LENDING.HOLDING_ID',\n",
      "                                           'EXPLANATORY_NOTE.ACCESSION_NUMBER',\n",
      "                                           'EXPLANATORY_NOTE.EXPLANATORY_NOTE_ID']},\n",
      " 'similar_matches': {'1': [],\n",
      "                     'asset': [('fund_reported_holding.asset_cat', 1.0),\n",
      "                               ('monthly_return_cat_instrument.asset_cat', 1.0),\n",
      "                               ('fund_reported_info.total_assets', 0.9),\n",
      "                               ('fund_reported_info.net_assets', 0.9),\n",
      "                               ('fund_reported_info.assets_attrbt_to_misc_security',\n",
      "                                0.9)],\n",
      "                     'billion': [('submission.filing_date', 0.6153846153846154),\n",
      "                                 ('submission.is_last_filing',\n",
      "                                  0.6153846153846154)],\n",
      "                     'fund': [('securities_lending.is_loan_by_fund', 1.0),\n",
      "                              ('registrant.accession_number', 0.9),\n",
      "                              ('registrant.cik', 0.9),\n",
      "                              ('registrant.registrant_name', 0.9),\n",
      "                              ('registrant.file_num', 0.9)],\n",
      "                     'total': [('fund_reported_info.total_assets', 1.0),\n",
      "                               ('fund_reported_info.total_liabilities', 1.0),\n",
      "                               ('monthly_total_return.monthly_total_return_id',\n",
      "                                1.0),\n",
      "                               ('monthly_total_return.monthly_total_return1',\n",
      "                                1.0),\n",
      "                               ('monthly_total_return.monthly_total_return2',\n",
      "                                1.0)]}}\n"
     ]
    }
   ],
   "source": [
    "class PSLsh:\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class ValueRetrieval:\n",
    "    financial_terms = {\n",
    "            'total': ['total', 'sum', 'aggregate', 'combined'],\n",
    "            'assets': ['asset', 'holdings', 'investments', 'securities'],\n",
    "            'liabilities': ['liability', 'debt', 'obligations'],\n",
    "            'net': ['net', 'pure', 'adjusted'],\n",
    "            'fund': ['fund', 'portfolio', 'investment vehicle'],\n",
    "            'return': ['return', 'yield', 'profit', 'gain'],\n",
    "            'monthly': ['monthly', 'month', 'monthly basis'],\n",
    "            'rate': ['rate', 'percentage', 'ratio'],\n",
    "            'risk': ['risk', 'exposure', 'vulnerability']\n",
    "        }\n",
    "    def __init__(self, schema_path: str = 'chatgpt_api/schema.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load schema\n",
    "        print(\"DEBUG: Loading schema file:\", schema_path)\n",
    "        with open(schema_path, 'r') as f:\n",
    "            self.schema = json.load(f)\n",
    "\n",
    "        # Initialize lemmatizer and stop words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Build column name index\n",
    "        self.column_index = self._build_column_index()\n",
    "        \n",
    "        # Build common financial terms dictionary\n",
    "        \n",
    "\n",
    "        # Build vectorizer and LSH for backup matching\n",
    "        self.build_vectorizer_and_lsh(seed=lsh_seed)\n",
    "        \n",
    "        # Get schema relationships\n",
    "        self.primary_keys, self.foreign_keys = self.discover_schema_relationships()\n",
    "\n",
    "    def _build_column_index(self) -> Dict:\n",
    "        \"\"\"Build an index of all columns with their metadata.\"\"\"\n",
    "        column_index = {}\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        \n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                \n",
    "                # Store both the full qualified name and column properties\n",
    "                qualified_name = f\"{table_name}.{column_name}\"\n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': column_name,\n",
    "                    'type': column.get('type', ''),\n",
    "                    'words': self._split_column_name(column_name),\n",
    "                    'synonyms': self._get_column_synonyms(column_name)\n",
    "                }\n",
    "                \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        # Handle both underscore and camel case\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        synonyms = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.financial_terms:\n",
    "                synonyms.extend(self.financial_terms[word])\n",
    "                \n",
    "        return list(set(synonyms))\n",
    "\n",
    "    def build_vectorizer_and_lsh(self, seed: int):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "        self.term_list = self.get_schema_terms()\n",
    "        self.term_vectors = self.vectorizer.fit_transform(self.term_list)\n",
    "        self.lsh = PSLsh(self.term_vectors, n_planes=10, n_tables=5)\n",
    "\n",
    "    def get_schema_terms(self) -> List[str]:\n",
    "        terms = []\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            terms.append(table_name)\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                terms.append(f\"{table_name}.{column_name}\")\n",
    "        return terms\n",
    "\n",
    "    def discover_schema_relationships(self):\n",
    "        # Define our primary keys and foreign keys here\n",
    "        primary_keys = {\n",
    "            'SUBMISSION': ['ACCESSION_NUMBER'],\n",
    "            'REGISTRANT': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_INFO': ['ACCESSION_NUMBER'],\n",
    "            'INTEREST_RATE_RISK': ['ACCESSION_NUMBER', 'INTEREST_RATE_RISK_ID'],\n",
    "            'BORROWER': ['ACCESSION_NUMBER', 'BORROWER_ID'],\n",
    "            'BORROW_AGGREGATE': ['ACCESSION_NUMBER', 'BORROW_AGGREGATE_ID'],\n",
    "            'MONTHLY_TOTAL_RETURN': ['ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN_ID'],\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT': ['ACCESSION_NUMBER', 'ASSET_CAT', 'INSTRUMENT_KIND'],\n",
    "            'FUND_VAR_INFO': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_HOLDING': ['ACCESSION_NUMBER', 'HOLDING_ID'],\n",
    "            'IDENTIFIERS': ['HOLDING_ID', 'IDENTIFIERS_ID'],\n",
    "            'DEBT_SECURITY': [],  \n",
    "            'DEBT_SECURITY_REF_INSTRUMENT': ['HOLDING_ID', 'DEBT_SECURITY_REF_ID'],\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY': ['HOLDING_ID', 'CONVERTIBLE_SECURITY_ID'],\n",
    "            'REPURCHASE_AGREEMENT': ['HOLDING_ID'],\n",
    "            'REPURCHASE_COUNTERPARTY': ['HOLDING_ID', 'REPURCHASE_COUNTERPARTY_ID'],\n",
    "            'REPURCHASE_COLLATERAL': ['HOLDING_ID', 'REPURCHASE_COLLATERAL_ID'],\n",
    "            'DERIVATIVE_COUNTERPARTY': ['HOLDING_ID', 'DERIVATIVE_COUNTERPARTY_ID'],\n",
    "            'SWAPTION_OPTION_WARNT_DERIV': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_BASKET': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_COMPONENT': ['HOLDING_ID', 'DESC_REF_INDEX_COMPONENT_ID'],\n",
    "            'DESC_REF_OTHER': ['HOLDING_ID', 'DESC_REF_OTHER_ID'],\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT': ['HOLDING_ID'],\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP': ['HOLDING_ID'],\n",
    "            'NONFOREIGN_EXCHANGE_SWAP': ['HOLDING_ID'],\n",
    "            'FLOATING_RATE_RESET_TENOR': ['HOLDING_ID', 'RATE_RESET_TENOR_ID'],\n",
    "            'OTHER_DERIV': ['HOLDING_ID'],\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT': ['HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT_ID'],\n",
    "            'SECURITIES_LENDING': ['HOLDING_ID'],\n",
    "            'EXPLANATORY_NOTE': ['ACCESSION_NUMBER', 'EXPLANATORY_NOTE_ID']\n",
    "        }\n",
    "\n",
    "        foreign_keys = [\n",
    "            # ACCESSION_NUMBER relationships\n",
    "            'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "\n",
    "            # HOLDING_ID relationships\n",
    "            'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID'\n",
    "        ]\n",
    "\n",
    "        formatted_pks = []\n",
    "        for table, keys in primary_keys.items():\n",
    "            for key in keys:\n",
    "                formatted_pks.append(f\"{table}.{key}\")\n",
    "\n",
    "        return formatted_pks, foreign_keys\n",
    "\n",
    "    def find_similar_words(self, word: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Enhanced matching using multiple techniques.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        print(f\"\\nDEBUG: Finding matches for '{word}'\")\n",
    "        \n",
    "        matches = []\n",
    "        \n",
    "        # 1. Direct matching with column names and their components\n",
    "        for qualified_name, metadata in self.column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "                \n",
    "            # Check synonyms\n",
    "            if word in self.financial_terms.get(word, []):\n",
    "                matches.append((qualified_name, 0.9))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            # Fuzzy match with synonyms\n",
    "            for term, synonyms in self.financial_terms.items():\n",
    "                if term in metadata['words']:\n",
    "                    for synonym in synonyms:\n",
    "                        ratio = fuzz.ratio(word, synonym) / 100.0\n",
    "                        if ratio > score:\n",
    "                            score = ratio * 0.9  # Slightly lower weight for synonym matches\n",
    "            \n",
    "            if score > 0.6:  # Only include if similarity is above 60%\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # 2. LSH-based matching as backup\n",
    "        if len(matches) < 5:  # If we have fewer than 5 matches, try LSH\n",
    "            try:\n",
    "                word_vector = self.vectorizer.transform([word]).toarray()[0]\n",
    "                candidate_indices = self.lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = self.term_list[idx]\n",
    "                    if not any(term == m[0] for m in matches):  # Avoid duplicates\n",
    "                        candidate_vector = self.term_vectors[idx].toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:  # Only include if similarity is above 50%\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        # Remove duplicates keeping highest score and sort by score\n",
    "        unique_matches = {}\n",
    "        for term, score in matches:\n",
    "            if term not in unique_matches or score > unique_matches[term]:\n",
    "                unique_matches[term] = score\n",
    "        \n",
    "        matches = [(term, score) for term, score in unique_matches.items()]\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"Found {len(matches)} matches for '{word}':\")\n",
    "        for match, score in matches[:5]:\n",
    "            print(f\"  {match}: {score:.4f}\")\n",
    "        \n",
    "        return matches[:5] if matches else [('fund_reported_info.total_assets', 0.6)] if word in ['total', 'asset', 'assets'] else []\n",
    "\n",
    "    def extract_keywords(self, question: str) -> Dict:\n",
    "        system_prompt = \"\"\"You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\n",
    "Your task is to analyze questions about financial data and extract key components that will help in database queries.\n",
    "\n",
    "Objective: Break down the given question into essential components that will help formulate a database query.\n",
    "\n",
    "Instructions:\n",
    "1. Read the question carefully to identify:\n",
    "- Individual keywords that map to database columns or values\n",
    "- Technical terms related to financial data\n",
    "- Named entities (companies, funds, locations)\n",
    "- Numerical thresholds or values\n",
    "\n",
    "2. For each identified element, categorize it as:\n",
    "- keywords: Individual significant words that might match database columns\n",
    "- keyphrases: Multi-word expressions that represent single concepts\n",
    "- named_entities: Specific names of companies, funds, or locations\n",
    "- numerical_values: Any numbers, amounts, or thresholds\n",
    "\n",
    "3. Return a JSON object with these categories.\"\"\"\n",
    "\n",
    "        few_shot_examples = \"\"\"\n",
    "Example Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\n",
    "{\n",
    "    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\n",
    "    \"keyphrases\": [\"PIMCO funds\"],\n",
    "    \"named_entities\": [\"PIMCO\", \"California\"],\n",
    "    \"numerical_values\": [\"2020\", \"2023\"]\n",
    "}\n",
    "\n",
    "Example Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\n",
    "{\n",
    "    \"keywords\": [\"funds\", \"assets\", \"managed\"],\n",
    "    \"keyphrases\": [\"total assets\"],\n",
    "    \"named_entities\": [\"BlackRock\", \"New York\"],\n",
    "    \"numerical_values\": [\"1 billion\"]\n",
    "}\"\"\"\n",
    "\n",
    "        formatted_prompt = system_prompt\n",
    "        user_prompt = f\"Question: \\\"{question}\\\"\\n\\nExtract the key components and return as JSON.\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": formatted_prompt},\n",
    "                {\"role\": \"user\", \"content\": few_shot_examples + \"\\n\" + user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "        lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "        return lemmatized_tokens\n",
    "    \n",
    "    def process_question(self, question: str) -> Dict:\n",
    "        # Extract keywords using gpt\n",
    "        extracted_info = self.extract_keywords(question)\n",
    "\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'named_entities', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "\n",
    "        # Preprocess the words (lemmatize, remove stop words)\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "\n",
    "        # Remove duplicates\n",
    "        processed_words = list(set(processed_words))\n",
    "\n",
    "        # Find similar columns for each word\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self.find_similar_words(word)\n",
    "\n",
    "        # Combine the results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self.primary_keys,\n",
    "                \"foreign_keys\": self.foreign_keys\n",
    "            }\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vr = ValueRetrieval(schema_path='chatgpt_api/schema.json')\n",
    "\n",
    "    question = \"Show me all funds with total asset over 1 billion\"\n",
    "    result = vr.process_question(question)\n",
    "\n",
    "    from pprint import pprint\n",
    "    pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ CLASSIFICATION\n",
    "classification_prompt = '''Q: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\n",
    "schema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\n",
    "A: Lets think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Get the names and CIK of registrants who are located in California.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\n",
    "A: Lets think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\n",
    "A: Let's analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\n",
    "Label: \"NESTED\"\n",
    "\n",
    "'''\n",
    "\n",
    "def classification_prompt_maker(question,relevant_schema_links):\n",
    "  instruction = \"# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\"\n",
    "  instruction += \"\\nif need nested queries: predict NESTED\\n\"\n",
    "  instruction += \"elif need JOIN and don't need nested queries: predict NON-NESTED\\n\"\n",
    "  instruction += \"elif don't need JOIN and don't need nested queries: predict EASY\\n\\n\"\n",
    "  prompt = instruction + classification_prompt + 'Q: \"' + question + '\\nrelevant_schema_links: ' + relevant_schema_links + '\\nA: Lets think step by step.'\n",
    "  return prompt\n",
    "\n",
    "def process_question_classification(question, relevant_schema_links):\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            print(\"Attempting classification...\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(question, relevant_schema_links=relevant_schema_links)\n",
    "                }],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                temperature=0.0,\n",
    "                max_tokens=300,  # Reduced max tokens\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "                # Removed the stop sequence\n",
    "            )\n",
    "            classification = response.choices[0].message.content\n",
    "            print(\"Raw response:\", classification)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    if classification is None:\n",
    "        print(\"Failed to get classification after 3 attempts\")\n",
    "        return '\"NESTED\"'\n",
    "        \n",
    "    try:\n",
    "        predicted_class = classification.split(\"Label: \")[1].strip().strip('\"')\n",
    "        return f'\"{predicted_class}\"'\n",
    "    except:\n",
    "        print(\"Slicing error for the classification module\")\n",
    "        return '\"NESTED\"'\n",
    "\n",
    "\n",
    "############################################ SQL GENERATION\n",
    "easy_prompt = '''Q: \"Find the issuers with a balance greater than 1 million.\"\n",
    "Schema_links: [fund_reported_holding.balance]\n",
    "SQL: SELECT DISTINCT issuer_name \n",
    "      FROM fund_reported_holding \n",
    "      WHERE balance > 1000000\n",
    "'''\n",
    "\n",
    "medium_prompt = '''Q: \"Find the total upfront payments and receipts for swaps with fixed rate receipts.\"\n",
    "Schema_links: [nonforeign_exchange_swap.upfront_payment, nonforeign_exchange_swap.upfront_receipt, nonforeign_exchange_swap.fixed_rate_receipt]\n",
    "A: Lets think step by step. For creating the SQL for the given question, we need to filter the swaps that have fixed rate receipts. Then, sum up the upfront payments and receipts. First, create an intermediate representation, then use it to construct the SQL query.\n",
    "Intermediate_representation: \n",
    "SELECT SUM(nonforeign_exchange_swap.upfront_payment) + SUM(nonforeign_exchange_swap.upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE nonforeign_exchange_swap.fixed_rate_receipt IS NOT NULL\n",
    "SQL: \n",
    "SELECT SUM(upfront_payment) + SUM(upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE fixed_rate_receipt IS NOT NULL\n",
    "'''\n",
    "\n",
    "hard_prompt = '''Q: \"Find the borrowers with aggregate value greater than $1 million and whose interest rate change at 10-year maturity for a 100 basis point change is positive.\"\n",
    "Schema_links: [borrower.aggregate_value, borrower.name, interest_rate_risk.intrst_rate_change_10yr_dv100]\n",
    "A: Let's think step by step. First, we need to filter borrowers with aggregate values greater than $1 million. Then, we need to check for interest rate changes at 10-year maturity where the change is positive. \n",
    "The SQL query for the sub-question \"What are the borrowers with aggregate value greater than $1 million and positive interest rate change at 10-year maturity for 100 basis points?\" is:\n",
    "\n",
    "Intermediate_representation: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0\n",
    "\n",
    "SQL: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0\n",
    "'''\n",
    "\n",
    "def hard_prompt_maker(question,database,schema_links,sub_questions=\"\"):\n",
    "    instruction = \"# Use the intermediate representation and the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "    if sub_questions==\"\":\n",
    "        stepping = f'''\\nA: Let's think step by step. \"{question}\" can be solved by first solving a sub-question using nested queries\".'''\n",
    "    else:\n",
    "        stepping = f'''\\nA: Let's think step by step. \"{question}\" can be solved by first solving the answer to the following sub-question \"{sub_questions}\".'''\n",
    "    prompt = instruction + hard_prompt+ chat_prompt.gpt_queries_hard+ 'Q: \"' + question + '\"' + '\\nschema_links: ' + schema_links + stepping +'\\nThe SQL query for the sub-question:\"'\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question,database,schema_links):\n",
    "    instruction = \"# Use the the schema links and Intermediate_representation to generate the SQL queries for each of the questions.\\n\"\n",
    "    prompt = instruction + medium_prompt + chat_prompt.gpt_queries_medium+ 'Q: \"' + question + '\\nSchema_links: ' + schema_links + '\\nA: Lets think step by step.'\n",
    "    return prompt\n",
    "\n",
    "def easy_prompt_maker(question,database,schema_links):\n",
    "    instruction = \"# Use the the schema links to generate the SQL queries for each of the questions.\\n\"\n",
    "    prompt = instruction + easy_prompt + chat_prompt.gpt_queries_easy + 'Q: \"' + question + '\\nSchema_links: ' + schema_links + '\\nSQL:'\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\\nif need nested queries: predict NESTED\\nelif need JOIN and don\\'t need nested queries: predict NON-NESTED\\nelif don\\'t need JOIN and don\\'t need nested queries: predict EASY\\n\\nQ: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\\nschema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\\nA: Lets think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Get the names and CIK of registrants who are located in California.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\\nA: Lets think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\\nA: Let\\'s analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\\nLabel: \"NESTED\"\\n\\nQ: \"Show me all funds with total assets over 1 billion\\nrelevant_schema_links: [fund_reported_info.total_assets > 1000000000]\\nA: Lets think step by step.'}], 'model': 'gpt-4', 'frequency_penalty': 0.0, 'max_tokens': 300, 'n': 1, 'presence_penalty': 0.0, 'stream': False, 'temperature': 0.0, 'top_p': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x176a13010>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x176c42840> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1772ea810>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Prompt:\n",
      "# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\n",
      "\n",
      "if need nested queries: predict NESTED\n",
      "elif need JOIN and don't need nested queries: predict NON-NESTED\n",
      "elif don't need JOIN and don't need nested queries: predict EASY\n",
      "\n",
      "Q: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\n",
      "schema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\n",
      "A: Lets think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don't need JOIN.\n",
      "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
      "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
      "Label: \"EASY\"\n",
      "\n",
      "Q: \"Get the names and CIK of registrants who are located in California.\"\n",
      "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\n",
      "A: Lets think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don't need JOIN.\n",
      "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
      "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
      "Label: \"EASY\"\n",
      "\n",
      "Q: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\n",
      "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\n",
      "A: Let's analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\n",
      "Label: \"NESTED\"\n",
      "\n",
      "Q: \"Show me all funds with total assets over 1 billion\n",
      "relevant_schema_links: [fund_reported_info.total_assets > 1000000000]\n",
      "A: Lets think step by step.\n",
      "Attempting classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 21:43:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'4456'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'299124'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'175ms'), (b'x-request-id', b'req_856ba4a7c9db16a8228b22b117843b6f'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Qs2WG3DjpcT5UK.vHEhKkyOWRwFARjvTMpOtiEdPa94-1732139030-1.0.1.1-XrRkyOA_Cfkqb7ZHuan_t5V7EdZDaRJFRaJrHyE6NLH5CeDQQIxmGoM5Txs7NPjf7BsG5RReXG8CMfqTgT6Qkg; path=/; expires=Wed, 20-Nov-24 22:13:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=3GH_rmFRm9R1Kglx.wRpLDJnZkO3uVtwesKImOVniXw-1732139030006-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5b9e0ce9eb08f4-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 20 Nov 2024 21:43:50 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-annfuni26pdtuawdwdj6zorw'), ('openai-processing-ms', '4456'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '300000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '299124'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '175ms'), ('x-request-id', 'req_856ba4a7c9db16a8228b22b117843b6f'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=Qs2WG3DjpcT5UK.vHEhKkyOWRwFARjvTMpOtiEdPa94-1732139030-1.0.1.1-XrRkyOA_Cfkqb7ZHuan_t5V7EdZDaRJFRaJrHyE6NLH5CeDQQIxmGoM5Txs7NPjf7BsG5RReXG8CMfqTgT6Qkg; path=/; expires=Wed, 20-Nov-24 22:13:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=3GH_rmFRm9R1Kglx.wRpLDJnZkO3uVtwesKImOVniXw-1732139030006-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8e5b9e0ce9eb08f4-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_856ba4a7c9db16a8228b22b117843b6f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: The SQL query for the question \"Show me all funds with total assets over 1 billion\" needs these tables = [fund_reported_info], so we don't need JOIN.\n",
      "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
      "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
      "Label: \"EASY\"\n",
      "\n",
      "Classification Result:\n",
      "\"EASY\"\n"
     ]
    }
   ],
   "source": [
    "# Test classification directly\n",
    "test_question = \"Show me all funds with total assets over 1 billion\"\n",
    "test_schema_links = \"[fund_reported_info.total_assets > 1000000000]\"\n",
    "\n",
    "# Generate classification prompt\n",
    "prompt = classification_prompt_maker(test_question, test_schema_links)\n",
    "print(\"Classification Prompt:\")\n",
    "print(prompt)\n",
    "\n",
    "# Get classification\n",
    "classification = process_question_classification(test_question, test_schema_links)\n",
    "print(\"\\nClassification Result:\")\n",
    "print(classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/virounikamina/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an expert financial data analyst specializing in natural language understanding and database schema analysis.\\nYour task is to analyze questions about financial data and extract key components that will help in database queries.\\n\\nObjective: Break down the given question into essential components that will help formulate a database query.\\n\\nInstructions:\\n1. Read the question carefully to identify:\\n- Individual keywords that map to database columns or values\\n- Technical terms related to financial data\\n- Named entities (companies, funds, locations)\\n- Numerical thresholds or values\\n\\n2. For each identified element, categorize it as:\\n- keywords: Individual significant words that might match database columns\\n- keyphrases: Multi-word expressions that represent single concepts\\n- named_entities: Specific names of companies, funds, or locations\\n- numerical_values: Any numbers, amounts, or thresholds\\n\\n3. Return a JSON object with these categories.'}, {'role': 'user', 'content': '\\nExample Question: \"Which PIMCO funds were registered between 2020 and 2023 with California addresses?\"\\n{\\n    \"keywords\": [\"funds\", \"registered\", \"addresses\"],\\n    \"keyphrases\": [\"PIMCO funds\"],\\n    \"named_entities\": [\"PIMCO\", \"California\"],\\n    \"numerical_values\": [\"2020\", \"2023\"]\\n}\\n\\nExample Question: \"Show me BlackRock funds with total assets over 1 billion managed in New York\"\\n{\\n    \"keywords\": [\"funds\", \"assets\", \"managed\"],\\n    \"keyphrases\": [\"total assets\"],\\n    \"named_entities\": [\"BlackRock\", \"New York\"],\\n    \"numerical_values\": [\"1 billion\"]\\n}\\nQuestion: \"Show me all funds with total assets over 1 billion\"\\n\\nExtract the key components and return as JSON.'}], 'model': 'gpt-4o', 'response_format': {'type': 'json_object'}}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1681ee410>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x1772b2330> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1772ad790>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading schema file: chatgpt_api/schema.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 21:45:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'742'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999576'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'12ms'), (b'x-request-id', b'req_aee87e512475399559d01b7cd3689cb7'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=8lRpEkaHDlU77PcceZgrxRr4T5TY22Ox.6P5avQ.DnM-1732139132-1.0.1.1-9hF70tVhqHuxvuGFEmqDokb2XMBkl_SwG07ovNu5VjtZYKZq29H_.3_l__SYwJ5NQnRDECtyM22T60B7fmihvw; path=/; expires=Wed, 20-Nov-24 22:15:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=tVNiSpxt3DktWPBbVJgFAd98ktCHwSmjCjy2mqwwWXQ-1732139132494-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5ba0a49a1e2f4b-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 20 Nov 2024 21:45:32 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-annfuni26pdtuawdwdj6zorw'), ('openai-processing-ms', '742'), ('openai-version', '2020-10-01'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '2000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '1999576'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '12ms'), ('x-request-id', 'req_aee87e512475399559d01b7cd3689cb7'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=8lRpEkaHDlU77PcceZgrxRr4T5TY22Ox.6P5avQ.DnM-1732139132-1.0.1.1-9hF70tVhqHuxvuGFEmqDokb2XMBkl_SwG07ovNu5VjtZYKZq29H_.3_l__SYwJ5NQnRDECtyM22T60B7fmihvw; path=/; expires=Wed, 20-Nov-24 22:15:32 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=tVNiSpxt3DktWPBbVJgFAd98ktCHwSmjCjy2mqwwWXQ-1732139132494-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8e5ba0a49a1e2f4b-LAX'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "DEBUG:openai._base_client:request_id: req_aee87e512475399559d01b7cd3689cb7\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': '# For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN.\\n\\nif need nested queries: predict NESTED\\nelif need JOIN and don\\'t need nested queries: predict NON-NESTED\\nelif don\\'t need JOIN and don\\'t need nested queries: predict EASY\\n\\nQ: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\\nschema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\\nA: Lets think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Get the names and CIK of registrants who are located in California.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\\nA: Lets think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don\\'t need JOIN.\\nPlus, it doesn\\'t require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \\nSo, we don\\'t need JOIN and don\\'t need nested queries, then the SQL query can be classified as \"EASY\".\\nLabel: \"EASY\"\\n\\nQ: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\\nschema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\\nA: Let\\'s analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\\nLabel: \"NESTED\"\\n\\nQ: \"Show me all funds with total assets over 1 billion\\nrelevant_schema_links: [\\'securities_lending.is_loan_by_fund\\', \\'fund_reported_holding.asset_cat\\', \\'fund_reported_info.total_assets\\', \\'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\\', \\'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\', \\'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID\\']\\nA: Lets think step by step.'}], 'model': 'gpt-4', 'frequency_penalty': 0.0, 'max_tokens': 300, 'n': 1, 'presence_penalty': 0.0, 'stream': False, 'temperature': 0.0, 'top_p': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x176e4bb90>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x176c42840> server_hostname='api.openai.com' timeout=5.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x176e48990>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBUG: Finding matches for 'fund'\n",
      "Found 279 matches for 'fund':\n",
      "  securities_lending.is_loan_by_fund: 1.0000\n",
      "  registrant.accession_number: 0.9000\n",
      "  registrant.cik: 0.9000\n",
      "  registrant.registrant_name: 0.9000\n",
      "  registrant.file_num: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for 'asset'\n",
      "Found 7 matches for 'asset':\n",
      "  fund_reported_holding.asset_cat: 1.0000\n",
      "  monthly_return_cat_instrument.asset_cat: 1.0000\n",
      "  fund_reported_info.total_assets: 0.9000\n",
      "  fund_reported_info.net_assets: 0.9000\n",
      "  fund_reported_info.assets_attrbt_to_misc_security: 0.9000\n",
      "\n",
      "DEBUG: Finding matches for '1'\n",
      "Found 0 matches for '1':\n",
      "\n",
      "DEBUG: Finding matches for 'billion'\n",
      "Found 2 matches for 'billion':\n",
      "  submission.filing_date: 0.6154\n",
      "  submission.is_last_filing: 0.6154\n",
      "\n",
      "DEBUG: Finding matches for 'total'\n",
      "Found 279 matches for 'total':\n",
      "  fund_reported_info.total_assets: 1.0000\n",
      "  fund_reported_info.total_liabilities: 1.0000\n",
      "  monthly_total_return.monthly_total_return_id: 1.0000\n",
      "  monthly_total_return.monthly_total_return1: 1.0000\n",
      "  monthly_total_return.monthly_total_return2: 1.0000\n",
      "Filtered schema links: ['securities_lending.is_loan_by_fund', 'fund_reported_holding.asset_cat', 'fund_reported_info.total_assets', 'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER', 'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID', 'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID']\n",
      "\n",
      "Testing classification with filtered schema links...\n",
      "Attempting classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 20 Nov 2024 21:45:38 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-annfuni26pdtuawdwdj6zorw'), (b'openai-processing-ms', b'5616'), (b'openai-version', b'2020-10-01'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'300000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'298579'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'284ms'), (b'x-request-id', b'req_d12ecaef606352ff310f0d35d62844d7'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e5ba0aa8a902ab4-LAX'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 20 Nov 2024 21:45:38 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-annfuni26pdtuawdwdj6zorw', 'openai-processing-ms': '5616', 'openai-version': '2020-10-01', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '300000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '298579', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '284ms', 'x-request-id': 'req_d12ecaef606352ff310f0d35d62844d7', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8e5ba0aa8a902ab4-LAX', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_d12ecaef606352ff310f0d35d62844d7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response: The SQL query for the question \"Show me all funds with total assets over 1 billion\" needs these tables = [fund_reported_info], so we don't need JOIN.\n",
      "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
      "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
      "Label: \"EASY\"\n",
      "Classification: \"EASY\"\n"
     ]
    }
   ],
   "source": [
    "def get_schema_links_from_retrieval(value_retrieval, question):\n",
    "    retrieval_result = value_retrieval.process_question(question)\n",
    "    \n",
    "    # Get the most relevant schema links only\n",
    "    schema_links = []\n",
    "    \n",
    "    # 1. Get the main table/column matches\n",
    "    for word, matches in retrieval_result['similar_matches'].items():\n",
    "        if matches:\n",
    "            # Only take the top match if score > 0.7\n",
    "            top_match = matches[0]  # (match, score)\n",
    "            if top_match[1] > 0.7:\n",
    "                # Handle numerical values\n",
    "                if word in retrieval_result['extracted_info'].get('numerical_values', []):\n",
    "                    if 'billion' in word.lower():\n",
    "                        schema_links.append(f\"{top_match[0]} > 1000000000\")\n",
    "                    elif 'million' in word.lower():\n",
    "                        schema_links.append(f\"{top_match[0]} > 1000000\")\n",
    "                    else:\n",
    "                        schema_links.append(f\"{top_match[0]} > {word}\")\n",
    "                else:\n",
    "                    schema_links.append(top_match[0])\n",
    "    \n",
    "    # 2. Add only the foreign keys that connect tables we found\n",
    "    if len(schema_links) > 1:\n",
    "        tables_needed = set()\n",
    "        for link in schema_links:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].upper())\n",
    "        \n",
    "        # Only add foreign keys that connect our needed tables\n",
    "        for fk in retrieval_result['schema_relationships']['foreign_keys']:\n",
    "            tables_in_fk = set(part.split('.')[0] for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                schema_links.append(fk)\n",
    "    \n",
    "    print(\"Filtered schema links:\", schema_links)\n",
    "    return str(schema_links)\n",
    "\n",
    "# Test it\n",
    "vr = ValueRetrieval(schema_path='chatgpt_api/schema.json')\n",
    "question = \"Show me all funds with total assets over 1 billion\"\n",
    "\n",
    "schema_links = get_schema_links_from_retrieval(vr, question)\n",
    "print(\"\\nTesting classification with filtered schema links...\")\n",
    "classification = process_question_classification(question, schema_links)\n",
    "print(\"Classification:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
