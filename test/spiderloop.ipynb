{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir_path = \"/Users/virounikamina/Desktop/spider_data\"\n",
    "SCHEMA_FILE = \"/Users/virounikamina/Desktop/spider_data/tables.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_databases(dir_path):\n",
    "    \"\"\"\n",
    "    List all SQLite database files in the Spider dataset.\n",
    "    \"\"\"\n",
    "    db_files = []\n",
    "    db_paths = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".sqlite\"):\n",
    "                db_files.append(file)\n",
    "                db_paths.append(os.path.join(root, file))\n",
    "    return db_files, db_paths\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"\n",
    "    Connect to a specific SQLite database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def list_tables(conn):\n",
    "    \"\"\"\n",
    "    List all tables in the connected SQLite database.\n",
    "    \"\"\"\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def preview_table(conn, table_name, limit=5):\n",
    "    \"\"\"\n",
    "    Preview data from a specific table.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    columns = [desc[0] for desc in cursor.description]  # Column names\n",
    "    rows = cursor.fetchall()  # Data rows\n",
    "    return columns, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to: /Users/virounikamina/Desktop/spider_data/database/browser_web/browser_web.sqlite\n",
      "# Tables:  3\n",
      "\n",
      "Tables in the database:\n",
      "1: Web_client_accelerator\n",
      "2: browser\n",
      "3: accelerator_compatible_browser\n",
      "\n",
      "Previewing table: Web_client_accelerator\n",
      "\n",
      "Columns: ['id', 'name', 'Operating_system', 'Client', 'Connection']\n",
      "\n",
      "Rows:\n",
      "(1, 'CACHEbox', 'Appliance (Linux)', 'End user, ISP', 'Broadband, Satellite, Wireless, Fiber, DSL')\n",
      "(2, 'CProxy', 'Windows', 'user', 'up to 756kbit/s')\n",
      "(3, 'Fasterfox', 'Windows, Mac, Linux and Mobile devices', 'user', 'Dialup, Wireless, Broadband, DSL')\n",
      "(4, 'fasTun', 'Any', 'All', 'Any')\n",
      "(5, 'Freewire', 'Windows, except NT and 95', 'ISP', 'Dial-up')\n",
      "\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "db_names, db_paths = list_databases(database_dir_path)\n",
    "# print(\"Available Databases:\")\n",
    "# for idx, db_name in enumerate(db_names):\n",
    "#     print(f\"{idx + 1}: {db_name}\")\n",
    "\n",
    "# Select a database to open\n",
    "db_index = int(input(\"\\nEnter the number of the database to open: \")) - 1\n",
    "db_path = db_paths[db_index]\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "print(f\"\\nConnected to: {db_path}\")\n",
    "\n",
    "# List tables in the database\n",
    "tables = list_tables(conn)\n",
    "print(\"# Tables: \", len(tables))\n",
    "print(\"\\nTables in the database:\")\n",
    "for idx, table in enumerate(tables):\n",
    "    print(f\"{idx + 1}: {table}\")\n",
    "\n",
    "# Select a table to preview\n",
    "table_index = int(input(\"\\nEnter the number of the table to preview: \")) - 1\n",
    "table_name = tables[table_index]\n",
    "\n",
    "# Preview the selected table\n",
    "print(f\"\\nPreviewing table: {table_name}\")\n",
    "columns, rows = preview_table(conn, table_name)\n",
    "print(\"\\nColumns:\", columns)\n",
    "print(\"\\nRows:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"\\nConnection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "class PSLsh:\n",
    "    \"\"\"Locality-Sensitive Hashing implementation for fast approximate nearest neighbor search.\"\"\"\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random planes for each hash table\n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        \"\"\"Build hash tables from input vectors.\"\"\"\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        \"\"\"Generate hash codes for a vector.\"\"\"\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        \"\"\"Find candidate nearest neighbors for a query vector.\"\"\"\n",
    "        vector = vector.toarray()[0]  # Convert sparse matrix to 1D array\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SpiderValueRetrieval:\n",
    "    def __init__(self, spider_tables_path: str = 'spider/tables.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load all Spider schemas\n",
    "        print(\"DEBUG: Loading Spider schemas from:\", spider_tables_path)\n",
    "        with open(spider_tables_path, 'r') as f:\n",
    "            self.schemas = json.load(f)\n",
    "        \n",
    "        # Create a mapping of db_id to schema\n",
    "        self.db_schemas = {schema['db_id']: schema for schema in self.schemas}\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize database-specific components\n",
    "        self.column_indices = {}\n",
    "        self.vectorizers = {}\n",
    "        self.lsh_indices = {}\n",
    "        \n",
    "        # Build indices for each database\n",
    "        self._build_indices()\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "\n",
    "    def _get_schema_relationships(self, schema: Dict) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Extract primary and foreign keys for a specific database.\"\"\"\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        primary_keys = schema.get('primary_keys', [])\n",
    "        foreign_keys = schema.get('foreign_keys', [])\n",
    "        \n",
    "        # Format primary keys\n",
    "        formatted_pks = []\n",
    "        for pk in primary_keys:\n",
    "            table_idx, col_name = column_names[pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = table_names[table_idx]\n",
    "                formatted_pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        # Format foreign keys\n",
    "        formatted_fks = []\n",
    "        for fk in foreign_keys:\n",
    "            fk_col = column_names[fk[0]]\n",
    "            pk_col = column_names[fk[1]]\n",
    "            fk_table = table_names[fk_col[0]]\n",
    "            pk_table = table_names[pk_col[0]]\n",
    "            formatted_fks.append(\n",
    "                f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\"\n",
    "            )\n",
    "            \n",
    "        return formatted_pks, formatted_fks\n",
    "\n",
    "    def _parse_numeric_value(self, word: str) -> str:\n",
    "        \"\"\"Parse numeric values from words.\"\"\"\n",
    "        if 'billion' in word.lower():\n",
    "            return '1000000000'\n",
    "        elif 'million' in word.lower():\n",
    "            return '1000000'\n",
    "        return word\n",
    "    \n",
    "    def _build_indices(self):\n",
    "        \"\"\"Build all necessary indices for all databases.\"\"\"\n",
    "        for db_id, schema in self.db_schemas.items():\n",
    "            # Build column index\n",
    "            self.column_indices[db_id] = self._build_column_index(schema)\n",
    "            \n",
    "            # Build vectorizer and LSH\n",
    "            terms = self._get_schema_terms(schema)\n",
    "            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "            term_vectors = vectorizer.fit_transform(terms)\n",
    "            \n",
    "            self.vectorizers[db_id] = {\n",
    "                'vectorizer': vectorizer,\n",
    "                'terms': terms\n",
    "            }\n",
    "            \n",
    "            self.lsh_indices[db_id] = {\n",
    "                'lsh': PSLsh(term_vectors, n_planes=10, n_tables=5),\n",
    "                'vectors': term_vectors\n",
    "            }\n",
    "\n",
    "    def _build_column_index(self, schema: Dict) -> Dict:\n",
    "        \"\"\"Build column index for a specific database schema.\"\"\"\n",
    "        column_index = {}\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        column_types = schema.get('column_types', [])\n",
    "        \n",
    "        for (table_idx, col_name), col_type in zip(column_names[1:], column_types[1:]):  # Skip first row (*) \n",
    "            if table_idx != -1:  # Skip table_idx == -1 which represents '*'\n",
    "                table_name = table_names[table_idx].lower()\n",
    "                qualified_name = f\"{table_name}.{col_name.lower()}\"\n",
    "                \n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': col_name.lower(),\n",
    "                    'type': col_type,\n",
    "                    'words': self._split_column_name(col_name),\n",
    "                    'synonyms': self._get_column_synonyms(col_name)\n",
    "                }\n",
    "        \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        return list(set(words))  # For Spider, we'll just use the words themselves as synonyms\n",
    "\n",
    "    def _get_schema_terms(self, schema: Dict) -> List[str]:\n",
    "        \"\"\"Get all terms from a specific database schema.\"\"\"\n",
    "        terms = []\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        \n",
    "        for idx, table in enumerate(table_names):\n",
    "            table = table.lower()\n",
    "            terms.append(table)\n",
    "            \n",
    "            # Add column terms\n",
    "            table_columns = [(t_idx, col) for t_idx, col in column_names if t_idx == idx]\n",
    "            for _, column in table_columns:\n",
    "                terms.append(f\"{table}.{column.lower()}\")\n",
    "                \n",
    "        return terms\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _find_similar_words(self, word: str, db_id: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find similar words in the database-specific schema.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        matches = []\n",
    "        \n",
    "        # Direct matching with column names\n",
    "        column_index = self.column_indices[db_id]\n",
    "        for qualified_name, metadata in column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            if score > 0.6:\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # LSH-based matching as backup\n",
    "        if len(matches) < 5:\n",
    "            try:\n",
    "                vectorizer = self.vectorizers[db_id]['vectorizer']\n",
    "                terms = self.vectorizers[db_id]['terms']\n",
    "                word_vector = vectorizer.transform([word])\n",
    "                \n",
    "                lsh = self.lsh_indices[db_id]['lsh']\n",
    "                vectors = self.lsh_indices[db_id]['vectors']\n",
    "                \n",
    "                candidate_indices = lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = terms[idx]\n",
    "                    if not any(term == match[0] for match in matches):\n",
    "                        candidate_vector = vectors[idx].toarray()[0]\n",
    "                        word_vector_array = word_vector.toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector_array - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return matches[:5]\n",
    "\n",
    "    def _extract_keywords(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Extract keywords with database-specific context.\"\"\"\n",
    "        schema = self.db_schemas[db_id]\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        system_prompt = f\"\"\"Given this database schema:\n",
    "        Tables: {schema['table_names_original']}\n",
    "        Columns: {schema['column_names_original']}\n",
    "        \n",
    "        Primary Keys: {primary_keys}\n",
    "        Foreign Keys: {foreign_keys}\n",
    "\n",
    "        Extract relevant keywords, keyphrases, and numerical values from the question in JSON format.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "            ],\n",
    "            functions=[\n",
    "                {\n",
    "                    \"name\": \"extract_components\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"keyphrases\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"numerical_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            function_call={\"name\": \"extract_components\"}\n",
    "        )\n",
    "\n",
    "        # Access the function_call attribute directly\n",
    "        function_call = response.choices[0].message.function_call\n",
    "        arguments = function_call.arguments\n",
    "        extracted_info = json.loads(arguments)\n",
    "\n",
    "        # Debugging statement\n",
    "        print(\"Extracted Info:\", extracted_info)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "\n",
    "    def process_question(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Process question with database context.\"\"\"\n",
    "        # Extract keywords using database-specific schema\n",
    "        extracted_info = self._extract_keywords(question, db_id)\n",
    "        \n",
    "        # Process words\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Words Extracted:\", words)\n",
    "\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "        \n",
    "        processed_words = list(set(processed_words))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Processed Words:\", processed_words)\n",
    "\n",
    "        # Find similar columns using database-specific indices\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self._find_similar_words(word, db_id)\n",
    "            # Debugging statement\n",
    "            print(f\"Similar matches for '{word}': {similar_matches[word]}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self._get_schema_relationships(self.db_schemas[db_id])[0],\n",
    "                \"foreign_keys\": self._get_schema_relationships(self.db_schemas[db_id])[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "        \n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # Use database-specific indices and relationships\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                top_match = matches[0]\n",
    "                if top_match[1] > 0.7:\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        # Handle numerical values\n",
    "                        value = self._parse_numeric_value(word)\n",
    "                        table_columns.append(f\"{top_match[0]} > {value}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "\n",
    "        # Get relevant tables for this database\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].lower())\n",
    "        \n",
    "        # Add relevant primary keys\n",
    "        for pk in primary_keys:\n",
    "            table = pk.split('.')[0].lower()\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "                \n",
    "        # Add relevant foreign keys\n",
    "        for fk in foreign_keys:\n",
    "            tables_in_fk = set(part.split('.')[0].lower() for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        \n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys,\n",
    "            \"schema_links\": table_columns  # Added for DIN SQL compatibility\n",
    "        }\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Schema Dict:\", schema_dict)\n",
    "        \n",
    "        return str(schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def classification_prompt_maker(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Create classification prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format schema info for the prompt\n",
    "    schema_info = f\"Tables: {schema['table_names_original']}\\nColumns: {schema['column_names_original']}\"\n",
    "    \n",
    "    instruction = \"\"\"Given the database schema:\n",
    "{schema_info}\n",
    "\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "Classify the question as:\n",
    "- EASY: no JOIN and no nested queries needed\n",
    "- NON-NESTED: needs JOIN but no nested queries\n",
    "- NESTED: needs nested queries\n",
    "\n",
    "Question: \"{question}\"\n",
    "Schema Links: {schema_links}\n",
    "\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "    # Get formatted keys\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "    \n",
    "    return instruction.format(\n",
    "        schema_info=schema_info,\n",
    "        primary_keys=primary_keys,\n",
    "        foreign_keys=foreign_keys,\n",
    "        question=question,\n",
    "        schema_links=schema_links\n",
    "    )\n",
    "\n",
    "def process_question_classification(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Process question classification with Spider database context.\"\"\"\n",
    "    def extract_classification(text):\n",
    "        print(f\"Trying to extract classification from: {text}\")\n",
    "        text = text.upper()\n",
    "        \n",
    "        for class_type in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "            if class_type in text:\n",
    "                return class_type\n",
    "                \n",
    "        patterns = [\"LABEL:\", \"CLASSIFICATION:\", \"CAN BE CLASSIFIED AS\"]\n",
    "        for pattern in patterns:\n",
    "            if pattern in text:\n",
    "                parts = text.split(pattern)\n",
    "                if len(parts) > 1:\n",
    "                    result = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    classification = result.split()[0].strip()\n",
    "                    if classification in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "                        return classification\n",
    "        \n",
    "        return \"NESTED\"  # Default fallback\n",
    "\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=spider_schemas\n",
    "                    )\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "            print(\"Raw response:\", raw_response)\n",
    "            classification = extract_classification(raw_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    final_class = classification if classification else \"NESTED\"\n",
    "    return f'\"{final_class}\"'\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        if \"SQL:\" in text:\n",
    "            return text.split(\"SQL:\")[-1].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    def make_spider_prompt(template_type):\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        # Spider-specific examples\n",
    "        examples = {\n",
    "            \"easy\": '''Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club''',\n",
    "            \n",
    "            \"medium\": '''Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id''',\n",
    "            \n",
    "            \"hard\": '''Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)'''\n",
    "        }\n",
    "\n",
    "        template = examples[template_type]\n",
    "        prompt = f\"\"\"Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "{template}\n",
    "\n",
    "Generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if '\"EASY\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"easy\")\n",
    "            elif '\"NON-NESTED\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"medium\")\n",
    "            else:\n",
    "                prompt = make_spider_prompt(\"hard\")\n",
    "\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            sql = extract_sql(response.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                sql = \"SELECT\"\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        print(f\"\\nTrying to extract SQL from: {text}\")  # Debug print\n",
    "        if not text:\n",
    "            return \"SELECT\"\n",
    "            \n",
    "        markers = [\"SQL:\", \"Query:\", \"QUERY:\", \"SQL Query:\", \"Final SQL:\"]\n",
    "        for marker in markers:\n",
    "            if marker in text:\n",
    "                parts = text.split(marker)\n",
    "                if len(parts) > 1:\n",
    "                    sql = parts[-1].strip()\n",
    "                    print(f\"Found SQL after {marker}: {sql}\")  # Debug print\n",
    "                    return sql\n",
    "        print(\"No SQL marker found, returning full text\")  # Debug print\n",
    "        return text.strip()\n",
    "\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "    if '\"EASY\"' in predicted_class:\n",
    "        print(\"EASY\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(easy_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "                    \n",
    "    elif '\"NON-NESTED\"' in predicted_class:\n",
    "        print(\"NON-NESTED\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(medium_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "                    \n",
    "    else:\n",
    "        print(\"NESTED\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(hard_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "\n",
    "    return SQL if SQL else \"SELECT\"\n",
    "\n",
    "def GPT4_generation(prompt, max_retries=3):\n",
    "    client = OpenAI()\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\", \n",
    "                messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a database assistant that generates SQL queries based on questions about any database schema.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": prompt}],\n",
    "                n = 1,\n",
    "                stream = False,\n",
    "                temperature=0.0,\n",
    "                max_tokens=600,\n",
    "                top_p = 1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def easy_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for easy Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "SQL:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for medium (non-nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def hard_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for hard (nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def debugger(question: str, sql: str, db_id: str, spider_schemas: list):\n",
    "    \"\"\"Create debug prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format relationships for better context\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "\n",
    "    instruction = f\"\"\"#### Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "#### For the given question, check and fix the SQL query based on these rules:\n",
    "1) Use the correct table and column names from the schema\n",
    "2) Use proper JOIN conditions based on the foreign key relationships\n",
    "3) Use DESC and DISTINCT when needed based on the question\n",
    "4) Ensure GROUP BY statements include necessary columns\n",
    "5) Verify SELECT statement columns match the question requirements\n",
    "6) Remove redundant columns from GROUP BY\n",
    "7) Use GROUP BY on one column when possible\n",
    "\n",
    "Question: {question}\n",
    "Original SQL: {sql}\n",
    "\n",
    "Return the fixed SQL query:\"\"\"\n",
    "\n",
    "    return instruction\n",
    "\n",
    "def GPT4_debug(prompt):\n",
    "    \"\"\"Debug SQL using GPT-4.\"\"\"\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a SQL expert focusing on fixing and optimizing SQL queries for SQLite.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.0,\n",
    "            max_tokens=350,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"#\", \";\", \"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT4_debug: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def refine_query(question: str, sql: str, db_id: str, spider_schemas: list) -> str:\n",
    "    \"\"\"Refine and debug the SQL query.\"\"\"\n",
    "    max_attempts = 3\n",
    "    attempt = 0\n",
    "    debugged_SQL = None\n",
    "    \n",
    "    while debugged_SQL is None and attempt < max_attempts:\n",
    "        try:\n",
    "            debug_prompt = debugger(question, sql, db_id, spider_schemas)\n",
    "            debugged_SQL = GPT4_debug(debug_prompt)\n",
    "            \n",
    "            if debugged_SQL:\n",
    "                # Clean up the response\n",
    "                debugged_SQL = debugged_SQL.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                # Extract just the SQL if there's additional text\n",
    "                if \"sql\" in debugged_SQL.lower():\n",
    "                    parts = debugged_SQL.lower().split(\"sql\")\n",
    "                    debugged_SQL = parts[-1].strip()\n",
    "                \n",
    "                print(f\"Refined SQL (Attempt {attempt + 1}):\", debugged_SQL)\n",
    "                return debugged_SQL\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in refinement attempt {attempt + 1}: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # If all attempts fail, return original SQL\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available modes:\n",
      "1. dev (1034 questions)\n",
      "2. train_spider (7000 questions)\n",
      "3. train_others (1659 questions)\n",
      "Loading dev.json...\n",
      "Loaded 1034 questions\n",
      "DEBUG: Loading Spider schemas from: /Users/virounikamina/Desktop/spider_data/tables.json\n",
      "\n",
      "Testing 10 questions\n",
      "\n",
      "Question 1/10\n",
      "Question: How many singers do we have?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singers', 'count'], 'keyphrases': [], 'numerical_values': []}\n",
      "Words Extracted: ['singers', 'count']\n",
      "Processed Words: ['count', 'singer']\n",
      "Similar matches for 'count': [('singer.country', 0.83), ('concert.concert_id', 0.67), ('concert.concert_name', 0.67), ('singer_in_concert.concert_id', 0.67)]\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Table Columns: ['singer.country', 'singer.singer_id']\n",
      "Schema Dict: {'table_columns': ['singer.country', 'singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.country', 'singer.singer_id']}\n",
      "Schema Links: {'table_columns': ['singer.country', 'singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.country', 'singer.singer_id']}\n",
      "Raw response: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table without needing to join with any other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table without needing to join with any other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT COUNT(*) FROM singer\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT COUNT(*) FROM singer\n",
      "Ground Truth: SELECT count(*) FROM singer\n",
      "\n",
      "Question 2/10\n",
      "Question: What is the total number of singers?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singers', 'total', 'number'], 'keyphrases': ['total number of singers']}\n",
      "Words Extracted: ['singers', 'total', 'number', 'total number of singers']\n",
      "Processed Words: ['total', 'singer', 'number']\n",
      "Similar matches for 'total': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'number': []\n",
      "Table Columns: ['singer.singer_id']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id']}\n",
      "Raw response: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This can be done with a simple COUNT function on the 'singer_id' column in the 'singer' table. \n",
      "\n",
      "There is no need to join multiple tables or use nested queries to answer this question. \n",
      "\n",
      "Therefore, this question is classified as EASY.\n",
      "Trying to extract classification from: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This can be done with a simple COUNT function on the 'singer_id' column in the 'singer' table. \n",
      "\n",
      "There is no need to join multiple tables or use nested queries to answer this question. \n",
      "\n",
      "Therefore, this question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT COUNT(*) FROM singer\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT COUNT(*) FROM singer\n",
      "Ground Truth: SELECT count(*) FROM singer\n",
      "\n",
      "Question 3/10\n",
      "Question: Show name, country, age for all singers ordered by age from the oldest to the youngest.\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singer', 'name', 'country', 'age'], 'keyphrases': ['ordered by age from the oldest to the youngest']}\n",
      "Words Extracted: ['singer', 'name', 'country', 'age', 'ordered by age from the oldest to the youngest']\n",
      "Processed Words: ['youngest', 'singer', 'age', 'ordered', 'country', 'name', 'oldest']\n",
      "Similar matches for 'youngest': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for 'ordered': []\n",
      "Similar matches for 'country': [('singer.country', 1.0)]\n",
      "Similar matches for 'name': [('stadium.name', 1.0), ('singer.name', 1.0), ('singer.song_name', 1.0), ('concert.concert_name', 1.0)]\n",
      "Similar matches for 'oldest': [('stadium.lowest', 0.67)]\n",
      "Table Columns: ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']}\n",
      "Raw response: The question is asking for information only from the 'singer' table. It does not require any JOIN operations with other tables or nested queries. Therefore, this question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for information only from the 'singer' table. It does not require any JOIN operations with other tables or nested queries. Therefore, this question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT Name, Country, Age FROM singer ORDER BY Age DESC\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT Name, Country, Age FROM singer ORDER BY Age DESC\n",
      "Ground Truth: SELECT name ,  country ,  age FROM singer ORDER BY age DESC\n",
      "\n",
      "Question 4/10\n",
      "Question: What are the names, countries, and ages for every singer in descending order of age?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['names', 'countries', 'ages', 'singer'], 'keyphrases': ['descending order of age']}\n",
      "Words Extracted: ['names', 'countries', 'ages', 'singer', 'descending order of age']\n",
      "Processed Words: ['singer', 'age', 'descending', 'order', 'country', 'name']\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for 'descending': []\n",
      "Similar matches for 'order': []\n",
      "Similar matches for 'country': [('singer.country', 1.0)]\n",
      "Similar matches for 'name': [('stadium.name', 1.0), ('singer.name', 1.0), ('singer.song_name', 1.0), ('concert.concert_name', 1.0)]\n",
      "Table Columns: ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country', 'stadium.name']}\n",
      "Raw response: The question is asking for information only from the 'singer' table (names, countries, and ages of singers). There is no need to join with other tables or use nested queries. Therefore, this question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for information only from the 'singer' table (names, countries, and ages of singers). There is no need to join with other tables or use nested queries. Therefore, this question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT Name, Country, Age FROM singer ORDER BY Age DESC\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT Name, Country, Age FROM singer ORDER BY Age DESC\n",
      "Ground Truth: SELECT name ,  country ,  age FROM singer ORDER BY age DESC\n",
      "\n",
      "Question 5/10\n",
      "Question: What is the average, minimum, and maximum age of all singers from France?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['average age', 'minimum age', 'maximum age', 'singers', 'France'], 'keyphrases': ['all singers from France'], 'numerical_values': []}\n",
      "Words Extracted: ['average age', 'minimum age', 'maximum age', 'singers', 'France', 'all singers from France']\n",
      "Processed Words: ['singer', 'age', 'average', 'france', 'maximum', 'minimum']\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for 'average': [('stadium.average', 1.0)]\n",
      "Similar matches for 'france': []\n",
      "Similar matches for 'maximum': []\n",
      "Similar matches for 'minimum': []\n",
      "Table Columns: ['singer.singer_id', 'singer.age', 'stadium.average']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.age', 'stadium.average'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'stadium.average']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.age', 'stadium.average'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'stadium.average']}\n",
      "Raw response: The question is asking for the average, minimum, and maximum age of all singers from France. This information can be retrieved from the 'singer' table only. We don't need to join any other tables to get this information. Also, we don't need to use any nested queries. \n",
      "\n",
      "So, the question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for the average, minimum, and maximum age of all singers from France. This information can be retrieved from the 'singer' table only. We don't need to join any other tables to get this information. Also, we don't need to use any nested queries. \n",
      "\n",
      "So, the question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT AVG(Age), MIN(Age), MAX(Age) FROM singer WHERE Country = 'France'\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT AVG(Age), MIN(Age), MAX(Age) FROM singer WHERE Country = 'France'\n",
      "Ground Truth: SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\n",
      "\n",
      "Question 6/10\n",
      "Question: What is the average, minimum, and maximum age for all French singers?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['average', 'minimum', 'maximum', 'age', 'French', 'singers'], 'keyphrases': ['French singers'], 'numerical_values': []}\n",
      "Words Extracted: ['average', 'minimum', 'maximum', 'age', 'French', 'singers', 'French singers']\n",
      "Processed Words: ['singer', 'age', 'average', 'french', 'maximum', 'minimum']\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for 'average': [('stadium.average', 1.0)]\n",
      "Similar matches for 'french': []\n",
      "Similar matches for 'maximum': []\n",
      "Similar matches for 'minimum': []\n",
      "Table Columns: ['singer.singer_id', 'singer.age', 'stadium.average']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.age', 'stadium.average'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'stadium.average']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.age', 'stadium.average'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'stadium.average']}\n",
      "Raw response: - We need to find the average, minimum, and maximum age of singers.\n",
      "- The age of singers is stored in the 'singer' table.\n",
      "- We also need to filter the singers based on their country, which is also stored in the 'singer' table.\n",
      "- We don't need to join any tables for this query.\n",
      "\n",
      "So, the classification of this question is EASY.\n",
      "Trying to extract classification from: - We need to find the average, minimum, and maximum age of singers.\n",
      "- The age of singers is stored in the 'singer' table.\n",
      "- We also need to filter the singers based on their country, which is also stored in the 'singer' table.\n",
      "- We don't need to join any tables for this query.\n",
      "\n",
      "So, the classification of this question is EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT AVG(Age), MIN(Age), MAX(Age) FROM singer WHERE Country = 'French'\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT AVG(Age), MIN(Age), MAX(Age) FROM singer WHERE Country = 'French'\n",
      "Ground Truth: SELECT avg(age) ,  min(age) ,  max(age) FROM singer WHERE country  =  'France'\n",
      "\n",
      "Question 7/10\n",
      "Question: Show the name and the release year of the song by the youngest singer.\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['song', 'youngest singer'], 'keyphrases': ['release year of the song'], 'numerical_values': []}\n",
      "Words Extracted: ['song', 'youngest singer', 'release year of the song']\n",
      "Processed Words: ['youngest', 'singer', 'release', 'song', 'year']\n",
      "Similar matches for 'youngest': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'release': [('singer.song_release_year', 1.0)]\n",
      "Similar matches for 'song': [('singer.song_name', 1.0), ('singer.song_release_year', 1.0)]\n",
      "Similar matches for 'year': [('singer.song_release_year', 1.0), ('concert.year', 1.0)]\n",
      "Table Columns: ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'singer.song_release_year']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'singer.song_release_year'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'singer.song_release_year']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'singer.song_release_year'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'singer.song_release_year']}\n",
      "Raw response: 1. We need to find the youngest singer. This information is in the 'singer' table, specifically in the 'Age' column. We need to find the minimum value in this column.\n",
      "\n",
      "2. Once we have the youngest singer, we need to find the song they sang and the release year of that song. This information is also in the 'singer' table, specifically in the 'Song_Name' and 'Song_release_year' columns.\n",
      "\n",
      "3. We don't need to join any tables because all the information we need is in the 'singer' table.\n",
      "\n",
      "4. However, we do need to use a nested query. The outer query will find the song name and release year, and the inner query will find the youngest singer.\n",
      "\n",
      "So, the question needs NESTED queries.\n",
      "Trying to extract classification from: 1. We need to find the youngest singer. This information is in the 'singer' table, specifically in the 'Age' column. We need to find the minimum value in this column.\n",
      "\n",
      "2. Once we have the youngest singer, we need to find the song they sang and the release year of that song. This information is also in the 'singer' table, specifically in the 'Song_Name' and 'Song_release_year' columns.\n",
      "\n",
      "3. We don't need to join any tables because all the information we need is in the 'singer' table.\n",
      "\n",
      "4. However, we do need to use a nested query. The outer query will find the song name and release year, and the inner query will find the youngest singer.\n",
      "\n",
      "So, the question needs NESTED queries.\n",
      "Classification: \"NESTED\"\n",
      "NESTED\n",
      "\n",
      "Trying to extract SQL from: 1. Find the youngest singer\n",
      "2. Get the name and release year of the song by this singer\n",
      "\n",
      "SQL: \n",
      "```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE age = (SELECT MIN(age) FROM singer)\n",
      "```\n",
      "Found SQL after SQL:: ```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE age = (SELECT MIN(age) FROM singer)\n",
      "```\n",
      "Generated SQL: ```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE age = (SELECT MIN(age) FROM singer)\n",
      "```\n",
      "Ground Truth: SELECT song_name ,  song_release_year FROM singer ORDER BY age LIMIT 1\n",
      "\n",
      "Question 8/10\n",
      "Question: What are the names and release years for all the songs of the youngest singer?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['names', 'release years', 'songs', 'youngest singer'], 'keyphrases': ['songs of the youngest singer'], 'numerical_values': []}\n",
      "Words Extracted: ['names', 'release years', 'songs', 'youngest singer', 'songs of the youngest singer']\n",
      "Processed Words: ['youngest', 'singer', 'release', 'song', 'name', 'year']\n",
      "Similar matches for 'youngest': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'release': [('singer.song_release_year', 1.0)]\n",
      "Similar matches for 'song': [('singer.song_name', 1.0), ('singer.song_release_year', 1.0)]\n",
      "Similar matches for 'name': [('stadium.name', 1.0), ('singer.name', 1.0), ('singer.song_name', 1.0), ('concert.concert_name', 1.0)]\n",
      "Similar matches for 'year': [('singer.song_release_year', 1.0), ('concert.year', 1.0)]\n",
      "Table Columns: ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'stadium.name', 'singer.song_release_year']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'stadium.name', 'singer.song_release_year'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'stadium.name', 'singer.song_release_year']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'stadium.name', 'singer.song_release_year'], 'primary_keys': ['stadium.stadium_id', 'singer.singer_id'], 'foreign_keys': ['concert.stadium_id = stadium.stadium_id', 'singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.song_release_year', 'singer.song_name', 'stadium.name', 'singer.song_release_year']}\n",
      "Raw response: 1. We need to find the youngest singer. This requires us to look at the 'Age' column in the 'singer' table.\n",
      "2. Once we have identified the youngest singer, we need to find all the songs of this singer. This requires us to look at the 'Song_Name' and 'Song_release_year' columns in the 'singer' table.\n",
      "\n",
      "This process involves looking at multiple columns in the same table based on a condition (finding the youngest singer). Therefore, this question requires a nested query.\n",
      "\n",
      "So, the answer is NESTED.\n",
      "Trying to extract classification from: 1. We need to find the youngest singer. This requires us to look at the 'Age' column in the 'singer' table.\n",
      "2. Once we have identified the youngest singer, we need to find all the songs of this singer. This requires us to look at the 'Song_Name' and 'Song_release_year' columns in the 'singer' table.\n",
      "\n",
      "This process involves looking at multiple columns in the same table based on a condition (finding the youngest singer). Therefore, this question requires a nested query.\n",
      "\n",
      "So, the answer is NESTED.\n",
      "Classification: \"NESTED\"\n",
      "NESTED\n",
      "\n",
      "Trying to extract SQL from: 1. Find the youngest singer\n",
      "2. Get the names and release years of all their songs\n",
      "\n",
      "SQL: \n",
      "```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE singer_id = (\n",
      "    SELECT singer_id \n",
      "    FROM singer \n",
      "    ORDER BY age ASC \n",
      "    LIMIT 1\n",
      ")\n",
      "```\n",
      "Found SQL after SQL:: ```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE singer_id = (\n",
      "    SELECT singer_id \n",
      "    FROM singer \n",
      "    ORDER BY age ASC \n",
      "    LIMIT 1\n",
      ")\n",
      "```\n",
      "Generated SQL: ```sql\n",
      "SELECT song_name, song_release_year \n",
      "FROM singer \n",
      "WHERE singer_id = (\n",
      "    SELECT singer_id \n",
      "    FROM singer \n",
      "    ORDER BY age ASC \n",
      "    LIMIT 1\n",
      ")\n",
      "```\n",
      "Ground Truth: SELECT song_name ,  song_release_year FROM singer ORDER BY age LIMIT 1\n",
      "\n",
      "Question 9/10\n",
      "Question: What are all distinct countries where singers above age 20 are from?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singers', 'countries', 'age'], 'keyphrases': ['distinct countries where singers are from', 'singers above age 20'], 'numerical_values': ['20']}\n",
      "Words Extracted: ['singers', 'countries', 'age', 'distinct countries where singers are from', 'singers above age 20', '20']\n",
      "Processed Words: ['distinct', 'singer', 'age', 'country', '20']\n",
      "Similar matches for 'distinct': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for 'country': [('singer.country', 1.0)]\n",
      "Similar matches for '20': []\n",
      "Table Columns: ['singer.singer_id', 'singer.age', 'singer.country']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.age', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.age', 'singer.country']}\n",
      "Raw response: The question is asking for distinct countries where singers above age 20 are from. This information can be directly retrieved from the 'singer' table. We need to filter the singers based on their age and then select the distinct countries. \n",
      "\n",
      "There is no need to join multiple tables or use nested queries to answer this question. \n",
      "\n",
      "So, the difficulty level of this question is EASY.\n",
      "Trying to extract classification from: The question is asking for distinct countries where singers above age 20 are from. This information can be directly retrieved from the 'singer' table. We need to filter the singers based on their age and then select the distinct countries. \n",
      "\n",
      "There is no need to join multiple tables or use nested queries to answer this question. \n",
      "\n",
      "So, the difficulty level of this question is EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT DISTINCT Country FROM singer WHERE Age > 20\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT DISTINCT Country FROM singer WHERE Age > 20\n",
      "Ground Truth: SELECT DISTINCT country FROM singer WHERE age  >  20\n",
      "\n",
      "Question 10/10\n",
      "Question: What are  the different countries with singers above age 20?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singers', 'countries', 'age'], 'numerical_values': ['20']}\n",
      "Words Extracted: ['singers', 'countries', 'age', '20']\n",
      "Processed Words: ['age', '20', 'singer', 'country']\n",
      "Similar matches for 'age': [('singer.age', 1.0)]\n",
      "Similar matches for '20': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', np.float64(0.8))]\n",
      "Similar matches for 'country': [('singer.country', 1.0)]\n",
      "Table Columns: ['singer.age', 'singer.singer_id', 'singer.country']\n",
      "Schema Dict: {'table_columns': ['singer.age', 'singer.singer_id', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.age', 'singer.singer_id', 'singer.country']}\n",
      "Schema Links: {'table_columns': ['singer.age', 'singer.singer_id', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.age', 'singer.singer_id', 'singer.country']}\n",
      "Raw response: The question is asking for the different countries where there are singers above the age of 20. This information can be directly retrieved from the 'singer' table. We don't need to join any tables or use nested queries to get this information. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for the different countries where there are singers above the age of 20. This information can be directly retrieved from the 'singer' table. We don't need to join any tables or use nested queries to get this information. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "EASY\n",
      "\n",
      "Trying to extract SQL from: SELECT DISTINCT Country FROM singer WHERE Age > 20\n",
      "No SQL marker found, returning full text\n",
      "Generated SQL: SELECT DISTINCT Country FROM singer WHERE Age > 20\n",
      "Ground Truth: SELECT DISTINCT country FROM singer WHERE age  >  20\n",
      "\n",
      "=== Testing Summary ===\n",
      "Total Questions Processed: 10\n",
      "Successful: 0\n",
      "Success Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class SpiderTester:\n",
    "\n",
    "    def log(self, message: str):\n",
    "        \"\"\"Log message to file and console.\"\"\"\n",
    "        print(message)\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(message + '\\n')\n",
    "\n",
    "    def __init__(self, spider_path: str = '/Users/virounikamina/Desktop/spider_data', mode='dev'):\n",
    "        \"\"\"Initialize Spider tester.\n",
    "        Args:\n",
    "            spider_path: Path to Spider dataset\n",
    "            mode: Which dataset to use ('dev', 'train_spider', or 'train_others')\n",
    "        \"\"\"\n",
    "        self.spider_path = spider_path\n",
    "        \n",
    "        # Load schemas\n",
    "        with open(os.path.join(spider_path, 'tables.json'), 'r') as f:\n",
    "            self.spider_schemas = json.load(f)\n",
    "            \n",
    "        # note you must select correct data file based on mode (else it will say database not found)\n",
    "        data_files = {\n",
    "            'dev': 'dev.json',\n",
    "            'train_spider': 'train_spider.json',\n",
    "            'train_others': 'train_others.json'\n",
    "        }\n",
    "        \n",
    "        if mode not in data_files:\n",
    "            raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "            \n",
    "        data_file = data_files[mode]\n",
    "        print(f\"Loading {data_file}...\")\n",
    "        \n",
    "        # Load questions\n",
    "        with open(os.path.join(spider_path, data_file), 'r') as f:\n",
    "            self.test_data = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.test_data)} questions\")\n",
    "            \n",
    "        # Initialize value retrieval\n",
    "        self.value_retrieval = SpiderValueRetrieval(\n",
    "            spider_tables_path=os.path.join(spider_path, 'tables.json')\n",
    "        )\n",
    "        \n",
    "        # Create output file\n",
    "        self.output_file = os.path.join(spider_path, f'results_{mode}_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
    "\n",
    "    def run_test(self, num_questions: int = None):\n",
    "        if num_questions is None:\n",
    "            num_questions = len(self.test_data)\n",
    "        else:\n",
    "            num_questions = min(num_questions, len(self.test_data))\n",
    "\n",
    "        processed = 0\n",
    "        successful = 0\n",
    "        \n",
    "        self.log(f\"\\nTesting {num_questions} questions\")\n",
    "        \n",
    "        for idx, test_case in enumerate(self.test_data[:num_questions]):\n",
    "            question = test_case['question']\n",
    "            db_id = test_case['db_id']\n",
    "            \n",
    "            self.log(f\"\\nQuestion {idx + 1}/{num_questions}\")\n",
    "            self.log(f\"Question: {question}\")\n",
    "            self.log(f\"Database: {db_id}\")\n",
    "                \n",
    "            try:\n",
    "                # Process question\n",
    "                schema_links = self.value_retrieval.process_schema(question, db_id)\n",
    "                self.log(f\"Schema Links: {schema_links}\")\n",
    "                \n",
    "                # Get classification\n",
    "                classification = process_question_classification(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    db_id=db_id,\n",
    "                    spider_schemas=self.spider_schemas\n",
    "                )\n",
    "                self.log(f\"Classification: {classification}\")\n",
    "                \n",
    "                # Generate SQL\n",
    "                sql = process_question_sql(\n",
    "                    question=question,\n",
    "                    predicted_class=classification,\n",
    "                    schema_links=schema_links,\n",
    "                    db_id=db_id,\n",
    "                    spider_schemas=self.spider_schemas\n",
    "                )\n",
    "                \n",
    "                self.log(f\"Generated SQL: {sql}\")\n",
    "                self.log(f\"Ground Truth: {test_case['query']}\")\n",
    "                \n",
    "                processed += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error processing question: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        self.log(\"\\n=== Testing Summary ===\")\n",
    "        self.log(f\"Total Questions Processed: {processed}\")\n",
    "        self.log(f\"Successful: {successful}\")\n",
    "        if processed > 0:\n",
    "            self.log(f\"Success Rate: {(successful/processed)*100:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    # Choose which dataset to use HERE (dev, train_spider, or train_others)\n",
    "    print(\"Available modes:\")\n",
    "    print(\"1. dev (1034 questions)\")\n",
    "    print(\"2. train_spider (7000 questions)\")\n",
    "    print(\"3. train_others (1659 questions)\")\n",
    "    \n",
    "    mode = input(\"Choose mode (dev/train_spider/train_others) [default: dev]: \").strip() or 'dev'\n",
    "    num_questions = input(\"How many questions to process? (press Enter for all): \").strip()\n",
    "    \n",
    "    tester = SpiderTester(mode=mode)\n",
    "    tester.run_test(num_questions=int(num_questions) if num_questions else None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
