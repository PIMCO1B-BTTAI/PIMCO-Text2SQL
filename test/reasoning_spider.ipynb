{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir_path = \"/Users/virounikamina/Desktop/spider_data\"\n",
    "SCHEMA_FILE = \"/Users/virounikamina/Desktop/spider_data/tables.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_databases(dir_path):\n",
    "    \"\"\"\n",
    "    List all SQLite database files in the Spider dataset.\n",
    "    \"\"\"\n",
    "    db_files = []\n",
    "    db_paths = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".sqlite\"):\n",
    "                db_files.append(file)\n",
    "                db_paths.append(os.path.join(root, file))\n",
    "    return db_files, db_paths\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"\n",
    "    Connect to a specific SQLite database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def list_tables(conn):\n",
    "    \"\"\"\n",
    "    List all tables in the connected SQLite database.\n",
    "    \"\"\"\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def preview_table(conn, table_name, limit=5):\n",
    "    \"\"\"\n",
    "    Preview data from a specific table.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    columns = [desc[0] for desc in cursor.description]  # Column names\n",
    "    rows = cursor.fetchall()  # Data rows\n",
    "    return columns, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to: /Users/virounikamina/Desktop/spider_data/database/browser_web/browser_web.sqlite\n",
      "# Tables:  3\n",
      "\n",
      "Tables in the database:\n",
      "1: Web_client_accelerator\n",
      "2: browser\n",
      "3: accelerator_compatible_browser\n",
      "\n",
      "Previewing table: Web_client_accelerator\n",
      "\n",
      "Columns: ['id', 'name', 'Operating_system', 'Client', 'Connection']\n",
      "\n",
      "Rows:\n",
      "(1, 'CACHEbox', 'Appliance (Linux)', 'End user, ISP', 'Broadband, Satellite, Wireless, Fiber, DSL')\n",
      "(2, 'CProxy', 'Windows', 'user', 'up to 756kbit/s')\n",
      "(3, 'Fasterfox', 'Windows, Mac, Linux and Mobile devices', 'user', 'Dialup, Wireless, Broadband, DSL')\n",
      "(4, 'fasTun', 'Any', 'All', 'Any')\n",
      "(5, 'Freewire', 'Windows, except NT and 95', 'ISP', 'Dial-up')\n",
      "\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "db_names, db_paths = list_databases(database_dir_path)\n",
    "# print(\"Available Databases:\")\n",
    "# for idx, db_name in enumerate(db_names):\n",
    "#     print(f\"{idx + 1}: {db_name}\")\n",
    "\n",
    "# Select a database to open\n",
    "db_index = int(input(\"\\nEnter the number of the database to open: \")) - 1\n",
    "db_path = db_paths[db_index]\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "print(f\"\\nConnected to: {db_path}\")\n",
    "\n",
    "# List tables in the database\n",
    "tables = list_tables(conn)\n",
    "print(\"# Tables: \", len(tables))\n",
    "print(\"\\nTables in the database:\")\n",
    "for idx, table in enumerate(tables):\n",
    "    print(f\"{idx + 1}: {table}\")\n",
    "\n",
    "# Select a table to preview\n",
    "table_index = int(input(\"\\nEnter the number of the table to preview: \")) - 1\n",
    "table_name = tables[table_index]\n",
    "\n",
    "# Preview the selected table\n",
    "print(f\"\\nPreviewing table: {table_name}\")\n",
    "columns, rows = preview_table(conn, table_name)\n",
    "print(\"\\nColumns:\", columns)\n",
    "print(\"\\nRows:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"\\nConnection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "class PSLsh:\n",
    "    \"\"\"Locality-Sensitive Hashing implementation for fast approximate nearest neighbor search.\"\"\"\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random planes for each hash table\n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        \"\"\"Build hash tables from input vectors.\"\"\"\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        \"\"\"Generate hash codes for a vector.\"\"\"\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        \"\"\"Find candidate nearest neighbors for a query vector.\"\"\"\n",
    "        vector = vector.toarray()[0]  # Convert sparse matrix to 1D array\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SpiderValueRetrieval:\n",
    "    def __init__(self, spider_tables_path: str = 'spider/tables.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load all Spider schemas\n",
    "        print(\"DEBUG: Loading Spider schemas from:\", spider_tables_path)\n",
    "        with open(spider_tables_path, 'r') as f:\n",
    "            self.schemas = json.load(f)\n",
    "        \n",
    "        # Create a mapping of db_id to schema\n",
    "        self.db_schemas = {schema['db_id']: schema for schema in self.schemas}\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize database-specific components\n",
    "        self.column_indices = {}\n",
    "        self.vectorizers = {}\n",
    "        self.lsh_indices = {}\n",
    "        \n",
    "        # Build indices for each database\n",
    "        self._build_indices()\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "\n",
    "    def _get_schema_relationships(self, schema: Dict) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Extract primary and foreign keys for a specific database.\"\"\"\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        primary_keys = schema.get('primary_keys', [])\n",
    "        foreign_keys = schema.get('foreign_keys', [])\n",
    "        \n",
    "        # Format primary keys\n",
    "        formatted_pks = []\n",
    "        for pk in primary_keys:\n",
    "            table_idx, col_name = column_names[pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = table_names[table_idx]\n",
    "                formatted_pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        # Format foreign keys\n",
    "        formatted_fks = []\n",
    "        for fk in foreign_keys:\n",
    "            fk_col = column_names[fk[0]]\n",
    "            pk_col = column_names[fk[1]]\n",
    "            fk_table = table_names[fk_col[0]]\n",
    "            pk_table = table_names[pk_col[0]]\n",
    "            formatted_fks.append(\n",
    "                f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\"\n",
    "            )\n",
    "            \n",
    "        return formatted_pks, formatted_fks\n",
    "\n",
    "    def _parse_numeric_value(self, word: str) -> str:\n",
    "        \"\"\"Parse numeric values from words.\"\"\"\n",
    "        if 'billion' in word.lower():\n",
    "            return '1000000000'\n",
    "        elif 'million' in word.lower():\n",
    "            return '1000000'\n",
    "        return word\n",
    "    \n",
    "    def _build_indices(self):\n",
    "        \"\"\"Build all necessary indices for all databases.\"\"\"\n",
    "        for db_id, schema in self.db_schemas.items():\n",
    "            # Build column index\n",
    "            self.column_indices[db_id] = self._build_column_index(schema)\n",
    "            \n",
    "            # Build vectorizer and LSH\n",
    "            terms = self._get_schema_terms(schema)\n",
    "            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "            term_vectors = vectorizer.fit_transform(terms)\n",
    "            \n",
    "            self.vectorizers[db_id] = {\n",
    "                'vectorizer': vectorizer,\n",
    "                'terms': terms\n",
    "            }\n",
    "            \n",
    "            self.lsh_indices[db_id] = {\n",
    "                'lsh': PSLsh(term_vectors, n_planes=10, n_tables=5),\n",
    "                'vectors': term_vectors\n",
    "            }\n",
    "\n",
    "    def _build_column_index(self, schema: Dict) -> Dict:\n",
    "        \"\"\"Build column index for a specific database schema.\"\"\"\n",
    "        column_index = {}\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        column_types = schema.get('column_types', [])\n",
    "        \n",
    "        for (table_idx, col_name), col_type in zip(column_names[1:], column_types[1:]):  # Skip first row (*) \n",
    "            if table_idx != -1:  # Skip table_idx == -1 which represents '*'\n",
    "                table_name = table_names[table_idx].lower()\n",
    "                qualified_name = f\"{table_name}.{col_name.lower()}\"\n",
    "                \n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': col_name.lower(),\n",
    "                    'type': col_type,\n",
    "                    'words': self._split_column_name(col_name),\n",
    "                    'synonyms': self._get_column_synonyms(col_name)\n",
    "                }\n",
    "        \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        return list(set(words))  # For Spider, we'll just use the words themselves as synonyms\n",
    "\n",
    "    def _get_schema_terms(self, schema: Dict) -> List[str]:\n",
    "        \"\"\"Get all terms from a specific database schema.\"\"\"\n",
    "        terms = []\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        \n",
    "        for idx, table in enumerate(table_names):\n",
    "            table = table.lower()\n",
    "            terms.append(table)\n",
    "            \n",
    "            # Add column terms\n",
    "            table_columns = [(t_idx, col) for t_idx, col in column_names if t_idx == idx]\n",
    "            for _, column in table_columns:\n",
    "                terms.append(f\"{table}.{column.lower()}\")\n",
    "                \n",
    "        return terms\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _find_similar_words(self, word: str, db_id: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find similar words in the database-specific schema.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        matches = []\n",
    "        \n",
    "        # Direct matching with column names\n",
    "        column_index = self.column_indices[db_id]\n",
    "        for qualified_name, metadata in column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            if score > 0.6:\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # LSH-based matching as backup\n",
    "        if len(matches) < 5:\n",
    "            try:\n",
    "                vectorizer = self.vectorizers[db_id]['vectorizer']\n",
    "                terms = self.vectorizers[db_id]['terms']\n",
    "                word_vector = vectorizer.transform([word])\n",
    "                \n",
    "                lsh = self.lsh_indices[db_id]['lsh']\n",
    "                vectors = self.lsh_indices[db_id]['vectors']\n",
    "                \n",
    "                candidate_indices = lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = terms[idx]\n",
    "                    if not any(term == match[0] for match in matches):\n",
    "                        candidate_vector = vectors[idx].toarray()[0]\n",
    "                        word_vector_array = word_vector.toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector_array - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return matches[:5]\n",
    "\n",
    "    def _extract_keywords(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Extract keywords with database-specific context.\"\"\"\n",
    "        schema = self.db_schemas[db_id]\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        system_prompt = f\"\"\"Given this database schema:\n",
    "        Tables: {schema['table_names_original']}\n",
    "        Columns: {schema['column_names_original']}\n",
    "        \n",
    "        Primary Keys: {primary_keys}\n",
    "        Foreign Keys: {foreign_keys}\n",
    "\n",
    "        Extract relevant keywords, keyphrases, and numerical values from the question in JSON format.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "            ],\n",
    "            functions=[\n",
    "                {\n",
    "                    \"name\": \"extract_components\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"keyphrases\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"numerical_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            function_call={\"name\": \"extract_components\"}\n",
    "        )\n",
    "\n",
    "        # Access the function_call attribute directly\n",
    "        function_call = response.choices[0].message.function_call\n",
    "        arguments = function_call.arguments\n",
    "        extracted_info = json.loads(arguments)\n",
    "\n",
    "        # Debugging statement\n",
    "        print(\"Extracted Info:\", extracted_info)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "\n",
    "    def process_question(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Process question with database context.\"\"\"\n",
    "        # Extract keywords using database-specific schema\n",
    "        extracted_info = self._extract_keywords(question, db_id)\n",
    "        \n",
    "        # Process words\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Words Extracted:\", words)\n",
    "\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "        \n",
    "        processed_words = list(set(processed_words))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Processed Words:\", processed_words)\n",
    "\n",
    "        # Find similar columns using database-specific indices\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self._find_similar_words(word, db_id)\n",
    "            # Debugging statement\n",
    "            print(f\"Similar matches for '{word}': {similar_matches[word]}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self._get_schema_relationships(self.db_schemas[db_id])[0],\n",
    "                \"foreign_keys\": self._get_schema_relationships(self.db_schemas[db_id])[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "        \n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # Use database-specific indices and relationships\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                top_match = matches[0]\n",
    "                if top_match[1] > 0.7:\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        # Handle numerical values\n",
    "                        value = self._parse_numeric_value(word)\n",
    "                        table_columns.append(f\"{top_match[0]} > {value}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "\n",
    "        # Get relevant tables for this database\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].lower())\n",
    "        \n",
    "        # Add relevant primary keys\n",
    "        for pk in primary_keys:\n",
    "            table = pk.split('.')[0].lower()\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "                \n",
    "        # Add relevant foreign keys\n",
    "        for fk in foreign_keys:\n",
    "            tables_in_fk = set(part.split('.')[0].lower() for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        \n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys,\n",
    "            \"schema_links\": table_columns  # Added for DIN SQL compatibility\n",
    "        }\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Schema Dict:\", schema_dict)\n",
    "        \n",
    "        return str(schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def classification_prompt_maker(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Create classification prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format schema info for the prompt\n",
    "    schema_info = f\"Tables: {schema['table_names_original']}\\nColumns: {schema['column_names_original']}\"\n",
    "    \n",
    "    instruction = \"\"\"Given the database schema:\n",
    "{schema_info}\n",
    "\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "Classify the question as:\n",
    "- EASY: no JOIN and no nested queries needed\n",
    "- NON-NESTED: needs JOIN but no nested queries\n",
    "- NESTED: needs nested queries\n",
    "\n",
    "Question: \"{question}\"\n",
    "Schema Links: {schema_links}\n",
    "\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "    # Get formatted keys\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "    \n",
    "    return instruction.format(\n",
    "        schema_info=schema_info,\n",
    "        primary_keys=primary_keys,\n",
    "        foreign_keys=foreign_keys,\n",
    "        question=question,\n",
    "        schema_links=schema_links\n",
    "    )\n",
    "\n",
    "def process_question_classification(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Process question classification with Spider database context.\"\"\"\n",
    "    def extract_classification(text):\n",
    "        print(f\"Trying to extract classification from: {text}\")\n",
    "        text = text.upper()\n",
    "        \n",
    "        for class_type in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "            if class_type in text:\n",
    "                return class_type\n",
    "                \n",
    "        patterns = [\"LABEL:\", \"CLASSIFICATION:\", \"CAN BE CLASSIFIED AS\"]\n",
    "        for pattern in patterns:\n",
    "            if pattern in text:\n",
    "                parts = text.split(pattern)\n",
    "                if len(parts) > 1:\n",
    "                    result = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    classification = result.split()[0].strip()\n",
    "                    if classification in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "                        return classification\n",
    "        \n",
    "        return \"NESTED\"  # Default fallback\n",
    "\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=spider_schemas\n",
    "                    )\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "            print(\"Raw response:\", raw_response)\n",
    "            classification = extract_classification(raw_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    final_class = classification if classification else \"NESTED\"\n",
    "    return f'\"{final_class}\"'\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        if \"SQL:\" in text:\n",
    "            return text.split(\"SQL:\")[-1].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    def make_spider_prompt(template_type):\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        # Spider-specific examples\n",
    "        examples = {\n",
    "            \"easy\": '''Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club''',\n",
    "            \n",
    "            \"medium\": '''Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id''',\n",
    "            \n",
    "            \"hard\": '''Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)'''\n",
    "        }\n",
    "\n",
    "        template = examples[template_type]\n",
    "        prompt = f\"\"\"Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "{template}\n",
    "\n",
    "Generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if '\"EASY\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"easy\")\n",
    "            elif '\"NON-NESTED\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"medium\")\n",
    "            else:\n",
    "                prompt = make_spider_prompt(\"hard\")\n",
    "\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            sql = extract_sql(response.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                sql = \"SELECT\"\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from datetime import datetime\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "final_output_schema_json = json.dumps({\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"user_nlp_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The original natural language query to be translated into SQL\"\n",
    "        },\n",
    "        \"reasonings\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"thought\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A thought about the user's question\"\n",
    "                    },\n",
    "                    \"helpful\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Whether the thought is helpful to solving the user's question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"description\": \"Step-by-step reasoning process for query generation\"\n",
    "        },\n",
    "        \"generated_sql_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The final SQL query that answers the natural language question\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Define comprehensive thought instructions\n",
    "thought_instructions = \"\"\"\n",
    "```\n",
    "Thought Instructions:\n",
    "```\n",
    "\n",
    "```\n",
    "1. Initial Analysis\n",
    "- Identify the core request in the question\n",
    "- Map question terms to database schema elements\n",
    "- Determine if aggregation is needed\n",
    "```\n",
    "\n",
    "```\n",
    "2. Complexity Assessment\n",
    "- Evaluate if joins are needed\n",
    "- Check if subqueries or CTEs are required\n",
    "- Determine grouping requirements\n",
    "```\n",
    "\n",
    "```\n",
    "3. Schema Analysis\n",
    "- Identify primary tables needed\n",
    "- Map columns to required data\n",
    "- Understand table relationships\n",
    "```\n",
    "\n",
    "```\n",
    "4. Query Planning\n",
    "- Determine optimal join order if needed\n",
    "- Plan filtering conditions\n",
    "- Consider performance implications\n",
    "```\n",
    "\n",
    "```\n",
    "5. SQL Components\n",
    "- Select clause composition\n",
    "- From clause and join structure\n",
    "- Where clause conditions\n",
    "- Group by and having requirements\n",
    "```\n",
    "\n",
    "```\n",
    "6. Final Validation\n",
    "- Verify schema compatibility\n",
    "- Check column name accuracy\n",
    "- Ensure proper syntax\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "reasoning_instructions = \"\"\"\n",
    "```\n",
    "SQL Generation Guidelines:\n",
    "1. Use COUNT(*) for simple counts\n",
    "2. Avoid unnecessary DISTINCT\n",
    "3. Use table aliases for clarity\n",
    "4. Include proper JOIN conditions\n",
    "5. Handle NULL values appropriately\n",
    "```\n",
    "\n",
    "```\n",
    "Format Requirements:\n",
    "1. Use clear indentation\n",
    "2. Align related clauses\n",
    "3. Use meaningful aliases\n",
    "4. Format complex conditions clearly\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"Thought structure with improved validation\"\"\"\n",
    "    thought: str = Field(description=\"The reasoning step\")\n",
    "    helpful: bool = Field(default=True)\n",
    "\n",
    "class FinalOutput(BaseModel):\n",
    "    \"\"\"Complete output structure containing the query, reasoning, and SQL\"\"\"\n",
    "    user_nlp_query: str = Field(\n",
    "        description=\"The original natural language query to be translated into SQL\"\n",
    "    )\n",
    "    reasonings: List[Thought] = Field(\n",
    "        description=\"Step-by-step reasoning process for query generation\"\n",
    "    )\n",
    "    generated_sql_query: str = Field(\n",
    "        description=\"The final SQL query that answers the natural language question\"\n",
    "    )\n",
    "\n",
    "def create_prompt(question: str, schema_links: List[str], schema: Dict[str, Any], complexity: str) -> str:\n",
    "    \"\"\"Create enhanced prompt with better examples and guidance\"\"\"\n",
    "    \n",
    "    examples = {\n",
    "        \"EASY\": \"\"\"\n",
    "Example:\n",
    "Q: \"How many students are there?\"\n",
    "A: This requires a simple count from the student table\n",
    "SQL: SELECT COUNT(*) FROM student\n",
    "\"\"\",\n",
    "        \"NON-NESTED\": \"\"\"\n",
    "Example:\n",
    "Q: \"List student names and their department names\"\n",
    "A: This requires joining student and department tables\n",
    "SQL: SELECT s.name, d.name \n",
    "FROM student s\n",
    "JOIN department d ON s.dept_id = d.id\n",
    "\"\"\",\n",
    "        \"NESTED\": \"\"\"\n",
    "Example:\n",
    "Q: \"Find students with above average grades\"\n",
    "A: This requires a subquery to calculate the average\n",
    "SQL: SELECT name \n",
    "FROM student \n",
    "WHERE grade > (SELECT AVG(grade) FROM student)\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    return f\"\"\"You are an expert SQL developer tasked with generating precise SQL queries.\n",
    "\n",
    "SCHEMA INFORMATION:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "QUESTION: {question}\n",
    "SCHEMA_LINKS: {schema_links}\n",
    "COMPLEXITY: {complexity}\n",
    "\n",
    "{thought_instructions}\n",
    "\n",
    "{reasoning_instructions}\n",
    "\n",
    "{examples.get(complexity, examples['EASY'])}\n",
    "\n",
    "GUIDELINES:\n",
    "1. Generate clear, efficient SQL\n",
    "2. Use proper table aliases\n",
    "3. Include complete reasoning\n",
    "4. Match schema exactly\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "{final_output_schema_json} \"\"\"\n",
    "\n",
    "def process_question_sql(\n",
    "    question: str,\n",
    "    predicted_class: str,\n",
    "    schema_links: List[str],\n",
    "    db_id: str,\n",
    "    spider_schemas: List[Dict[str, Any]],\n",
    "    max_retries: int = 3\n",
    ") -> FinalOutput:\n",
    "    \"\"\"Generate SQL with thoughts and reasoning\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get schema\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                prompt = create_prompt(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema,\n",
    "                    complexity=predicted_class\n",
    "                )\n",
    "                \n",
    "                client = OpenAI()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\", \n",
    "                            \"content\": \"\"\"You are an SQL expert. Return JSON with this exact format:\n",
    "                            {\n",
    "                                \"user_nlp_query\": \"the original question\",\n",
    "                                \"reasonings\": [\n",
    "                                    {\"thought\": \"your reasoning step\", \"helpful\": true}\n",
    "                                ],\n",
    "                                \"generated_sql_query\": \"your SQL query\"\n",
    "                            }\"\"\"\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1500,\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content\n",
    "                print(f\"Raw GPT response: {content}\")  # Debug print\n",
    "                \n",
    "                try:\n",
    "                    result = json.loads(content)\n",
    "                    return FinalOutput(\n",
    "                        user_nlp_query=result.get(\"user_nlp_query\", question),\n",
    "                        reasonings=[\n",
    "                            Thought(**thought) for thought in result.get(\"reasonings\", [])\n",
    "                        ] or [Thought(thought=\"Direct SQL generation\", helpful=True)],\n",
    "                        generated_sql_query=result.get(\"generated_sql_query\", \"SELECT 1\")\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing response: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return FinalOutput(\n",
    "                            user_nlp_query=question,\n",
    "                            reasonings=[\n",
    "                                Thought(\n",
    "                                    thought=f\"Failed to parse response: {str(e)}\",\n",
    "                                    helpful=False\n",
    "                                )\n",
    "                            ],\n",
    "                            generated_sql_query=\"SELECT 1\"\n",
    "                        )\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    return FinalOutput(\n",
    "                        user_nlp_query=question,\n",
    "                        reasonings=[\n",
    "                            Thought(\n",
    "                                thought=f\"Error in process: {str(e)}\",\n",
    "                                helpful=False\n",
    "                            )\n",
    "                        ],\n",
    "                        generated_sql_query=\"SELECT 1\"\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return FinalOutput(\n",
    "            user_nlp_query=question,\n",
    "            reasonings=[\n",
    "                Thought(\n",
    "                    thought=f\"Critical error: {str(e)}\",\n",
    "                    helpful=False\n",
    "                )\n",
    "            ],\n",
    "            generated_sql_query=\"SELECT 1\"\n",
    "        )\n",
    "\n",
    "    return FinalOutput(\n",
    "        user_nlp_query=question,\n",
    "        reasonings=[\n",
    "            Thought(\n",
    "                thought=\"Maximum retries exceeded\",\n",
    "                helpful=False\n",
    "            )\n",
    "        ],\n",
    "        generated_sql_query=\"SELECT 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def easy_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for easy Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "SQL:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for medium (non-nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def hard_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for hard (nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def debugger(question: str, sql: str, predicted_class: str, schema_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Create debug prompt based on query complexity.\"\"\"\n",
    "    \n",
    "    if '\"EASY\"' in predicted_class:\n",
    "        prompt_used = easy_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "    elif '\"NON-NESTED\"' in predicted_class:\n",
    "        prompt_used = medium_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "    else:\n",
    "        prompt_used = hard_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "\n",
    "    instruction = f\"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to check if the given SQLite SQL QUERY has any issues. If there are any issues, fix them and return the fixed SQLite QUERY in the output. If there are no issues, return the SQLite SQL QUERY as is in the output.\n",
    "#### Background Information:\n",
    "Relevant Schema Links: {schema_dict.get(\"schema_links\", [])}\n",
    "Prompt Used to Generate the Candidate SQLite SQL Query:\n",
    "'''\n",
    "{prompt_used}\n",
    "'''\n",
    "#### Use the following instructions for fixing the SQL QUERY:\n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT only when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\n",
    "\n",
    "#### Question: {question}\n",
    "#### SQLite SQL QUERY\n",
    "{sql}\n",
    "#### SQLite FIXED SQL QUERY\n",
    "\"\"\"\n",
    "\n",
    "    return instruction\n",
    "\n",
    "def GPT4_debug(prompt: str) -> str:\n",
    "    \"\"\"Debug SQL using GPT-4.\"\"\"\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a SQL expert. Return only the fixed SQL query with no explanation.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.0,\n",
    "            max_tokens=350,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"#\", \";\", \"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT4_debug: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def refine_query(question: str, sql: str, predicted_class: str, schema_links: List[str], db_id: str, spider_schemas: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Refine and debug the SQL query.\"\"\"\n",
    "    max_attempts = 3\n",
    "    attempt = 0\n",
    "    debugged_SQL = None\n",
    "    \n",
    "    # Get schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        return sql\n",
    "\n",
    "    # Create schema dict with required format\n",
    "    schema_dict = {\n",
    "        \"table_names_original\": schema.get('table_names_original', []),\n",
    "        \"column_names_original\": schema.get('column_names_original', []),\n",
    "        \"schema_links\": schema_links,\n",
    "        \"primary_keys\": schema.get('primary_keys', []),\n",
    "        \"foreign_keys\": schema.get('foreign_keys', [])\n",
    "    }\n",
    "    \n",
    "    while debugged_SQL is None and attempt < max_attempts:\n",
    "        try:\n",
    "            debug_prompt = debugger(\n",
    "                question=question,\n",
    "                sql=sql,\n",
    "                predicted_class=predicted_class,\n",
    "                schema_dict=schema_dict\n",
    "            )\n",
    "            debugged_SQL = GPT4_debug(debug_prompt)\n",
    "            \n",
    "            if debugged_SQL:\n",
    "                # Clean up the response\n",
    "                debugged_SQL = debugged_SQL.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                try:\n",
    "                    # Try to extract SQL if wrapped in markdown\n",
    "                    if \"```\" in debugged_SQL:\n",
    "                        debugged_SQL = debugged_SQL.split(\"```sql\")[-1].split(\"```\")[0].strip()\n",
    "                except:\n",
    "                    # If extraction fails, use the whole response\n",
    "                    pass\n",
    "                \n",
    "                print(f\"Refined SQL (Attempt {attempt + 1}):\", debugged_SQL)\n",
    "                return debugged_SQL\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in refinement attempt {attempt + 1}: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available modes:\n",
      "1. dev (1034 questions)\n",
      "2. train_spider (7000 questions)\n",
      "3. train_others (1659 questions)\n",
      "Loading dev.json...\n",
      "Loaded 1034 questions\n",
      "DEBUG: Loading Spider schemas from: /Users/virounikamina/Desktop/spider_data/tables.json\n",
      "\n",
      "Testing 2 questions\n",
      "\n",
      "Processing Question 1/2\n",
      "Question: How many singers do we have?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['singers', 'count'], 'keyphrases': ['number of singers'], 'numerical_values': []}\n",
      "Words Extracted: ['singers', 'count', 'number of singers']\n",
      "Processed Words: ['singer', 'number', 'count']\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', 0.8)]\n",
      "Similar matches for 'number': []\n",
      "Similar matches for 'count': [('singer.country', 0.83), ('concert.concert_id', 0.67), ('concert.concert_name', 0.67), ('singer_in_concert.concert_id', 0.67)]\n",
      "Table Columns: ['singer.singer_id', 'singer.country']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.country']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.country']}\n",
      "Raw response: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table by counting the number of unique 'Singer_ID'. There is no need to join with other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table by counting the number of unique 'Singer_ID'. There is no need to join with other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "Raw GPT response: {\n",
      "    \"user_nlp_query\": \"How many singers do we have?\",\n",
      "    \"reasonings\": [\n",
      "        {\"thought\": \"The question asks for the count of singers, which maps to the 'singer' table in the schema.\", \"helpful\": true},\n",
      "        {\"thought\": \"Since we need to count the number of singers, we should use the COUNT function on the 'singer' table.\", \"helpful\": true},\n",
      "        {\"thought\": \"There is no need for joins or additional conditions as we are only interested in the total number of singers.\", \"helpful\": true},\n",
      "        {\"thought\": \"The 'singer' table has a primary key 'Singer_ID', which can be used to count distinct singers.\", \"helpful\": true},\n",
      "        {\"thought\": \"Using COUNT(*) is appropriate here as we are counting all entries in the 'singer' table.\", \"helpful\": true}\n",
      "    ],\n",
      "    \"generated_sql_query\": \"SELECT COUNT(*) FROM singer\"\n",
      "}\n",
      "Reasoning Steps:\n",
      "- The question asks for the count of singers, which maps to the 'singer' table in the schema.\n",
      "- Since we need to count the number of singers, we should use the COUNT function on the 'singer' table.\n",
      "- There is no need for joins or additional conditions as we are only interested in the total number of singers.\n",
      "- The 'singer' table has a primary key 'Singer_ID', which can be used to count distinct singers.\n",
      "- Using COUNT(*) is appropriate here as we are counting all entries in the 'singer' table.\n",
      "Initial SQL: SELECT COUNT(*) FROM singer\n",
      "Refined SQL (Attempt 1): SELECT COUNT(*) FROM singer\n",
      "Final SQL: SELECT COUNT(*) FROM singer\n",
      "Ground Truth: SELECT count(*) FROM singer\n",
      "\n",
      "Processing Question 2/2\n",
      "Question: What is the total number of singers?\n",
      "Database: concert_singer\n",
      "Extracted Info: {'keywords': ['total', 'number', 'singers'], 'keyphrases': ['total number of singers']}\n",
      "Words Extracted: ['total', 'number', 'singers', 'total number of singers']\n",
      "Processed Words: ['singer', 'number', 'total']\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', 0.8)]\n",
      "Similar matches for 'number': []\n",
      "Similar matches for 'total': []\n",
      "Table Columns: ['singer.singer_id']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id']}\n",
      "Schema Links: {'table_columns': ['singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id']}\n",
      "Raw response: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This operation does not require joining multiple tables or using nested queries. Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This operation does not require joining multiple tables or using nested queries. Therefore, the question is classified as EASY.\n",
      "Classification: \"EASY\"\n",
      "Raw GPT response: {\n",
      "    \"user_nlp_query\": \"What is the total number of singers?\",\n",
      "    \"reasonings\": [\n",
      "        {\"thought\": \"The question asks for the total number of singers, which implies a count of entries in the singer table.\", \"helpful\": true},\n",
      "        {\"thought\": \"The singer table contains a unique identifier for each singer, which is the Singer_ID column.\", \"helpful\": true},\n",
      "        {\"thought\": \"Since we are counting the total number of singers, we do not need to join with any other tables.\", \"helpful\": true},\n",
      "        {\"thought\": \"Using COUNT(*) on the singer table will give us the total number of singers.\", \"helpful\": true}\n",
      "    ],\n",
      "    \"generated_sql_query\": \"SELECT COUNT(*) FROM singer\"\n",
      "}\n",
      "Reasoning Steps:\n",
      "- The question asks for the total number of singers, which implies a count of entries in the singer table.\n",
      "- The singer table contains a unique identifier for each singer, which is the Singer_ID column.\n",
      "- Since we are counting the total number of singers, we do not need to join with any other tables.\n",
      "- Using COUNT(*) on the singer table will give us the total number of singers.\n",
      "Initial SQL: SELECT COUNT(*) FROM singer\n",
      "Refined SQL (Attempt 1): SELECT COUNT(*) FROM singer\n",
      "Final SQL: SELECT COUNT(*) FROM singer\n",
      "Ground Truth: SELECT count(*) FROM singer\n",
      "\n",
      "=== Testing Summary ===\n",
      "Total Questions Processed: 0\n",
      "Successful Matches: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "from openai import OpenAI\n",
    "\n",
    "class SpiderTester:\n",
    "    def __init__(self, spider_path: str = '/Users/virounikamina/Desktop/spider_data', mode='dev'):\n",
    "        \"\"\"Initialize Spider tester with dataset path and mode.\"\"\"\n",
    "        self.spider_path = spider_path\n",
    "        \n",
    "        # Load schemas\n",
    "        with open(os.path.join(spider_path, 'tables.json'), 'r') as f:\n",
    "            self.spider_schemas = json.load(f)\n",
    "            \n",
    "        # Select correct data file\n",
    "        data_files = {\n",
    "            'dev': 'dev.json',\n",
    "            'train_spider': 'train_spider.json',\n",
    "            'train_others': 'train_others.json'\n",
    "        }\n",
    "        \n",
    "        if mode not in data_files:\n",
    "            raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "            \n",
    "        data_file = data_files[mode]\n",
    "        print(f\"Loading {data_file}...\")\n",
    "        \n",
    "        # Load test data\n",
    "        with open(os.path.join(spider_path, data_file), 'r') as f:\n",
    "            self.test_data = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.test_data)} questions\")\n",
    "            \n",
    "        # Create results file\n",
    "        self.results_file = os.path.join(\n",
    "            spider_path, \n",
    "            f'results_{mode}_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        )\n",
    "        \n",
    "        # Initialize value retrieval if needed\n",
    "        self.value_retrieval = SpiderValueRetrieval(\n",
    "            spider_tables_path=os.path.join(spider_path, 'tables.json')\n",
    "        )\n",
    "\n",
    "    def log(self, message: str, question_num: int = None):\n",
    "        \"\"\"Log message to file and console with optional question number.\"\"\"\n",
    "        if question_num is not None:\n",
    "            message = f\"Question {question_num}: {message}\"\n",
    "            \n",
    "        print(message)\n",
    "        with open(self.results_file, 'a') as f:\n",
    "            f.write(message + '\\n')\n",
    "\n",
    "    def run_test(self, num_questions: int = None):\n",
    "        if num_questions is None:\n",
    "            num_questions = len(self.test_data)\n",
    "        else:\n",
    "            num_questions = min(num_questions, len(self.test_data))\n",
    "\n",
    "        processed = 0\n",
    "        successful = 0\n",
    "        \n",
    "        self.log(f\"\\nTesting {num_questions} questions\")\n",
    "        \n",
    "        for idx, test_case in enumerate(self.test_data[:num_questions]):\n",
    "            try:\n",
    "                question = test_case['question']\n",
    "                db_id = test_case['db_id']\n",
    "                ground_truth = test_case['query']\n",
    "                \n",
    "                self.log(f\"\\nProcessing Question {idx + 1}/{num_questions}\")\n",
    "                self.log(f\"Question: {question}\")\n",
    "                self.log(f\"Database: {db_id}\")\n",
    "                \n",
    "                # Get schema links\n",
    "                schema_links = self.value_retrieval.process_schema(question, db_id)\n",
    "                self.log(f\"Schema Links: {schema_links}\")\n",
    "                \n",
    "                try:\n",
    "                    # Get classification\n",
    "                    classification = process_question_classification(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    self.log(f\"Classification: {classification}\")\n",
    "                    \n",
    "                    # Get initial SQL with reasoning\n",
    "                    process_thesql = process_question_sql(\n",
    "                        question=question,\n",
    "                        predicted_class=classification,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    \n",
    "                    self.log(\"Reasoning Steps:\")\n",
    "                    for thought in process_thesql.reasonings:\n",
    "                        self.log(f\"- {thought.thought}\")\n",
    "                        \n",
    "                    self.log(f\"Initial SQL: {process_thesql.generated_sql_query}\")\n",
    "                    \n",
    "                    # Refine the SQL with all required parameters\n",
    "                    final_sql = refine_query(\n",
    "                        question=question,\n",
    "                        sql=process_thesql.generated_sql_query,\n",
    "                        predicted_class=classification,  # Added\n",
    "                        schema_links=schema_links,      # Added\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    \n",
    "                    self.log(f\"Final SQL: {final_sql}\")\n",
    "                    self.log(f\"Ground Truth: {ground_truth}\")\n",
    "                    \n",
    "                    # Compare results\n",
    "                    #if self.compare_sql(final_sql, ground_truth):\n",
    "                     #   successful += 1\n",
    "                     #   self.log(\" Match\")\n",
    "                    #else:\n",
    "                    #    self.log(\" No match\")\n",
    "                        \n",
    "                    #processed += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.log(f\"Error processing question: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error in test case: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Print summary\n",
    "        self.log(\"\\n=== Testing Summary ===\")\n",
    "        self.log(f\"Total Questions Processed: {processed}\")\n",
    "        self.log(f\"Successful Matches: {successful}\")\n",
    "        if processed > 0:\n",
    "            self.log(f\"Success Rate: {(successful/processed)*100:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for Spider testing.\"\"\"\n",
    "    print(\"Available modes:\")\n",
    "    print(\"1. dev (1034 questions)\")\n",
    "    print(\"2. train_spider (7000 questions)\")\n",
    "    print(\"3. train_others (1659 questions)\")\n",
    "    \n",
    "    mode = input(\"Choose mode (dev/train_spider/train_others) [default: dev]: \").strip() or 'dev'\n",
    "    num_questions = input(\"How many questions to process? (press Enter for all): \").strip()\n",
    "    \n",
    "    try:\n",
    "        tester = SpiderTester(mode=mode)\n",
    "        tester.run_test(num_questions=int(num_questions) if num_questions else None)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
