{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir_path = \"/Users/virounikamina/Desktop/spider_data\"\n",
    "SCHEMA_FILE = \"/Users/virounikamina/Desktop/spider_data/tables.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_databases(dir_path):\n",
    "    \"\"\"\n",
    "    List all SQLite database files in the Spider dataset.\n",
    "    \"\"\"\n",
    "    db_files = []\n",
    "    db_paths = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".sqlite\"):\n",
    "                db_files.append(file)\n",
    "                db_paths.append(os.path.join(root, file))\n",
    "    return db_files, db_paths\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"\n",
    "    Connect to a specific SQLite database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def list_tables(conn):\n",
    "    \"\"\"\n",
    "    List all tables in the connected SQLite database.\n",
    "    \"\"\"\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def preview_table(conn, table_name, limit=5):\n",
    "    \"\"\"\n",
    "    Preview data from a specific table.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    columns = [desc[0] for desc in cursor.description]  # Column names\n",
    "    rows = cursor.fetchall()  # Data rows\n",
    "    return columns, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to: /Users/virounikamina/Desktop/spider_data/database/browser_web/browser_web.sqlite\n",
      "# Tables:  3\n",
      "\n",
      "Tables in the database:\n",
      "1: Web_client_accelerator\n",
      "2: browser\n",
      "3: accelerator_compatible_browser\n",
      "\n",
      "Previewing table: Web_client_accelerator\n",
      "\n",
      "Columns: ['id', 'name', 'Operating_system', 'Client', 'Connection']\n",
      "\n",
      "Rows:\n",
      "(1, 'CACHEbox', 'Appliance (Linux)', 'End user, ISP', 'Broadband, Satellite, Wireless, Fiber, DSL')\n",
      "(2, 'CProxy', 'Windows', 'user', 'up to 756kbit/s')\n",
      "(3, 'Fasterfox', 'Windows, Mac, Linux and Mobile devices', 'user', 'Dialup, Wireless, Broadband, DSL')\n",
      "(4, 'fasTun', 'Any', 'All', 'Any')\n",
      "(5, 'Freewire', 'Windows, except NT and 95', 'ISP', 'Dial-up')\n",
      "\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "db_names, db_paths = list_databases(database_dir_path)\n",
    "# print(\"Available Databases:\")\n",
    "# for idx, db_name in enumerate(db_names):\n",
    "#     print(f\"{idx + 1}: {db_name}\")\n",
    "\n",
    "# Select a database to open\n",
    "db_index = int(input(\"\\nEnter the number of the database to open: \")) - 1\n",
    "db_path = db_paths[db_index]\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "print(f\"\\nConnected to: {db_path}\")\n",
    "\n",
    "# List tables in the database\n",
    "tables = list_tables(conn)\n",
    "print(\"# Tables: \", len(tables))\n",
    "print(\"\\nTables in the database:\")\n",
    "for idx, table in enumerate(tables):\n",
    "    print(f\"{idx + 1}: {table}\")\n",
    "\n",
    "# Select a table to preview\n",
    "table_index = int(input(\"\\nEnter the number of the table to preview: \")) - 1\n",
    "table_name = tables[table_index]\n",
    "\n",
    "# Preview the selected table\n",
    "print(f\"\\nPreviewing table: {table_name}\")\n",
    "columns, rows = preview_table(conn, table_name)\n",
    "print(\"\\nColumns:\", columns)\n",
    "print(\"\\nRows:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"\\nConnection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "class PSLsh:\n",
    "    \"\"\"Locality-Sensitive Hashing implementation for fast approximate nearest neighbor search.\"\"\"\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random planes for each hash table\n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        \"\"\"Build hash tables from input vectors.\"\"\"\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        \"\"\"Generate hash codes for a vector.\"\"\"\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        \"\"\"Find candidate nearest neighbors for a query vector.\"\"\"\n",
    "        vector = vector.toarray()[0]  # Convert sparse matrix to 1D array\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class SpiderValueRetrieval:\n",
    "    def __init__(self, spider_tables_path: str = 'spider/tables.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load all Spider schemas\n",
    "        print(\"DEBUG: Loading Spider schemas from:\", spider_tables_path)\n",
    "        with open(spider_tables_path, 'r') as f:\n",
    "            self.schemas = json.load(f)\n",
    "        \n",
    "        # Create a mapping of db_id to schema\n",
    "        self.db_schemas = {schema['db_id']: schema for schema in self.schemas}\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize database-specific components\n",
    "        self.column_indices = {}\n",
    "        self.vectorizers = {}\n",
    "        self.lsh_indices = {}\n",
    "        \n",
    "        # Build indices for each database\n",
    "        self._build_indices()\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "\n",
    "    def _get_schema_relationships(self, schema: Dict) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Extract primary and foreign keys for a specific database.\"\"\"\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        primary_keys = schema.get('primary_keys', [])\n",
    "        foreign_keys = schema.get('foreign_keys', [])\n",
    "        \n",
    "        # Format primary keys\n",
    "        formatted_pks = []\n",
    "        for pk in primary_keys:\n",
    "            table_idx, col_name = column_names[pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = table_names[table_idx]\n",
    "                formatted_pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        # Format foreign keys\n",
    "        formatted_fks = []\n",
    "        for fk in foreign_keys:\n",
    "            fk_col = column_names[fk[0]]\n",
    "            pk_col = column_names[fk[1]]\n",
    "            fk_table = table_names[fk_col[0]]\n",
    "            pk_table = table_names[pk_col[0]]\n",
    "            formatted_fks.append(\n",
    "                f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\"\n",
    "            )\n",
    "            \n",
    "        return formatted_pks, formatted_fks\n",
    "\n",
    "    def _parse_numeric_value(self, word: str) -> str:\n",
    "        \"\"\"Parse numeric values from words.\"\"\"\n",
    "        if 'billion' in word.lower():\n",
    "            return '1000000000'\n",
    "        elif 'million' in word.lower():\n",
    "            return '1000000'\n",
    "        return word\n",
    "    \n",
    "    def _build_indices(self):\n",
    "        \"\"\"Build all necessary indices for all databases.\"\"\"\n",
    "        for db_id, schema in self.db_schemas.items():\n",
    "            # Build column index\n",
    "            self.column_indices[db_id] = self._build_column_index(schema)\n",
    "            \n",
    "            # Build vectorizer and LSH\n",
    "            terms = self._get_schema_terms(schema)\n",
    "            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "            term_vectors = vectorizer.fit_transform(terms)\n",
    "            \n",
    "            self.vectorizers[db_id] = {\n",
    "                'vectorizer': vectorizer,\n",
    "                'terms': terms\n",
    "            }\n",
    "            \n",
    "            self.lsh_indices[db_id] = {\n",
    "                'lsh': PSLsh(term_vectors, n_planes=10, n_tables=5),\n",
    "                'vectors': term_vectors\n",
    "            }\n",
    "\n",
    "    def _build_column_index(self, schema: Dict) -> Dict:\n",
    "        \"\"\"Build column index for a specific database schema.\"\"\"\n",
    "        column_index = {}\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        column_types = schema.get('column_types', [])\n",
    "        \n",
    "        for (table_idx, col_name), col_type in zip(column_names[1:], column_types[1:]):  # Skip first row (*) \n",
    "            if table_idx != -1:  # Skip table_idx == -1 which represents '*'\n",
    "                table_name = table_names[table_idx].lower()\n",
    "                qualified_name = f\"{table_name}.{col_name.lower()}\"\n",
    "                \n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': col_name.lower(),\n",
    "                    'type': col_type,\n",
    "                    'words': self._split_column_name(col_name),\n",
    "                    'synonyms': self._get_column_synonyms(col_name)\n",
    "                }\n",
    "        \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        return list(set(words))  # For Spider, we'll just use the words themselves as synonyms\n",
    "\n",
    "    def _get_schema_terms(self, schema: Dict) -> List[str]:\n",
    "        \"\"\"Get all terms from a specific database schema.\"\"\"\n",
    "        terms = []\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        \n",
    "        for idx, table in enumerate(table_names):\n",
    "            table = table.lower()\n",
    "            terms.append(table)\n",
    "            \n",
    "            # Add column terms\n",
    "            table_columns = [(t_idx, col) for t_idx, col in column_names if t_idx == idx]\n",
    "            for _, column in table_columns:\n",
    "                terms.append(f\"{table}.{column.lower()}\")\n",
    "                \n",
    "        return terms\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _find_similar_words(self, word: str, db_id: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find similar words in the database-specific schema.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        matches = []\n",
    "        \n",
    "        # Direct matching with column names\n",
    "        column_index = self.column_indices[db_id]\n",
    "        for qualified_name, metadata in column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            if score > 0.6:\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # LSH-based matching as backup\n",
    "        if len(matches) < 5:\n",
    "            try:\n",
    "                vectorizer = self.vectorizers[db_id]['vectorizer']\n",
    "                terms = self.vectorizers[db_id]['terms']\n",
    "                word_vector = vectorizer.transform([word])\n",
    "                \n",
    "                lsh = self.lsh_indices[db_id]['lsh']\n",
    "                vectors = self.lsh_indices[db_id]['vectors']\n",
    "                \n",
    "                candidate_indices = lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = terms[idx]\n",
    "                    if not any(term == match[0] for match in matches):\n",
    "                        candidate_vector = vectors[idx].toarray()[0]\n",
    "                        word_vector_array = word_vector.toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector_array - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return matches[:5]\n",
    "\n",
    "    def _extract_keywords(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Extract keywords with database-specific context.\"\"\"\n",
    "        schema = self.db_schemas[db_id]\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        system_prompt = f\"\"\"Given this database schema:\n",
    "        Tables: {schema['table_names_original']}\n",
    "        Columns: {schema['column_names_original']}\n",
    "        \n",
    "        Primary Keys: {primary_keys}\n",
    "        Foreign Keys: {foreign_keys}\n",
    "\n",
    "        Extract relevant keywords, keyphrases, and numerical values from the question in JSON format.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "            ],\n",
    "            functions=[\n",
    "                {\n",
    "                    \"name\": \"extract_components\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"keyphrases\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"numerical_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            function_call={\"name\": \"extract_components\"}\n",
    "        )\n",
    "\n",
    "        # Access the function_call attribute directly\n",
    "        function_call = response.choices[0].message.function_call\n",
    "        arguments = function_call.arguments\n",
    "        extracted_info = json.loads(arguments)\n",
    "\n",
    "        # Debugging statement\n",
    "        print(\"Extracted Info:\", extracted_info)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "\n",
    "    def process_question(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Process question with database context.\"\"\"\n",
    "        # Extract keywords using database-specific schema\n",
    "        extracted_info = self._extract_keywords(question, db_id)\n",
    "        \n",
    "        # Process words\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Words Extracted:\", words)\n",
    "\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "        \n",
    "        processed_words = list(set(processed_words))\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Processed Words:\", processed_words)\n",
    "\n",
    "        # Find similar columns using database-specific indices\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self._find_similar_words(word, db_id)\n",
    "            # Debugging statement\n",
    "            print(f\"Similar matches for '{word}': {similar_matches[word]}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self._get_schema_relationships(self.db_schemas[db_id])[0],\n",
    "                \"foreign_keys\": self._get_schema_relationships(self.db_schemas[db_id])[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "        \n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # Use database-specific indices and relationships\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                top_match = matches[0]\n",
    "                if top_match[1] > 0.7:\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        # Handle numerical values\n",
    "                        value = self._parse_numeric_value(word)\n",
    "                        table_columns.append(f\"{top_match[0]} > {value}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "\n",
    "        # Get relevant tables for this database\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].lower())\n",
    "        \n",
    "        # Add relevant primary keys\n",
    "        for pk in primary_keys:\n",
    "            table = pk.split('.')[0].lower()\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "                \n",
    "        # Add relevant foreign keys\n",
    "        for fk in foreign_keys:\n",
    "            tables_in_fk = set(part.split('.')[0].lower() for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        \n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys,\n",
    "            \"schema_links\": table_columns  # Added for DIN SQL compatibility\n",
    "        }\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Schema Dict:\", schema_dict)\n",
    "        \n",
    "        return str(schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def classification_prompt_maker(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Create classification prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format schema info for the prompt\n",
    "    schema_info = f\"Tables: {schema['table_names_original']}\\nColumns: {schema['column_names_original']}\"\n",
    "    \n",
    "    instruction = \"\"\"Given the database schema:\n",
    "{schema_info}\n",
    "\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "Classify the question as:\n",
    "- EASY: no JOIN and no nested queries needed\n",
    "- NON-NESTED: needs JOIN but no nested queries\n",
    "- NESTED: needs nested queries\n",
    "\n",
    "Question: \"{question}\"\n",
    "Schema Links: {schema_links}\n",
    "\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "    # Get formatted keys\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "    \n",
    "    return instruction.format(\n",
    "        schema_info=schema_info,\n",
    "        primary_keys=primary_keys,\n",
    "        foreign_keys=foreign_keys,\n",
    "        question=question,\n",
    "        schema_links=schema_links\n",
    "    )\n",
    "\n",
    "def process_question_classification(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Process question classification with Spider database context.\"\"\"\n",
    "    def extract_classification(text):\n",
    "        print(f\"Trying to extract classification from: {text}\")\n",
    "        text = text.upper()\n",
    "        \n",
    "        for class_type in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "            if class_type in text:\n",
    "                return class_type\n",
    "                \n",
    "        patterns = [\"LABEL:\", \"CLASSIFICATION:\", \"CAN BE CLASSIFIED AS\"]\n",
    "        for pattern in patterns:\n",
    "            if pattern in text:\n",
    "                parts = text.split(pattern)\n",
    "                if len(parts) > 1:\n",
    "                    result = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    classification = result.split()[0].strip()\n",
    "                    if classification in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "                        return classification\n",
    "        \n",
    "        return \"NESTED\"  # Default fallback\n",
    "\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=spider_schemas\n",
    "                    )\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "            print(\"Raw response:\", raw_response)\n",
    "            classification = extract_classification(raw_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    final_class = classification if classification else \"NESTED\"\n",
    "    return f'\"{final_class}\"'\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        if \"SQL:\" in text:\n",
    "            return text.split(\"SQL:\")[-1].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    def make_spider_prompt(template_type):\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        # Spider-specific examples\n",
    "        examples = {\n",
    "            \"easy\": '''Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club''',\n",
    "            \n",
    "            \"medium\": '''Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id''',\n",
    "            \n",
    "            \"hard\": '''Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)'''\n",
    "        }\n",
    "\n",
    "        template = examples[template_type]\n",
    "        prompt = f\"\"\"Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "{template}\n",
    "\n",
    "Generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if '\"EASY\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"easy\")\n",
    "            elif '\"NON-NESTED\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"medium\")\n",
    "            else:\n",
    "                prompt = make_spider_prompt(\"hard\")\n",
    "\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            sql = extract_sql(response.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                sql = \"SELECT\"\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "from datetime import datetime\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "\n",
    "final_output_schema_json = json.dumps({\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"user_nlp_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The original natural language query to be translated into SQL\"\n",
    "        },\n",
    "        \"reasonings\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"thought\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A thought about the user's question\"\n",
    "                    },\n",
    "                    \"helpful\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Whether the thought is helpful to solving the user's question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"description\": \"Step-by-step reasoning process for query generation\"\n",
    "        },\n",
    "        \"generated_sql_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The final SQL query that answers the natural language question\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Define comprehensive thought instructions\n",
    "thought_instructions = \"\"\"\n",
    "```\n",
    "Thought Instructions:\n",
    "```\n",
    "\n",
    "```\n",
    "1. Initial Analysis\n",
    "- Identify the core request in the question\n",
    "- Map question terms to database schema elements\n",
    "- Determine if aggregation is needed\n",
    "```\n",
    "\n",
    "```\n",
    "2. Complexity Assessment\n",
    "- Evaluate if joins are needed\n",
    "- Check if subqueries or CTEs are required\n",
    "- Determine grouping requirements\n",
    "```\n",
    "\n",
    "```\n",
    "3. Schema Analysis\n",
    "- Identify primary tables needed\n",
    "- Map columns to required data\n",
    "- Understand table relationships\n",
    "```\n",
    "\n",
    "```\n",
    "4. Query Planning\n",
    "- Determine optimal join order if needed\n",
    "- Plan filtering conditions\n",
    "- Consider performance implications\n",
    "```\n",
    "\n",
    "```\n",
    "5. SQL Components\n",
    "- Select clause composition\n",
    "- From clause and join structure\n",
    "- Where clause conditions\n",
    "- Group by and having requirements\n",
    "```\n",
    "\n",
    "```\n",
    "6. Final Validation\n",
    "- Verify schema compatibility\n",
    "- Check column name accuracy\n",
    "- Ensure proper syntax\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "reasoning_instructions = \"\"\"\n",
    "```\n",
    "SQL Generation Guidelines:\n",
    "1. Use COUNT(*) for simple counts\n",
    "2. Avoid unnecessary DISTINCT\n",
    "3. Use table aliases for clarity\n",
    "4. Include proper JOIN conditions\n",
    "5. Handle NULL values appropriately\n",
    "```\n",
    "\n",
    "```\n",
    "Format Requirements:\n",
    "1. Use clear indentation\n",
    "2. Align related clauses\n",
    "3. Use meaningful aliases\n",
    "4. Format complex conditions clearly\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"Thought structure with improved validation\"\"\"\n",
    "    thought: str = Field(description=\"The reasoning step\")\n",
    "    helpful: bool = Field(default=True)\n",
    "\n",
    "class FinalOutput(BaseModel):\n",
    "    \"\"\"Complete output structure containing the query, reasoning, and SQL\"\"\"\n",
    "    user_nlp_query: str = Field(\n",
    "        description=\"The original natural language query to be translated into SQL\"\n",
    "    )\n",
    "    reasonings: List[Thought] = Field(\n",
    "        description=\"Step-by-step reasoning process for query generation\"\n",
    "    )\n",
    "    generated_sql_query: str = Field(\n",
    "        description=\"The final SQL query that answers the natural language question\"\n",
    "    )\n",
    "\n",
    "def create_prompt(question: str, schema_links: List[str], schema: Dict[str, Any], complexity: str) -> str:\n",
    "    \"\"\"Create enhanced prompt with better examples and guidance\"\"\"\n",
    "    \n",
    "    examples = {\n",
    "        \"EASY\": \"\"\"\n",
    "Example:\n",
    "Q: \"How many students are there?\"\n",
    "A: This requires a simple count from the student table\n",
    "SQL: SELECT COUNT(*) FROM student\n",
    "\"\"\",\n",
    "        \"NON-NESTED\": \"\"\"\n",
    "Example:\n",
    "Q: \"List student names and their department names\"\n",
    "A: This requires joining student and department tables\n",
    "SQL: SELECT s.name, d.name \n",
    "FROM student s\n",
    "JOIN department d ON s.dept_id = d.id\n",
    "\"\"\",\n",
    "        \"NESTED\": \"\"\"\n",
    "Example:\n",
    "Q: \"Find students with above average grades\"\n",
    "A: This requires a subquery to calculate the average\n",
    "SQL: SELECT name \n",
    "FROM student \n",
    "WHERE grade > (SELECT AVG(grade) FROM student)\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    return f\"\"\"You are an expert SQL developer tasked with generating precise SQL queries.\n",
    "\n",
    "SCHEMA INFORMATION:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "QUESTION: {question}\n",
    "SCHEMA_LINKS: {schema_links}\n",
    "COMPLEXITY: {complexity}\n",
    "\n",
    "{thought_instructions}\n",
    "\n",
    "{reasoning_instructions}\n",
    "\n",
    "{examples.get(complexity, examples['EASY'])}\n",
    "\n",
    "GUIDELINES:\n",
    "1. Generate clear, efficient SQL\n",
    "2. Use proper table aliases\n",
    "3. Include complete reasoning\n",
    "4. Match schema exactly\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "{final_output_schema_json} \"\"\"\n",
    "\n",
    "def process_question_sql(\n",
    "    question: str,\n",
    "    predicted_class: str,\n",
    "    schema_links: List[str],\n",
    "    db_id: str,\n",
    "    spider_schemas: List[Dict[str, Any]],\n",
    "    max_retries: int = 3\n",
    ") -> FinalOutput:\n",
    "    \"\"\"Generate SQL with thoughts and reasoning\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get schema\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                prompt = create_prompt(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema,\n",
    "                    complexity=predicted_class\n",
    "                )\n",
    "                \n",
    "                client = OpenAI()\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\", \n",
    "                            \"content\": \"\"\"You are an SQL expert. Return JSON with this exact format:\n",
    "                            {\n",
    "                                \"user_nlp_query\": \"the original question\",\n",
    "                                \"reasonings\": [\n",
    "                                    {\"thought\": \"your reasoning step\", \"helpful\": true}\n",
    "                                ],\n",
    "                                \"generated_sql_query\": \"your SQL query\"\n",
    "                            }\"\"\"\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1500,\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                \n",
    "                content = response.choices[0].message.content\n",
    "                print(f\"Raw GPT response: {content}\")  # Debug print\n",
    "                \n",
    "                try:\n",
    "                    result = json.loads(content)\n",
    "                    return FinalOutput(\n",
    "                        user_nlp_query=result.get(\"user_nlp_query\", question),\n",
    "                        reasonings=[\n",
    "                            Thought(**thought) for thought in result.get(\"reasonings\", [])\n",
    "                        ] or [Thought(thought=\"Direct SQL generation\", helpful=True)],\n",
    "                        generated_sql_query=result.get(\"generated_sql_query\", \"SELECT 1\")\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing response: {str(e)}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return FinalOutput(\n",
    "                            user_nlp_query=question,\n",
    "                            reasonings=[\n",
    "                                Thought(\n",
    "                                    thought=f\"Failed to parse response: {str(e)}\",\n",
    "                                    helpful=False\n",
    "                                )\n",
    "                            ],\n",
    "                            generated_sql_query=\"SELECT 1\"\n",
    "                        )\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    return FinalOutput(\n",
    "                        user_nlp_query=question,\n",
    "                        reasonings=[\n",
    "                            Thought(\n",
    "                                thought=f\"Error in process: {str(e)}\",\n",
    "                                helpful=False\n",
    "                            )\n",
    "                        ],\n",
    "                        generated_sql_query=\"SELECT 1\"\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return FinalOutput(\n",
    "            user_nlp_query=question,\n",
    "            reasonings=[\n",
    "                Thought(\n",
    "                    thought=f\"Critical error: {str(e)}\",\n",
    "                    helpful=False\n",
    "                )\n",
    "            ],\n",
    "            generated_sql_query=\"SELECT 1\"\n",
    "        )\n",
    "\n",
    "    return FinalOutput(\n",
    "        user_nlp_query=question,\n",
    "        reasonings=[\n",
    "            Thought(\n",
    "                thought=\"Maximum retries exceeded\",\n",
    "                helpful=False\n",
    "            )\n",
    "        ],\n",
    "        generated_sql_query=\"SELECT 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def easy_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for easy Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "SQL:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for medium (non-nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def hard_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for hard (nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def debugger(question: str, sql: str, predicted_class: str, schema_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"Create debug prompt based on query complexity.\"\"\"\n",
    "    \n",
    "    if '\"EASY\"' in predicted_class:\n",
    "        prompt_used = easy_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "    elif '\"NON-NESTED\"' in predicted_class:\n",
    "        prompt_used = medium_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "    else:\n",
    "        prompt_used = hard_prompt_maker(\n",
    "            question=question,\n",
    "            schema_links=schema_dict.get(\"schema_links\", []),\n",
    "            schema=schema_dict\n",
    "        )\n",
    "\n",
    "    instruction = f\"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to check if the given SQLite SQL QUERY has any issues. If there are any issues, fix them and return the fixed SQLite QUERY in the output. If there are no issues, return the SQLite SQL QUERY as is in the output.\n",
    "#### Background Information:\n",
    "Relevant Schema Links: {schema_dict.get(\"schema_links\", [])}\n",
    "Prompt Used to Generate the Candidate SQLite SQL Query:\n",
    "'''\n",
    "{prompt_used}\n",
    "'''\n",
    "#### Use the following instructions for fixing the SQL QUERY:\n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT only when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\n",
    "\n",
    "#### Question: {question}\n",
    "#### SQLite SQL QUERY\n",
    "{sql}\n",
    "#### SQLite FIXED SQL QUERY\n",
    "\"\"\"\n",
    "\n",
    "    return instruction\n",
    "\n",
    "def GPT4_debug(prompt: str) -> str:\n",
    "    \"\"\"Debug SQL using GPT-4.\"\"\"\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a SQL expert. Return only the fixed SQL query with no explanation.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.0,\n",
    "            max_tokens=350,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"#\", \";\", \"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT4_debug: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def refine_query(question: str, sql: str, predicted_class: str, schema_links: List[str], db_id: str, spider_schemas: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Refine and debug the SQL query.\"\"\"\n",
    "    max_attempts = 3\n",
    "    attempt = 0\n",
    "    debugged_SQL = None\n",
    "    \n",
    "    # Get schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        return sql\n",
    "\n",
    "    # Create schema dict with required format\n",
    "    schema_dict = {\n",
    "        \"table_names_original\": schema.get('table_names_original', []),\n",
    "        \"column_names_original\": schema.get('column_names_original', []),\n",
    "        \"schema_links\": schema_links,\n",
    "        \"primary_keys\": schema.get('primary_keys', []),\n",
    "        \"foreign_keys\": schema.get('foreign_keys', [])\n",
    "    }\n",
    "    \n",
    "    while debugged_SQL is None and attempt < max_attempts:\n",
    "        try:\n",
    "            debug_prompt = debugger(\n",
    "                question=question,\n",
    "                sql=sql,\n",
    "                predicted_class=predicted_class,\n",
    "                schema_dict=schema_dict\n",
    "            )\n",
    "            debugged_SQL = GPT4_debug(debug_prompt)\n",
    "            \n",
    "            if debugged_SQL:\n",
    "                # Clean up the response\n",
    "                debugged_SQL = debugged_SQL.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                try:\n",
    "                    # Try to extract SQL if wrapped in markdown\n",
    "                    if \"```\" in debugged_SQL:\n",
    "                        debugged_SQL = debugged_SQL.split(\"```sql\")[-1].split(\"```\")[0].strip()\n",
    "                except:\n",
    "                    # If extraction fails, use the whole response\n",
    "                    pass\n",
    "                \n",
    "                print(f\"Refined SQL (Attempt {attempt + 1}):\", debugged_SQL)\n",
    "                return debugged_SQL\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in refinement attempt {attempt + 1}: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nimport os\\nimport json\\nimport time\\nfrom typing import Dict, Any, List\\nfrom openai import OpenAI\\n\\nclass SpiderTester:\\n    def __init__(self, spider_path: str = \\'/Users/virounikamina/Desktop/spider_data\\', mode=\\'dev\\'):\\n        \"\"\"Initialize Spider tester with dataset path and mode.\"\"\"\\n        self.spider_path = spider_path\\n        \\n        # Load schemas\\n        with open(os.path.join(spider_path, \\'tables.json\\'), \\'r\\') as f:\\n            self.spider_schemas = json.load(f)\\n            \\n        # Select correct data file\\n        data_files = {\\n            \\'dev\\': \\'dev.json\\',\\n            \\'train_spider\\': \\'train_spider.json\\',\\n            \\'train_others\\': \\'train_others.json\\'\\n        }\\n        \\n        if mode not in data_files:\\n            raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\\n            \\n        data_file = data_files[mode]\\n        print(f\"Loading {data_file}...\")\\n        \\n        # Load test data\\n        with open(os.path.join(spider_path, data_file), \\'r\\') as f:\\n            self.test_data = json.load(f)\\n            \\n        print(f\"Loaded {len(self.test_data)} questions\")\\n            \\n        # Create results file\\n        self.results_file = os.path.join(\\n            spider_path, \\n            f\\'results_{mode}_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt\\'\\n        )\\n        \\n        # Initialize value retrieval if needed\\n        self.value_retrieval = SpiderValueRetrieval(\\n            spider_tables_path=os.path.join(spider_path, \\'tables.json\\')\\n        )\\n\\n    def log(self, message: str, question_num: int = None):\\n        \"\"\"Log message to file and console with optional question number.\"\"\"\\n        if question_num is not None:\\n            message = f\"Question {question_num}: {message}\"\\n            \\n        print(message)\\n        with open(self.results_file, \\'a\\') as f:\\n            f.write(message + \\'\\n\\')\\n\\n    def run_test(self, num_questions: int = None):\\n        if num_questions is None:\\n            num_questions = len(self.test_data)\\n        else:\\n            num_questions = min(num_questions, len(self.test_data))\\n\\n        processed = 0\\n        successful = 0\\n        \\n        self.log(f\"\\nTesting {num_questions} questions\")\\n        \\n        for idx, test_case in enumerate(self.test_data[:num_questions]):\\n            try:\\n                question = test_case[\\'question\\']\\n                db_id = test_case[\\'db_id\\']\\n                ground_truth = test_case[\\'query\\']\\n                \\n                self.log(f\"\\nProcessing Question {idx + 1}/{num_questions}\")\\n                self.log(f\"Question: {question}\")\\n                self.log(f\"Database: {db_id}\")\\n                \\n                # Get schema links\\n                schema_links = self.value_retrieval.process_schema(question, db_id)\\n                self.log(f\"Schema Links: {schema_links}\")\\n                \\n                try:\\n                    # Get classification\\n                    classification = process_question_classification(\\n                        question=question,\\n                        schema_links=schema_links,\\n                        db_id=db_id,\\n                        spider_schemas=self.spider_schemas\\n                    )\\n                    self.log(f\"Classification: {classification}\")\\n                    \\n                    # Get initial SQL with reasoning\\n                    process_thesql = process_question_sql(\\n                        question=question,\\n                        predicted_class=classification,\\n                        schema_links=schema_links,\\n                        db_id=db_id,\\n                        spider_schemas=self.spider_schemas\\n                    )\\n                    \\n                    self.log(\"Reasoning Steps:\")\\n                    for thought in process_thesql.reasonings:\\n                        self.log(f\"- {thought.thought}\")\\n                        \\n                    self.log(f\"Initial SQL: {process_thesql.generated_sql_query}\")\\n                    \\n                    # Refine the SQL with all required parameters\\n                    final_sql = refine_query(\\n                        question=question,\\n                        sql=process_thesql.generated_sql_query,\\n                        predicted_class=classification,  # Added\\n                        schema_links=schema_links,      # Added\\n                        db_id=db_id,\\n                        spider_schemas=self.spider_schemas\\n                    )\\n                    \\n                    self.log(f\"Final SQL: {final_sql}\")\\n                    self.log(f\"Ground Truth: {ground_truth}\")\\n                    \\n                    # Compare results\\n                    #if self.compare_sql(final_sql, ground_truth):\\n                     #   successful += 1\\n                     #   self.log(\" Match\")\\n                    #else:\\n                    #    self.log(\" No match\")\\n                        \\n                    #processed += 1\\n                    \\n                except Exception as e:\\n                    self.log(f\"Error processing question: {str(e)}\")\\n                    continue\\n                    \\n            except Exception as e:\\n                self.log(f\"Error in test case: {str(e)}\")\\n                continue\\n\\n        # Print summary\\n        self.log(\"\\n=== Testing Summary ===\")\\n        self.log(f\"Total Questions Processed: {processed}\")\\n        self.log(f\"Successful Matches: {successful}\")\\n        if processed > 0:\\n            self.log(f\"Success Rate: {(successful/processed)*100:.2f}%\")\\n\\ndef main():\\n    \"\"\"Main entry point for Spider testing.\"\"\"\\n    print(\"Available modes:\")\\n    print(\"1. dev (1034 questions)\")\\n    print(\"2. train_spider (7000 questions)\")\\n    print(\"3. train_others (1659 questions)\")\\n    \\n    mode = input(\"Choose mode (dev/train_spider/train_others) [default: dev]: \").strip() or \\'dev\\'\\n    num_questions = input(\"How many questions to process? (press Enter for all): \").strip()\\n    \\n    try:\\n        tester = SpiderTester(mode=mode)\\n        tester.run_test(num_questions=int(num_questions) if num_questions else None)\\n    except Exception as e:\\n        print(f\"Critical error: {str(e)}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "from openai import OpenAI\n",
    "\n",
    "class SpiderTester:\n",
    "    def __init__(self, spider_path: str = '/Users/virounikamina/Desktop/spider_data', mode='dev'):\n",
    "        \"\"\"Initialize Spider tester with dataset path and mode.\"\"\"\n",
    "        self.spider_path = spider_path\n",
    "        \n",
    "        # Load schemas\n",
    "        with open(os.path.join(spider_path, 'tables.json'), 'r') as f:\n",
    "            self.spider_schemas = json.load(f)\n",
    "            \n",
    "        # Select correct data file\n",
    "        data_files = {\n",
    "            'dev': 'dev.json',\n",
    "            'train_spider': 'train_spider.json',\n",
    "            'train_others': 'train_others.json'\n",
    "        }\n",
    "        \n",
    "        if mode not in data_files:\n",
    "            raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "            \n",
    "        data_file = data_files[mode]\n",
    "        print(f\"Loading {data_file}...\")\n",
    "        \n",
    "        # Load test data\n",
    "        with open(os.path.join(spider_path, data_file), 'r') as f:\n",
    "            self.test_data = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.test_data)} questions\")\n",
    "            \n",
    "        # Create results file\n",
    "        self.results_file = os.path.join(\n",
    "            spider_path, \n",
    "            f'results_{mode}_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        )\n",
    "        \n",
    "        # Initialize value retrieval if needed\n",
    "        self.value_retrieval = SpiderValueRetrieval(\n",
    "            spider_tables_path=os.path.join(spider_path, 'tables.json')\n",
    "        )\n",
    "\n",
    "    def log(self, message: str, question_num: int = None):\n",
    "        \"\"\"Log message to file and console with optional question number.\"\"\"\n",
    "        if question_num is not None:\n",
    "            message = f\"Question {question_num}: {message}\"\n",
    "            \n",
    "        print(message)\n",
    "        with open(self.results_file, 'a') as f:\n",
    "            f.write(message + '\\n')\n",
    "\n",
    "    def run_test(self, num_questions: int = None):\n",
    "        if num_questions is None:\n",
    "            num_questions = len(self.test_data)\n",
    "        else:\n",
    "            num_questions = min(num_questions, len(self.test_data))\n",
    "\n",
    "        processed = 0\n",
    "        successful = 0\n",
    "        \n",
    "        self.log(f\"\\nTesting {num_questions} questions\")\n",
    "        \n",
    "        for idx, test_case in enumerate(self.test_data[:num_questions]):\n",
    "            try:\n",
    "                question = test_case['question']\n",
    "                db_id = test_case['db_id']\n",
    "                ground_truth = test_case['query']\n",
    "                \n",
    "                self.log(f\"\\nProcessing Question {idx + 1}/{num_questions}\")\n",
    "                self.log(f\"Question: {question}\")\n",
    "                self.log(f\"Database: {db_id}\")\n",
    "                \n",
    "                # Get schema links\n",
    "                schema_links = self.value_retrieval.process_schema(question, db_id)\n",
    "                self.log(f\"Schema Links: {schema_links}\")\n",
    "                \n",
    "                try:\n",
    "                    # Get classification\n",
    "                    classification = process_question_classification(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    self.log(f\"Classification: {classification}\")\n",
    "                    \n",
    "                    # Get initial SQL with reasoning\n",
    "                    process_thesql = process_question_sql(\n",
    "                        question=question,\n",
    "                        predicted_class=classification,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    \n",
    "                    self.log(\"Reasoning Steps:\")\n",
    "                    for thought in process_thesql.reasonings:\n",
    "                        self.log(f\"- {thought.thought}\")\n",
    "                        \n",
    "                    self.log(f\"Initial SQL: {process_thesql.generated_sql_query}\")\n",
    "                    \n",
    "                    # Refine the SQL with all required parameters\n",
    "                    final_sql = refine_query(\n",
    "                        question=question,\n",
    "                        sql=process_thesql.generated_sql_query,\n",
    "                        predicted_class=classification,  # Added\n",
    "                        schema_links=schema_links,      # Added\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=self.spider_schemas\n",
    "                    )\n",
    "                    \n",
    "                    self.log(f\"Final SQL: {final_sql}\")\n",
    "                    self.log(f\"Ground Truth: {ground_truth}\")\n",
    "                    \n",
    "                    # Compare results\n",
    "                    #if self.compare_sql(final_sql, ground_truth):\n",
    "                     #   successful += 1\n",
    "                     #   self.log(\" Match\")\n",
    "                    #else:\n",
    "                    #    self.log(\" No match\")\n",
    "                        \n",
    "                    #processed += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.log(f\"Error processing question: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error in test case: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Print summary\n",
    "        self.log(\"\\n=== Testing Summary ===\")\n",
    "        self.log(f\"Total Questions Processed: {processed}\")\n",
    "        self.log(f\"Successful Matches: {successful}\")\n",
    "        if processed > 0:\n",
    "            self.log(f\"Success Rate: {(successful/processed)*100:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for Spider testing.\"\"\"\n",
    "    print(\"Available modes:\")\n",
    "    print(\"1. dev (1034 questions)\")\n",
    "    print(\"2. train_spider (7000 questions)\")\n",
    "    print(\"3. train_others (1659 questions)\")\n",
    "    \n",
    "    mode = input(\"Choose mode (dev/train_spider/train_others) [default: dev]: \").strip() or 'dev'\n",
    "    num_questions = input(\"How many questions to process? (press Enter for all): \").strip()\n",
    "    \n",
    "    try:\n",
    "        tester = SpiderTester(mode=mode)\n",
    "        tester.run_test(num_questions=int(num_questions) if num_questions else None)\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ COLUMN MAPPING\n",
    "\n",
    "from typing import Union, Tuple, List, Optional\n",
    "\n",
    "class CMBackground(BaseModel):\n",
    "    \"\"\"A setup to the background for the user.\"\"\"\n",
    "\n",
    "    background: str = Field(description=\"Background for the user's question\", min_length=10)\n",
    "\n",
    "\n",
    "class CMThought(BaseModel):\n",
    "    \"\"\"A thought about the user's question.\"\"\"\n",
    "\n",
    "    thought: str  = Field(description=\"Text of the thought.\")\n",
    "#     helpful: bool = Field(description=\"Whether the thought is helpful to solving the user's question.\")\n",
    "\n",
    "\n",
    "class CMObservation(BaseModel):\n",
    "    \"\"\"An observation on the sequence of thoughts and observations generated so far.\"\"\"\n",
    "\n",
    "    observation: str = Field(description=\"An insightful observation on the sequence of thoughts and observations generated so far.\")\n",
    "    \n",
    "\n",
    "class CMReasonings(BaseModel):\n",
    "    \"\"\"Returns a detailed reasoning to the user's question.\"\"\"\n",
    "\n",
    "    reasonings: list[Union[CMBackground, CMThought, CMObservation]] = Field(\n",
    "        description=\"Reasonings to solve the users questions.\"\n",
    "        #, min_length=5\n",
    "    )\n",
    "\n",
    "reasonings_schema_json = CMReasonings.model_json_schema()\n",
    "\n",
    "class FinalQueryOutput(BaseModel):\n",
    "    \n",
    "    input_sql_query_1: str = Field(\n",
    "        description=f\"\"\"Returns the exact same first query that the user gave as input.\"\"\")\n",
    "        \n",
    "    input_sql_query_2: str = Field(\n",
    "        description=f\"\"\"Returns the exact same second query that the user gave as input.\"\"\")\n",
    "\n",
    "    reasonings: list[Union[CMBackground, CMThought, CMObservation]] = Field(\n",
    "        description=\"Reasonings to solve the users questions.\"\n",
    "        #, min_length=5\n",
    "    )\n",
    "        \n",
    "    column_mapping_list: List[Tuple[str, str]] = Field(\n",
    "        description=f\"\"\"Returns the list of the corresponding column names in first sql query, sql 1, which\n",
    "        corresponds to the column name in the other sql query, sql 2, as a list of tuple entries\"\"\")\n",
    "    \n",
    "column_mapping_schema_json = FinalQueryOutput.model_json_schema()\n",
    "\n",
    "complete_user_prompts = \"\"\"\n",
    "```\n",
    "Task Overview\n",
    "```\n",
    "Given two sql queries which are supposed to be equivalent, as inputs, \n",
    "the task is to give a column mapping between the output columns in one sql query\n",
    "to the other sql query.\n",
    "\n",
    "The mapping should include any table aliases present in the column names.\n",
    "For example, if one query uses 'COLUMN_NAME' and another uses 'alias.column_name',\n",
    "the mapping should be ['COLUMN_NAME', 'alias.column_name'].\n",
    "```\n",
    "\n",
    "```\n",
    "The mapping is to be generated as a list of tuples.\n",
    "```\n",
    "\n",
    "```\n",
    "For each element of the list which would be a tuple, \n",
    "the first entry in the tuple would be the column name used in sql query 1,\n",
    "and the second entry in the tuple would be the corresponding column name in the sql query 2.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "reasoning_instructions = \"\"\"\n",
    "```\n",
    "1. Reasoning you provide should first focus on whether the input sql queries contain \n",
    "a nested query or not.\n",
    "2. It should give a plan on how to solve this question.\n",
    "3. It should explain each of the clauses and why they are structured the way they are structured. \n",
    "For example, if there is a `group_by`, an explanation should be given as to why it exists.\n",
    "```\n",
    "\n",
    "```\n",
    "Format the generated sql with proper indentation - the columns in the\n",
    "(`select` statement should have more indentation than keyword `select`\n",
    "and so on for each SQL clause.)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "thought_instructions = f\"\"\"\n",
    "```\n",
    "Thought Instructions:\n",
    "```\n",
    "\n",
    "```\n",
    "Generate thoughts of increasing complexity.\n",
    "Each thought should build on the previous ones and thoughts \n",
    "should progressively cover the nuances of the problem at hand.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate two separate thoughts, one each for the two input sql queries, \n",
    "to figure out the list of output columns in each of the sql queries.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to figure out the list of columns in sql query 1\n",
    "which are present in both the sql queries.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to figure out the list of columns in sql query 1 \n",
    "which are in sql query 1 but \n",
    "which are not present in sql query 2.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to figure out the list of columns in sql query 1\n",
    "which are in sql query 2 but \n",
    "which are not present in sql query 1.\n",
    "```\n",
    "\n",
    "```\n",
    "If the query uses common table expressions or nested queries, \n",
    "the above thoughts should be generated for each of the CTE separately.\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Closing Thoughts and Observations\n",
    "```\n",
    "These should summarize:\n",
    "1. The structure of the SQL query:\n",
    "    - This states whether the query has any nested query.\n",
    "    If so, the structure of the nested query is also mentioned.\n",
    "    If not, a summary of the function of each of the select`, `where`, `group_by` etc. clauses\n",
    "    should be mentioned.\n",
    "2. An explanation of why the mapping is correct.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_schema_instructions = f\"\"\"\n",
    "```\n",
    "Use the following JSON Schema as the grammar to create the structure \n",
    "for the step by step reasoning, and then to \n",
    "create the final SQL query.\n",
    "```\n",
    "\n",
    "```\n",
    "Schema for Reasoning:\n",
    "```\n",
    "{reasonings_schema_json}\n",
    "```\n",
    "\n",
    "```\n",
    "The instructions on how to structure the reasoning is provided below:\n",
    "```\n",
    "{thought_instructions}\n",
    "```\n",
    "\n",
    "```\n",
    "Schema for Overall Output:\n",
    "(This includes the reasonings schema above as an element)\n",
    "```\n",
    "{column_mapping_schema_json}\n",
    "```\n",
    "\n",
    "```\n",
    "The final response should be a json with `names` as \n",
    "    `input_sql_query_1`,\n",
    "    `input_sql_query_2`,\n",
    "    `reasonings`,\n",
    "    `column_mapping_list`.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_user_prompt_for_question(input_sql_query_1, input_sql_query_2, input_table_schema, complete_user_prompts):\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "```\n",
    "Here are the two sql statements that are to be compared:\n",
    "```\n",
    "\n",
    "```\n",
    "SQL Query 1:\n",
    "```\n",
    "{input_sql_query_1}\n",
    "```\n",
    "\n",
    "```\n",
    "SQL Query 2:\n",
    "```\n",
    "{input_sql_query_2}\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a column mapping corresponding to the given input sql queries\n",
    "and the description of the table provided below.\n",
    "```\n",
    "{input_table_schema}\n",
    "```\n",
    "\n",
    "```\n",
    "Here's a more detailed set of instructions:\n",
    "```\n",
    "{complete_user_prompts}\n",
    "```\n",
    "\n",
    "```\n",
    "Reasoning as to why the query is correct:\n",
    "```\n",
    "{reasoning_instructions}\n",
    "\n",
    "\n",
    "{reasoning_schema_instructions}\n",
    "\n",
    "```\n",
    "Response for Column Mapping Generation:\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt\n",
    "\n",
    "\n",
    "def call_openai_model(system_prompt, user_prompt, model_name):\n",
    "\n",
    "    chat_history = [\n",
    "        {\n",
    "            'role': 'system', \n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': user_prompt\n",
    "        }, \n",
    "\n",
    "    ]\n",
    "    \n",
    "    final_response = {}\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model           = model_name, \n",
    "            messages        = chat_history, \n",
    "            response_format = {\"type\":\"json_object\"}\n",
    "        )\n",
    "        \n",
    "        final_response = response.choices[0].message.content\n",
    "    \n",
    "    except Exception:\n",
    "\n",
    "        response = {\n",
    "            \"content\": \"An error occured. Please retry your chat. \\\n",
    "                If you keep getting this error, you may be out of OpenAI \\\n",
    "                completion tokens. Contact #help-ai on slack for assistance.\"\n",
    "        }\n",
    "        return response\n",
    "\n",
    "    return final_response\n",
    "\n",
    "\n",
    "system_prompt_snippet_001 = \"\"\"\n",
    "```\n",
    "You are the most intelligent person in the world.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_snippet_002 = \"\"\"\n",
    "\n",
    "```\n",
    "You will receive a $500 tip if you follow ALL the instructions specified.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_snippet_003 = \"\"\"\n",
    "\n",
    "```\n",
    "Instructions\n",
    "```\n",
    "Give a column mapping between two equivalent sql statements\n",
    "which may differ in the names of columns used in the output\n",
    "and may also differ in the structure, but the overall meaning\n",
    "and function of the query is meant to be the same.\n",
    "```\n",
    "\n",
    "```\n",
    "Use step by step reasoning and at each step generate thoughts of increasing complexity.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_snippet_004 = \"\"\"\n",
    "\n",
    "```\n",
    "Getting this answer right is important for my career. Please do your best.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "{system_prompt_snippet_001}\n",
    "{system_prompt_snippet_002}\n",
    "{system_prompt_snippet_003}\n",
    "{system_prompt_snippet_004}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/virounikamina/Desktop/PIMCO-Text2SQL/test\n"
     ]
    }
   ],
   "source": [
    "curr = os.getcwd()\n",
    "print(curr)\n",
    "output_file = os.path.join(curr, 'spider_all_outputs')\n",
    "def append_to_file(output, qnum, filename=output_file):\n",
    "    # Check if file exists\n",
    "    output_filename= filename+str(qnum)+'.txt'\n",
    "    if not os.path.exists(output_filename):\n",
    "        with open(output_filename, 'w') as file:\n",
    "            file.write(\"Test_Spider Output Log\\n\")\n",
    "            file.write(\"=\" * 80 + \"\\n\")\n",
    "    # Append the output\n",
    "    with open(output_filename, 'a') as file:\n",
    "        file.write(output + \"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import io\n",
    "import csv\n",
    "def execute_sql(query: str) -> str:\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect('sqlite/nport.db')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Execute the query with a timeout\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch column names and rows\n",
    "        columns = [description[0] for description in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Convert the results to CSV\n",
    "        output = io.StringIO()\n",
    "        writer = csv.writer(output)\n",
    "        writer.writerow(columns)\n",
    "        writer.writerows(rows)\n",
    "        csv_data = output.getvalue()\n",
    "        output.close()\n",
    "\n",
    "        return csv_data\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {str(e)}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_csv_strings(csv_data1: str, csv_data2: str) -> bool:\n",
    "    # Use io.StringIO to read the CSV strings as file-like objects\n",
    "    csv_file1 = io.StringIO(csv_data1)\n",
    "    csv_file2 = io.StringIO(csv_data2)\n",
    "    \n",
    "    # Create CSV readers for each CSV string\n",
    "    reader1 = csv.reader(csv_file1)\n",
    "    reader2 = csv.reader(csv_file2)\n",
    "    \n",
    "    # Compare rows one by one\n",
    "    for row1, row2 in zip(reader1, reader2):\n",
    "        if row1 != row2:\n",
    "            return False  # Rows are different\n",
    "    \n",
    "    # Check if there are extra rows in either file\n",
    "    try:\n",
    "        next(reader1)\n",
    "        return False  # Extra rows in csv_data1\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        next(reader2)\n",
    "        return False  # Extra rows in csv_data2\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    return True  # CSVs are identical\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_aggregate_columns(sql_query):\n",
    "    \"\"\"\n",
    "    Extract resulting output column names of aggregate functions in the SQL query,\n",
    "    handling duplicates and default naming conventions.\n",
    "    \"\"\"\n",
    "    aggregate_functions = [\"SUM\", \"AVG\", \"COUNT\", \"MAX\", \"MIN\"]\n",
    "    output_columns = []\n",
    "\n",
    "    # Regex to match aggregate functions with optional aliasing\n",
    "    pattern = rf\"({'|'.join(aggregate_functions)})\\((.*?)\\)(?:\\s+AS\\s+([\\w_]+))?\"\n",
    "    \n",
    "    matches = re.findall(pattern, sql_query, re.IGNORECASE)\n",
    "    function_counter = {}  # Track occurrences of each aggregate function\n",
    "    \n",
    "    for func, inner, alias in matches:\n",
    "        func_lower = func.lower()\n",
    "        if alias:  # Explicit alias defined\n",
    "            output_columns.append(alias)\n",
    "        else:  # No alias, use default naming conventions\n",
    "            if func_lower not in function_counter:\n",
    "                function_counter[func_lower] = 0\n",
    "            else:\n",
    "                function_counter[func_lower] += 1\n",
    "            # Generate default name (e.g., sum, sum_1, sum_2, etc.)\n",
    "            if function_counter[func_lower] == 0:\n",
    "                output_columns.append(f\"{func_lower}({inner.strip()})\")  # Default naming for SQLite\n",
    "            else:\n",
    "                output_columns.append(f\"{func_lower}({inner.strip()})_{function_counter[func_lower]}\")  # Add suffix\n",
    "\n",
    "    return output_columns\n",
    "\n",
    "def evaluate_sql_accuracy(generated_sql, ground_truth_sql, generated_csv, ground_truth_csv, qnum):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of generated SQL by comparing the resulting CSV files.\n",
    "    \"\"\"\n",
    "    # Load CSV files\n",
    "    gen_df = pd.read_csv(io.StringIO(generated_csv))\n",
    "    gt_df = pd.read_csv(io.StringIO(ground_truth_csv))\n",
    "    \n",
    "    # Ensure all ground truth columns are in the generated DataFrame\n",
    "    for col in gt_df.columns:\n",
    "        if col not in gen_df.columns:\n",
    "            append_to_file(\"False, not all ground truth columns are in generated csv\",qnum)\n",
    "            return False\n",
    "\n",
    "    # Identify resulting output columns of aggregate functions in both SQL queries\n",
    "    gt_agg_columns = get_aggregate_columns(ground_truth_sql)\n",
    "\n",
    "    # Remove aggregate function columns from both DataFrames\n",
    "    gen_df = gen_df.drop(columns=gt_agg_columns, errors='ignore')\n",
    "    gt_df = gt_df.drop(columns=gt_agg_columns, errors='ignore')\n",
    "\n",
    "    # Align columns in the generated DataFrame to match ground truth\n",
    "    gen_subset = gen_df[gt_df.columns]\n",
    "\n",
    "    # Check if rows match exactly\n",
    "    if not gen_subset.equals(gt_df):\n",
    "        append_to_file(\"False, all ground truth columns exist, but rows mismatch\",qnum)\n",
    "        return False  # Row mismatch detected\n",
    "\n",
    "    append_to_file(\"True, all ground truth columns exist, and rows match\", qnum)\n",
    "    return True  # All checks passed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_csv_din(ground_truth_query: str, llm_query: str, qnum: int, db_id: str, spider_schemas: list, value_retrieval):\n",
    "    ## let LLM stack query the database\n",
    "    append_to_file(f\"Ground Truth Query: {ground_truth_query}\", qnum)\n",
    "\n",
    "    try: \n",
    "        # Get schema links using Spider-specific value retrieval\n",
    "        schema_links = value_retrieval.process_schema(llm_query, db_id)\n",
    "        append_to_file(f\"Schema Links for Question: {llm_query}\\n{schema_links}\", qnum)\n",
    "    \n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_schema of Value Retrieval: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    try:\n",
    "        # Get classification with Spider-specific parameters\n",
    "        classification = process_question_classification(\n",
    "            question=llm_query,\n",
    "            schema_links=schema_links,\n",
    "            db_id=db_id,\n",
    "            spider_schemas=spider_schemas\n",
    "        )\n",
    "        append_to_file(f\"classification: {classification}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_question_classification of Classification: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string, qnum)\n",
    "        raise e\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # Generate SQL with Spider-specific parameters\n",
    "        process_thesql = process_question_sql(\n",
    "            question=llm_query,\n",
    "            predicted_class=classification,\n",
    "            schema_links=schema_links,\n",
    "            db_id=db_id,\n",
    "            spider_schemas=spider_schemas\n",
    "        )\n",
    "        append_to_file(f\"Thoughts: {process_thesql.reasonings}\", qnum)\n",
    "        append_to_file(f\"SQL: {process_thesql.generated_sql_query}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_question_sql of SQL Generation: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string, qnum)\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        # Refine query with Spider-specific parameters\n",
    "        final_output = refine_query(\n",
    "            question=llm_query,\n",
    "            sql=process_thesql.generated_sql_query,\n",
    "            predicted_class=classification,\n",
    "            schema_links=schema_links,\n",
    "            db_id=db_id,\n",
    "            spider_schemas=spider_schemas\n",
    "        ).replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "        append_to_file(f\"final_output: {final_output}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in refine_query of Self-Correction: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string, qnum)\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "    # Add column mapping here\n",
    "    try:\n",
    "        column_mappings_prompt = get_user_prompt_for_question(\n",
    "            ground_truth_query,\n",
    "            final_output,\n",
    "            schema_links,  # Changed from schema_dict to schema_links\n",
    "            complete_user_prompts\n",
    "        )\n",
    "\n",
    "        column_mappings_response = call_openai_model(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=column_mappings_prompt,\n",
    "            model_name='gpt-4o'\n",
    "        )\n",
    "        \n",
    "        response_parsed = json.loads(column_mappings_response)\n",
    "        append_to_file(f\"Column Mappings: {json.dumps(response_parsed['column_mapping_list'], indent=2)}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = f\"Error Mapping Columns: {str(e)}\"\n",
    "        print(err_string)\n",
    "        append_to_file(err_string, qnum)\n",
    "        raise e\n",
    "    '''\n",
    "    try:\n",
    "        llm_csv = execute_sql(final_output)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error Executing LLM-Generated SQL: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "    ## direct query to the database\n",
    "        ground_truth_csv = execute_sql(ground_truth_query)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error Executing Ground Truth SQL: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    ## compare results\n",
    "    \n",
    "    try:\n",
    "        #diff=compare_csv_strings(ground_truth_csv,llm_csv)\n",
    "        diff = evaluate_sql_accuracy(generated_sql=final_output,ground_truth_sql=ground_truth_query,generated_csv=llm_csv,ground_truth_csv=ground_truth_csv, qnum=qnum)\n",
    "        if diff:\n",
    "            print(\"CSV outputs match perfectly.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Mismatch found.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        err_string=(f\"Error comparing CSVs: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Loading Spider schemas from: /Users/virounikamina/Desktop/spider_data/tables.json\n",
      "Loading dev.json...\n",
      "Loaded 1034 questions\n",
      "\n",
      "Testing 1034 questions\n",
      "========================================================================================================================\n",
      "Processing Question 1/1034\n",
      "Database ID: concert_singer\n",
      "========================================================================================================================\n",
      "Extracted Info: {'keywords': ['singers', 'count'], 'keyphrases': ['How many singers'], 'numerical_values': []}\n",
      "Words Extracted: ['singers', 'count', 'How many singers']\n",
      "Processed Words: ['many', 'singer', 'count']\n",
      "Similar matches for 'many': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', 0.8)]\n",
      "Similar matches for 'count': [('singer.country', 0.83), ('concert.concert_id', 0.67), ('concert.concert_name', 0.67), ('singer_in_concert.concert_id', 0.67)]\n",
      "Table Columns: ['singer.singer_id', 'singer.country']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id', 'singer.country'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id', 'singer.country']}\n",
      "Raw response: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table by counting the number of unique 'Singer_ID'. There is no need to join with other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: The question is asking for the total number of singers. This information can be directly retrieved from the 'singer' table by counting the number of unique 'Singer_ID'. There is no need to join with other tables or use nested queries. \n",
      "\n",
      "Therefore, the question is classified as EASY.\n",
      "Raw GPT response: {\n",
      "    \"user_nlp_query\": \"How many singers do we have?\",\n",
      "    \"reasonings\": [\n",
      "        {\n",
      "            \"thought\": \"The question asks for the total number of singers. This requires counting the number of unique entries in the singer table.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"The singer table contains a primary key 'Singer_ID' which uniquely identifies each singer. Counting the distinct 'Singer_ID' will give us the number of singers.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"There is no need for any joins or nested queries since all the required information is contained within the singer table.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"The SQL query should use the COUNT function on the distinct 'Singer_ID' to ensure we are counting unique singers.\",\n",
      "            \"helpful\": true\n",
      "        }\n",
      "    ],\n",
      "    \"generated_sql_query\": \"SELECT COUNT(DISTINCT Singer_ID) FROM singer\"\n",
      "}\n",
      "Refined SQL (Attempt 1): SELECT COUNT(DISTINCT singer_id) FROM singer\n",
      "========================================================================================================================\n",
      "Processing Question 2/1034\n",
      "Database ID: concert_singer\n",
      "========================================================================================================================\n",
      "Extracted Info: {'keywords': ['total number', 'singers'], 'keyphrases': ['total number of singers']}\n",
      "Words Extracted: ['total number', 'singers', 'total number of singers']\n",
      "Processed Words: ['number', 'singer', 'total']\n",
      "Similar matches for 'number': []\n",
      "Similar matches for 'singer': [('singer.singer_id', 1.0), ('singer_in_concert.singer_id', 1.0), ('singer', 0.8)]\n",
      "Similar matches for 'total': []\n",
      "Table Columns: ['singer.singer_id']\n",
      "Schema Dict: {'table_columns': ['singer.singer_id'], 'primary_keys': ['singer.singer_id'], 'foreign_keys': ['singer_in_concert.singer_id = singer.singer_id'], 'schema_links': ['singer.singer_id']}\n",
      "Raw response: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This operation does not require joining any tables or using nested queries. Therefore, the question is classified as EASY.\n",
      "Trying to extract classification from: To answer the question \"What is the total number of singers?\", we need to count the number of unique singer IDs in the 'singer' table. This operation does not require joining any tables or using nested queries. Therefore, the question is classified as EASY.\n",
      "Raw GPT response: {\n",
      "    \"user_nlp_query\": \"What is the total number of singers?\",\n",
      "    \"reasonings\": [\n",
      "        {\n",
      "            \"thought\": \"The question asks for the total number of singers, which implies counting the unique entries in the singer table.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"The singer table contains a primary key 'Singer_ID' which uniquely identifies each singer. Counting the number of unique 'Singer_ID' entries will give the total number of singers.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"There is no need for a nested query or any complex operations like joins or group by, as we are only interested in the count of unique singers.\",\n",
      "            \"helpful\": true\n",
      "        },\n",
      "        {\n",
      "            \"thought\": \"The SQL query should simply select the count of 'Singer_ID' from the singer table.\",\n",
      "            \"helpful\": true\n",
      "        }\n",
      "    ],\n",
      "    \"generated_sql_query\": \"SELECT COUNT(DISTINCT Singer_ID) FROM singer\"\n",
      "}\n",
      "Refined SQL (Attempt 1): SELECT COUNT(*) FROM singer\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 208\u001b[0m\n\u001b[1;32m    203\u001b[0m             append_to_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Call process_queries with Spider path\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mprocess_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspider_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPIDER_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or 'train_spider' or 'train_others'\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 185\u001b[0m, in \u001b[0;36mprocess_queries\u001b[0;34m(spider_path, mode)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m120\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_csv_din\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mground_truth_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqnum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdb_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspider_schemas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspider_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_retrieval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_retrieval\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     processed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "Cell \u001b[0;32mIn[13], line 177\u001b[0m, in \u001b[0;36mcompare_csv_din\u001b[0;34m(ground_truth_query, llm_query, qnum, db_id, spider_schemas, value_retrieval)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     column_mappings_prompt \u001b[38;5;241m=\u001b[39m get_user_prompt_for_question(\n\u001b[1;32m    171\u001b[0m         ground_truth_query,\n\u001b[1;32m    172\u001b[0m         final_output,\n\u001b[1;32m    173\u001b[0m         schema_links,  \u001b[38;5;66;03m# Changed from schema_dict to schema_links\u001b[39;00m\n\u001b[1;32m    174\u001b[0m         complete_user_prompts\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 177\u001b[0m     column_mappings_response \u001b[38;5;241m=\u001b[39m \u001b[43mcall_openai_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_mappings_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     response_parsed \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(column_mappings_response)\n\u001b[1;32m    184\u001b[0m     append_to_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn Mappings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(response_parsed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_mapping_list\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mindent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, qnum)\n",
      "Cell \u001b[0;32mIn[10], line 246\u001b[0m, in \u001b[0;36mcall_openai_model\u001b[0;34m(system_prompt, user_prompt, model_name)\u001b[0m\n\u001b[1;32m    242\u001b[0m final_response \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 246\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     final_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/openai/_base_client.py:990\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    996\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/PIMCO-Text2SQL/env/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Load data from query_summary.csv\n",
    "def load_queries(input_file):\n",
    "    llm_query = []\n",
    "    ground_truth_query = []\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            llm_query.append(row[\"Question\"])\n",
    "            ground_truth_query.append(row[\"SQL\"])\n",
    "    return llm_query, ground_truth_query\n",
    "\n",
    "# Save arrays to file\n",
    "def save_queries_to_file(file_path, llm_query, ground_truth_query):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump({\"llm_query\": llm_query, \"ground_truth_query\": ground_truth_query}, file)\n",
    "\n",
    "# Load arrays from file\n",
    "def load_queries_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        return data[\"llm_query\"], data[\"ground_truth_query\"]\n",
    "def process_queries(spider_path='/Users/virounikamina/Desktop/spider_data', mode='dev'):\n",
    "    \"\"\"Process queries from Spider dev.json file.\"\"\"\n",
    "    # Load Spider schemas\n",
    "    tables_path = os.path.join(spider_path, 'tables.json')\n",
    "    with open(tables_path, 'r') as f:\n",
    "        spider_schemas = json.load(f)\n",
    "    \n",
    "    # Initialize value retrieval once with proper path\n",
    "    value_retrieval = SpiderValueRetrieval(spider_tables_path=tables_path)\n",
    "    \n",
    "    # Load input file based on mode\n",
    "    data_files = {\n",
    "        'dev': 'dev.json',\n",
    "        'train_spider': 'train_spider.json',\n",
    "        'train_others': 'train_others.json'\n",
    "    }\n",
    "    \n",
    "    if mode not in data_files:\n",
    "        raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "        \n",
    "    input_file = data_files[mode]\n",
    "    print(f\"Loading {input_file}...\")\n",
    "    \n",
    "    # Load test data\n",
    "    with open(os.path.join(spider_path, input_file), 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded {len(test_data)} questions\")\n",
    "    \n",
    "    # Prepare output files in current directory\n",
    "    output_file = f\"spider_results_{mode}.csv\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            \"Question_Number\",\n",
    "            \"Database_ID\",\n",
    "            \"Question\",\n",
    "            \"Ground_Truth_Query\",\n",
    "            \"Match_Result\"\n",
    "        ])\n",
    "\n",
    "    processed = 0\n",
    "    successful = 0\n",
    "    \n",
    "    print(f\"\\nTesting {len(test_data)} questions\")\n",
    "    \n",
    "    # Process each question in test data\n",
    "    for i, test_case in enumerate(test_data):\n",
    "        question = test_case['question']\n",
    "        db_id = test_case['db_id']\n",
    "        ground_truth = test_case['query']\n",
    "\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"Processing Question {i + 1}/{len(test_data)}\")\n",
    "        print(f\"Database ID: {db_id}\")\n",
    "        print(\"=\" * 120)\n",
    "\n",
    "        try:\n",
    "            result = compare_csv_din(\n",
    "                ground_truth_query=ground_truth,\n",
    "                llm_query=question,\n",
    "                qnum=i,\n",
    "                db_id=db_id,\n",
    "                spider_schemas=spider_schemas,\n",
    "                value_retrieval=value_retrieval  # Pass the initialized value_retrieval instance\n",
    "            )\n",
    "            processed += 1\n",
    "            if result:\n",
    "                successful += 1\n",
    "            \n",
    "            write_to_output(output_file, i, db_id, question, ground_truth, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Error processing question {i}: {error_msg}\")\n",
    "            write_to_output(output_file, i, db_id, question, ground_truth, \"Error\")\n",
    "            append_to_file(f\"Error: {error_msg}\", i)\n",
    "    \n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define Spider path\n",
    "SPIDER_PATH = '/Users/virounikamina/Desktop/spider_data'\n",
    "\n",
    "def write_to_output(file_path, qnum, db_id, question, ground_truth, result):\n",
    "    \"\"\"Write a single result to the output CSV file.\"\"\"\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            qnum,\n",
    "            db_id,\n",
    "            question,\n",
    "            ground_truth,\n",
    "            result\n",
    "        ])\n",
    "\n",
    "def process_queries(spider_path='/Users/virounikamina/Desktop/spider_data', mode='dev'):\n",
    "    \"\"\"Process queries from Spider dev.json file.\"\"\"\n",
    "    # Load Spider schemas\n",
    "    tables_path = os.path.join(spider_path, 'tables.json')\n",
    "    with open(tables_path, 'r') as f:\n",
    "        spider_schemas = json.load(f)\n",
    "    \n",
    "    # Initialize value retrieval once with proper path\n",
    "    value_retrieval = SpiderValueRetrieval(spider_tables_path=tables_path)\n",
    "    \n",
    "    # Load input file based on mode\n",
    "    data_files = {\n",
    "        'dev': 'dev.json',\n",
    "        'train_spider': 'train_spider.json',\n",
    "        'train_others': 'train_others.json'\n",
    "    }\n",
    "    \n",
    "    if mode not in data_files:\n",
    "        raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "        \n",
    "    input_file = data_files[mode]\n",
    "    print(f\"Loading {input_file}...\")\n",
    "    \n",
    "    # Load test data\n",
    "    with open(os.path.join(spider_path, input_file), 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "        \n",
    "    print(f\"Loaded {len(test_data)} questions\")\n",
    "    \n",
    "    # Create timestamp for this run\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Prepare output files in current directory\n",
    "    output_file = f\"spider_results_{mode}_{timestamp}.csv\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            \"Question_Number\",\n",
    "            \"Database_ID\",\n",
    "            \"Question\",\n",
    "            \"Ground_Truth_Query\",\n",
    "            \"Match_Result\"\n",
    "        ])\n",
    "\n",
    "    processed = 0\n",
    "    successful = 0\n",
    "    \n",
    "    print(f\"\\nTesting {len(test_data)} questions\")\n",
    "    \n",
    "    # Process each question in test data\n",
    "    for i, test_case in enumerate(test_data):\n",
    "        question = test_case['question']\n",
    "        db_id = test_case['db_id']\n",
    "        ground_truth = test_case['query']\n",
    "\n",
    "        print(\"=\" * 120)\n",
    "        print(f\"Processing Question {i + 1}/{len(test_data)}\")\n",
    "        print(f\"Database ID: {db_id}\")\n",
    "        print(\"=\" * 120)\n",
    "\n",
    "        try:\n",
    "            result = compare_csv_din(\n",
    "                ground_truth_query=ground_truth,\n",
    "                llm_query=question,\n",
    "                qnum=i,\n",
    "                db_id=db_id,\n",
    "                spider_schemas=spider_schemas,\n",
    "                value_retrieval=value_retrieval\n",
    "            )\n",
    "            processed += 1\n",
    "            if result:\n",
    "                successful += 1\n",
    "            \n",
    "            write_to_output(output_file, i, db_id, question, ground_truth, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Error processing question {i}: {error_msg}\")\n",
    "            write_to_output(output_file, i, db_id, question, ground_truth, \"Error\")\n",
    "            append_to_file(f\"Error: {error_msg}\", i)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Call process_queries with Spider path\n",
    "    process_queries(\n",
    "        spider_path=SPIDER_PATH,\n",
    "        mode='dev'  # or 'train_spider' or 'train_others'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
