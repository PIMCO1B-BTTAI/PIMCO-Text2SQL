{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restartBool = True\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import openai as OpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz import fuzz\n",
    "os.chdir(\"..\")\n",
    "from chatgpt_api import chat_prompt_revised\n",
    "print(os.getcwd())\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(\"/d/GithubRepos/PIMCO-Text2SQL\"))\n",
    "din_modules_path = os.path.join(current_dir, 'chatgpt_api')\n",
    "sys.path.append(din_modules_path)\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"OpenAI API key not configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restartBool:\n",
    "    if os.path.isfile('temp_queries.json'):\n",
    "        os.remove('temp_queries.json')\n",
    "    if os.path.isfile('din_accuracy_120.csv'):\n",
    "        os.remove('din_accuracy_120.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = os.getcwd()\n",
    "print(curr)\n",
    "output_file = os.path.join(curr, 'all_outputs')\n",
    "def append_to_file(output, qnum, filename=output_file):\n",
    "    # Check if file exists\n",
    "    output_filename= filename+str(qnum)+'.txt'\n",
    "    if not os.path.exists(output_filename):\n",
    "        with open(output_filename, 'w') as file:\n",
    "            file.write(\"Test_Din Output Log\\n\")\n",
    "            file.write(\"=\" * 80 + \"\\n\")\n",
    "    # Append the output\n",
    "    with open(output_filename, 'a') as file:\n",
    "        file.write(output + \"\\n\" + \"=\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_FILE = 'chatgpt_api/schema.json'\n",
    "print(f\"Expected schema path: {SCHEMA_FILE}\")  \n",
    "\n",
    "\n",
    "def format_schema_for_gpt(schema):\n",
    "    if not schema:\n",
    "        return \"No schema available\"\n",
    "        \n",
    "    formatted_schema = []\n",
    "    tables = schema.get('schema', {}).get('tables', [])\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table.get('name')\n",
    "        formatted_schema.append(f\"\\nTable: {table_name}\")\n",
    "        formatted_schema.append(\"Columns:\")\n",
    "        for column in table.get('columns', []):\n",
    "            col_name = column.get('name')\n",
    "            col_type = column.get('type')\n",
    "            formatted_schema.append(f\"- {col_name} ({col_type})\")\n",
    "    \n",
    "    return \"\\n\".join(formatted_schema)\n",
    "\n",
    "def load_schema_from_json(file_path: str) -> dict:\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            schema = json.load(f)\n",
    "        return schema\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            status_code=404,\n",
    "            detail=f\"Schema file not found at {file_path}\"\n",
    "        )\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise Exception(\n",
    "            status_code=500,\n",
    "            detail=f\"Error decoding JSON schema: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    db_schema = load_schema_from_json(SCHEMA_FILE)\n",
    "except Exception as e:\n",
    "    db_schema = None\n",
    "\n",
    "# schema_info = format_schema_for_gpt(db_schema)\n",
    "# print(schema_info)\n",
    "schema_info = chat_prompt_revised.schema_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_keys = {\n",
    "            'SUBMISSION': ['ACCESSION_NUMBER'],\n",
    "            'REGISTRANT': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_INFO': ['ACCESSION_NUMBER'],\n",
    "            'INTEREST_RATE_RISK': ['ACCESSION_NUMBER', 'INTEREST_RATE_RISK_ID'],\n",
    "            'BORROWER': ['ACCESSION_NUMBER', 'BORROWER_ID'],\n",
    "            'BORROW_AGGREGATE': ['ACCESSION_NUMBER', 'BORROW_AGGREGATE_ID'],\n",
    "            'MONTHLY_TOTAL_RETURN': ['ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN_ID'],\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT': ['ACCESSION_NUMBER', 'ASSET_CAT', 'INSTRUMENT_KIND'],\n",
    "            'FUND_VAR_INFO': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_HOLDING': ['ACCESSION_NUMBER', 'HOLDING_ID'],\n",
    "            'IDENTIFIERS': ['HOLDING_ID', 'IDENTIFIERS_ID'],\n",
    "            'DEBT_SECURITY': [],  \n",
    "            'DEBT_SECURITY_REF_INSTRUMENT': ['HOLDING_ID', 'DEBT_SECURITY_REF_ID'],\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY': ['HOLDING_ID', 'CONVERTIBLE_SECURITY_ID'],\n",
    "            'REPURCHASE_AGREEMENT': ['HOLDING_ID'],\n",
    "            'REPURCHASE_COUNTERPARTY': ['HOLDING_ID', 'REPURCHASE_COUNTERPARTY_ID'],\n",
    "            'REPURCHASE_COLLATERAL': ['HOLDING_ID', 'REPURCHASE_COLLATERAL_ID'],\n",
    "            'DERIVATIVE_COUNTERPARTY': ['HOLDING_ID', 'DERIVATIVE_COUNTERPARTY_ID'],\n",
    "            'SWAPTION_OPTION_WARNT_DERIV': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_BASKET': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_COMPONENT': ['HOLDING_ID', 'DESC_REF_INDEX_COMPONENT_ID'],\n",
    "            'DESC_REF_OTHER': ['HOLDING_ID', 'DESC_REF_OTHER_ID'],\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT': ['HOLDING_ID'],\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP': ['HOLDING_ID'],\n",
    "            'NONFOREIGN_EXCHANGE_SWAP': ['HOLDING_ID'],\n",
    "            'FLOATING_RATE_RESET_TENOR': ['HOLDING_ID', 'RATE_RESET_TENOR_ID'],\n",
    "            'OTHER_DERIV': ['HOLDING_ID'],\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT': ['HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT_ID'],\n",
    "            'SECURITIES_LENDING': ['HOLDING_ID'],\n",
    "            'EXPLANATORY_NOTE': ['ACCESSION_NUMBER', 'EXPLANATORY_NOTE_ID']\n",
    "        }\n",
    "\n",
    "foreign_keys = [\n",
    "            # ACCESSION_NUMBER relationships\n",
    "            'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "\n",
    "            # HOLDING_ID relationships\n",
    "            'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explore_keys():\n",
    "#     #\"\"\"Explore potential primary and foreign keys in the database\"\"\"\n",
    "#     import sqlite3\n",
    "    \n",
    "#     # Connect to database\n",
    "#     conn = sqlite3.connect('sqlite/nport.db')\n",
    "#     cursor = conn.cursor()\n",
    "\n",
    "#     # Get all tables\n",
    "#     cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#     tables = cursor.fetchall()\n",
    "\n",
    "#     print(\"Database Key Analysis:\")\n",
    "#     print(\"-\" * 80)\n",
    "\n",
    "#     # Analyze each table\n",
    "#     for table in tables:\n",
    "#         table_name = table[0]\n",
    "#         print(f\"\\nTable: {table_name}\")\n",
    "\n",
    "#         # Get column info\n",
    "#         cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "#         columns = cursor.fetchall()\n",
    "        \n",
    "#         # Get sample count for potential key columns\n",
    "#         for col in columns:\n",
    "#             col_name = col[1]\n",
    "#             # Check if column name contains potential key indicators\n",
    "#             if any(key_term in col_name.lower() for key_term in ['_id', 'accession', 'number']):\n",
    "#                 cursor.execute(f\"\"\"\n",
    "#                     SELECT COUNT(*) total_rows, \n",
    "#                            COUNT(DISTINCT {col_name}) unique_values \n",
    "#                     FROM {table_name}\n",
    "#                     WHERE {col_name} IS NOT NULL\n",
    "#                 \"\"\")\n",
    "#                 stats = cursor.fetchone()\n",
    "#                 print(f\"Column: {col_name}\")\n",
    "#                 print(f\"Total rows: {stats[0]}\")\n",
    "#                 print(f\"Unique values: {stats[1]}\")\n",
    "                \n",
    "#                 # If unique values equals total rows, likely a key\n",
    "#                 if stats[0] == stats[1] and stats[0] > 0:\n",
    "#                     print(\">>> Potential PRIMARY KEY <<<\")\n",
    "\n",
    "#         # Look for foreign key relationships\n",
    "#         for col in columns:\n",
    "#             col_name = col[1]\n",
    "#             if col_name == 'ACCESSION_NUMBER':\n",
    "#                 cursor.execute(f\"\"\"\n",
    "#                     SELECT COUNT(*) FROM {table_name} t1\n",
    "#                     WHERE EXISTS (\n",
    "#                         SELECT 1 FROM FUND_REPORTED_INFO t2 \n",
    "#                         WHERE t1.ACCESSION_NUMBER = t2.ACCESSION_NUMBER\n",
    "#                     )\n",
    "#                 \"\"\")\n",
    "#                 match_count = cursor.fetchone()[0]\n",
    "#                 if match_count > 0:\n",
    "#                     print(f\"Foreign Key: {table_name}.ACCESSION_NUMBER -> FUND_REPORTED_INFO.ACCESSION_NUMBER\")\n",
    "            \n",
    "#             elif col_name == 'HOLDING_ID':\n",
    "#                 cursor.execute(f\"\"\"\n",
    "#                     SELECT COUNT(*) FROM {table_name} t1\n",
    "#                     WHERE EXISTS (\n",
    "#                         SELECT 1 FROM FUND_REPORTED_HOLDING t2 \n",
    "#                         WHERE t1.HOLDING_ID = t2.HOLDING_ID\n",
    "#                     )\n",
    "#                 \"\"\")\n",
    "#                 match_count = cursor.fetchone()[0]\n",
    "#                 if match_count > 0:\n",
    "#                     print(f\"Foreign Key: {table_name}.HOLDING_ID -> FUND_REPORTED_HOLDING.HOLDING_ID\")\n",
    "\n",
    "#     conn.close()\n",
    "\n",
    "# # Run the analysis\n",
    "# explore_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4_generation(prompt, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\", \n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                n = 1,\n",
    "                stream = False,\n",
    "                temperature=0.0,\n",
    "                #max_tokens=600,\n",
    "                top_p = 1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "                # Removed stop=[\"Q:\"] as it cause issues\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ VALUE RETRIEVAL AND SCHEMA LINKING\n",
    "def schema_linking_prompt_maker(question):\n",
    "  instruction = \"# Find the schema_links for generating SQL queries for each question based on the database schema and Foreign keys.\\n\"\n",
    "  fields = format_schema_for_gpt(db_schema)\n",
    "  foreign_keys = \"Foreign_keys = \" + foreign_keys + '\\n'\n",
    "  prompt = instruction + chat_prompt_revised.schema_linking_prompt + fields +foreign_keys+ 'Q: \"' + question + \"\"\"\"\\nA: Let’s think step by step.\"\"\"\n",
    "  return prompt\n",
    "\n",
    "class PSLsh:\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates\n",
    "\n",
    "\n",
    "class ValueRetrieval:\n",
    "    financial_terms = {\n",
    "            'total': ['total', 'sum', 'aggregate', 'combined'],\n",
    "            'assets': ['asset', 'holdings', 'investments', 'securities'],\n",
    "            'liabilities': ['liability', 'debt', 'obligations'],\n",
    "            'net': ['net', 'pure', 'adjusted'],\n",
    "            'fund': ['fund', 'portfolio', 'investment vehicle'],\n",
    "            'return': ['return', 'yield', 'profit', 'gain'],\n",
    "            'monthly': ['monthly', 'month', 'monthly basis'],\n",
    "            'rate': ['rate', 'percentage', 'ratio'],\n",
    "            'risk': ['risk', 'exposure', 'vulnerability']\n",
    "        }\n",
    "    \n",
    "    def __init__(self, schema_path: str = 'chatgpt_api/schema.json', lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load schema\n",
    "        print(\"DEBUG: Loading schema file:\", schema_path)\n",
    "        with open(schema_path, 'r') as f:\n",
    "            self.schema = json.load(f)\n",
    "\n",
    "        # Initialize lemmatizer and stop words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Build column name index\n",
    "        self.column_index = self._build_column_index()        \n",
    "\n",
    "        # Build vectorizer and LSH for backup matching\n",
    "        self.build_vectorizer_and_lsh(seed=lsh_seed)\n",
    "        \n",
    "        # Get schema relationships\n",
    "        self.primary_keys, self.foreign_keys = self.discover_schema_relationships()\n",
    "\n",
    "    def _build_column_index(self) -> Dict:\n",
    "        column_index = {}\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        \n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                \n",
    "                # Store the full qualified name and column properties\n",
    "                qualified_name = f\"{table_name}.{column_name}\"\n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': column_name,\n",
    "                    'type': column.get('type', ''),\n",
    "                    'words': self._split_column_name(column_name),\n",
    "                    'synonyms': self._get_column_synonyms(column_name)\n",
    "                }\n",
    "                \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        # Handle  underscore + camel case.\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        synonyms = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.financial_terms:\n",
    "                synonyms.extend(self.financial_terms[word])\n",
    "                \n",
    "        return list(set(synonyms))\n",
    "\n",
    "    def build_vectorizer_and_lsh(self, seed: int):\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "        self.term_list = self.get_schema_terms()\n",
    "        self.term_vectors = self.vectorizer.fit_transform(self.term_list)\n",
    "        self.lsh = PSLsh(self.term_vectors, n_planes=10, n_tables=5)\n",
    "\n",
    "    def get_schema_terms(self) -> List[str]:\n",
    "        terms = []\n",
    "        tables = self.schema.get('schema', {}).get('tables', [])\n",
    "        for table in tables:\n",
    "            table_name = table.get('name', '').lower()\n",
    "            terms.append(table_name)\n",
    "            for column in table.get('columns', []):\n",
    "                column_name = column.get('name', '').lower()\n",
    "                terms.append(f\"{table_name}.{column_name}\")\n",
    "        return terms\n",
    "\n",
    "    def discover_schema_relationships(self):\n",
    "        # Define our primary keys and foreign keys here\n",
    "        primary_keys = {\n",
    "            'SUBMISSION': ['ACCESSION_NUMBER'],\n",
    "            'REGISTRANT': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_INFO': ['ACCESSION_NUMBER'],\n",
    "            'INTEREST_RATE_RISK': ['ACCESSION_NUMBER', 'INTEREST_RATE_RISK_ID'],\n",
    "            'BORROWER': ['ACCESSION_NUMBER', 'BORROWER_ID'],\n",
    "            'BORROW_AGGREGATE': ['ACCESSION_NUMBER', 'BORROW_AGGREGATE_ID'],\n",
    "            'MONTHLY_TOTAL_RETURN': ['ACCESSION_NUMBER', 'MONTHLY_TOTAL_RETURN_ID'],\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT': ['ACCESSION_NUMBER', 'ASSET_CAT', 'INSTRUMENT_KIND'],\n",
    "            'FUND_VAR_INFO': ['ACCESSION_NUMBER'],\n",
    "            'FUND_REPORTED_HOLDING': ['ACCESSION_NUMBER', 'HOLDING_ID'],\n",
    "            'IDENTIFIERS': ['HOLDING_ID', 'IDENTIFIERS_ID'],\n",
    "            'DEBT_SECURITY': [],  \n",
    "            'DEBT_SECURITY_REF_INSTRUMENT': ['HOLDING_ID', 'DEBT_SECURITY_REF_ID'],\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY': ['HOLDING_ID', 'CONVERTIBLE_SECURITY_ID'],\n",
    "            'REPURCHASE_AGREEMENT': ['HOLDING_ID'],\n",
    "            'REPURCHASE_COUNTERPARTY': ['HOLDING_ID', 'REPURCHASE_COUNTERPARTY_ID'],\n",
    "            'REPURCHASE_COLLATERAL': ['HOLDING_ID', 'REPURCHASE_COLLATERAL_ID'],\n",
    "            'DERIVATIVE_COUNTERPARTY': ['HOLDING_ID', 'DERIVATIVE_COUNTERPARTY_ID'],\n",
    "            'SWAPTION_OPTION_WARNT_DERIV': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_BASKET': ['HOLDING_ID'],\n",
    "            'DESC_REF_INDEX_COMPONENT': ['HOLDING_ID', 'DESC_REF_INDEX_COMPONENT_ID'],\n",
    "            'DESC_REF_OTHER': ['HOLDING_ID', 'DESC_REF_OTHER_ID'],\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT': ['HOLDING_ID'],\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP': ['HOLDING_ID'],\n",
    "            'NONFOREIGN_EXCHANGE_SWAP': ['HOLDING_ID'],\n",
    "            'FLOATING_RATE_RESET_TENOR': ['HOLDING_ID', 'RATE_RESET_TENOR_ID'],\n",
    "            'OTHER_DERIV': ['HOLDING_ID'],\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT': ['HOLDING_ID', 'OTHER_DERIV_NOTIONAL_AMOUNT_ID'],\n",
    "            'SECURITIES_LENDING': ['HOLDING_ID'],\n",
    "            'EXPLANATORY_NOTE': ['ACCESSION_NUMBER', 'EXPLANATORY_NOTE_ID']\n",
    "        }\n",
    "\n",
    "        foreign_keys = [\n",
    "            # ACCESSION_NUMBER relationships\n",
    "            'REGISTRANT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'INTEREST_RATE_RISK.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROWER.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'BORROW_AGGREGATE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_TOTAL_RETURN.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'MONTHLY_RETURN_CAT_INSTRUMENT.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_VAR_INFO.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'EXPLANATORY_NOTE.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "            'SUBMISSION.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER',\n",
    "\n",
    "            # HOLDING_ID relationships\n",
    "            'IDENTIFIERS.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DEBT_SECURITY_REF_INSTRUMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'CONVERTIBLE_SECURITY_CURRENCY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_AGREEMENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'REPURCHASE_COLLATERAL.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DERIVATIVE_COUNTERPARTY.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SWAPTION_OPTION_WARNT_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_BASKET.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_INDEX_COMPONENT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'DESC_REF_OTHER.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FUT_FWD_NONFOREIGNCUR_CONTRACT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FWD_FOREIGNCUR_CONTRACT_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'NONFOREIGN_EXCHANGE_SWAP.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'FLOATING_RATE_RESET_TENOR.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'OTHER_DERIV_NOTIONAL_AMOUNT.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID',\n",
    "            'SECURITIES_LENDING.HOLDING_ID = FUND_REPORTED_HOLDING.HOLDING_ID'\n",
    "        ]\n",
    "\n",
    "        formatted_pks = []\n",
    "        for table, keys in primary_keys.items():\n",
    "            for key in keys:\n",
    "                formatted_pks.append(f\"{table}.{key}\")\n",
    "\n",
    "        return formatted_pks, foreign_keys\n",
    "\n",
    "    def find_similar_words(self, word: str) -> List[Tuple[str, float]]:\n",
    "        #\"\"\"Better matching using multiple techniques - backup method with financial terms dictionary.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        #print(f\"\\nDEBUG: Finding matches for '{word}'\")\n",
    "        \n",
    "        matches = []\n",
    "        \n",
    "        # 1. Direct matching with column names and their components\n",
    "        for qualified_name, metadata in self.column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "                \n",
    "            # Check synonyms\n",
    "            if word in self.financial_terms.get(word, []):\n",
    "                matches.append((qualified_name, 0.9))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            # Fuzzy match with synonyms\n",
    "            for term, synonyms in self.financial_terms.items():\n",
    "                if term in metadata['words']:\n",
    "                    for synonym in synonyms:\n",
    "                        ratio = fuzz.ratio(word, synonym) / 100.0\n",
    "                        if ratio > score:\n",
    "                            score = ratio * 0.9  # Slightly lower weight for synonym matches\n",
    "            \n",
    "            if score > 0.6:  # Only include if similarity is above 60%\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # 2. LSH-based matching as backup\n",
    "        if len(matches) < 5:  # If we have fewer than 5 matches, try LSH\n",
    "            try:\n",
    "                word_vector = self.vectorizer.transform([word]).toarray()[0]\n",
    "                candidate_indices = self.lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = self.term_list[idx]\n",
    "                    if not any(term == m[0] for m in matches):  # Avoid duplicates\n",
    "                        candidate_vector = self.term_vectors[idx].toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:  # Only include if similarity is above 50%\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        # Remove duplicates keeping highest score and sort by score\n",
    "        unique_matches = {}\n",
    "        for term, score in matches:\n",
    "            if term not in unique_matches or score > unique_matches[term]:\n",
    "                unique_matches[term] = score\n",
    "        \n",
    "        matches = [(term, score) for term, score in unique_matches.items()]\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Print debug info\n",
    "        print(f\"Found {len(matches)} matches for '{word}':\")\n",
    "        for match, score in matches[:5]:\n",
    "            print(f\"  {match}: {score:.4f}\")\n",
    "        \n",
    "        return matches[:5] if matches else [('fund_reported_info.total_assets', 0.6)] if word in ['total', 'asset', 'assets'] else []\n",
    "    \n",
    "    def extract_keywords(self, question: str) -> Dict:\n",
    "        system_prompt = \"\"\"Given a financial database schema: {schema_info}\n",
    "\n",
    "        Primary Keys: {primary_keys}\n",
    "\n",
    "        Foreign Keys: {foreign_keys}\n",
    "\n",
    "        Extract from the question schema-aware components using the examples below.\"\"\"\n",
    "\n",
    "        few_shot_examples = \"\"\"\n",
    "        ```\n",
    "        Example Question: \"Show me all equity-focused funds\"\n",
    "        {\n",
    "        \"keywords\": [\"equity\", \"funds\", \"series\"],\n",
    "        \"keyphrases\": [\"equity-focused funds\"], \n",
    "        \"table_matches\": [\"FUND_REPORTED_INFO\"],\n",
    "        \"column_matches\": [\"SERIES_NAME\", \"TOTAL_ASSETS\"],\n",
    "        \"primary_keys\": [\"FUND_REPORTED_INFO.ACCESSION_NUMBER\"]\n",
    "        }\n",
    "\n",
    "        Example Question: \"Show fund holdings over 1 billion in assets\"\n",
    "        {\n",
    "        \"keywords\": [\"holdings\", \"assets\", \"funds\"],\n",
    "        \"numerical_values\": [\"1 billion\"],\n",
    "        \"table_matches\": [\"FUND_REPORTED_INFO\", \"FUND_REPORTED_HOLDING\"],\n",
    "        \"column_matches\": [\"TOTAL_ASSETS\", \"SERIES_NAME\", \"HOLDING_VALUE\"],\n",
    "        \"required_joins\": [\n",
    "            \"FUND_REPORTED_INFO to FUND_REPORTED_HOLDING via ACCESSION_NUMBER\"\n",
    "        ],\n",
    "        \"primary_keys\": [\n",
    "            \"FUND_REPORTED_INFO.ACCESSION_NUMBER\",\n",
    "            \"FUND_REPORTED_HOLDING.HOLDING_ID\"\n",
    "        ],\n",
    "        \"foreign_keys\": [\n",
    "            \"FUND_REPORTED_HOLDING.ACCESSION_NUMBER = FUND_REPORTED_INFO.ACCESSION_NUMBER\"\n",
    "        ]\n",
    "        }\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt.format(\n",
    "                    schema_info=self.schema,\n",
    "                    primary_keys=self.primary_keys,\n",
    "                    foreign_keys=self.foreign_keys\n",
    "                )},\n",
    "                {\"role\": \"user\", \"content\": few_shot_examples + f\"```\\nQuestion: {question}\\n```\"}\n",
    "            ],\n",
    "            tools=[{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"extract_components\",\n",
    "                    \"description\": \"Extract components mapping to schema\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"keyphrases\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"table_matches\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"column_matches\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"required_joins\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"primary_keys\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"foreign_keys\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"numerical_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                        },\n",
    "                        \"required\": [\"keywords\", \"table_matches\", \"column_matches\"]\n",
    "                    }\n",
    "                }\n",
    "            }],\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_components\"}}\n",
    "        )\n",
    "\n",
    "        function_call = response.choices[0].message.tool_calls[0].function\n",
    "        return json.loads(function_call.arguments)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:  # Add check for empty text\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []  # Return empty list instead of None on error\n",
    "       \n",
    "       \n",
    "    def process_schema(self, question: str) -> str:\n",
    "        # Get all the processing results\n",
    "        results = self.process_question(question)\n",
    "        \n",
    "        # Organize schema links by type\n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # 1. Get main table/column matches\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                # Only take the top match if score > 0.7\n",
    "                top_match = matches[0]  # (match, score)\n",
    "                if top_match[1] > 0.7:\n",
    "                    # Handle numerical values\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        if 'billion' in word.lower():\n",
    "                            table_columns.append(f\"{top_match[0]} > 1000000000\")\n",
    "                        elif 'million' in word.lower():\n",
    "                            table_columns.append(f\"{top_match[0]} > 1000000\")\n",
    "                        else:\n",
    "                            table_columns.append(f\"{top_match[0]} > {word}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # 2. Get relevant tables\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].upper())\n",
    "        \n",
    "        # 3. Add relevant primary keys\n",
    "        for pk in results['schema_relationships']['primary_keys']:\n",
    "            table = pk.split('.')[0]\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "        \n",
    "        # 4. Add relevant foreign keys\n",
    "        for fk in results['schema_relationships']['foreign_keys']:\n",
    "            tables_in_fk = set(part.split('.')[0] for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        print(\"Attempting to generate schema_links\")\n",
    "        counterIndex = 0\n",
    "        schema_links = None\n",
    "        while schema_links is None and counterIndex<3:\n",
    "            try:\n",
    "                schema_links = GPT4_generation(schema_linking_prompt_maker(question))\n",
    "            except:\n",
    "                print(\"Error while generating schema_link\")\n",
    "                counterIndex+=1\n",
    "        try:\n",
    "            schema_links = schema_links.split(\"Schema_links: \")[1]\n",
    "        except:\n",
    "            print(\"Slicing error for the schema_linking module\")\n",
    "            schema_links = \"[]\"\n",
    "\n",
    "        # Format output with sections\n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys,\n",
    "            \"schema_links\": schema_links\n",
    "        }\n",
    "        \n",
    "        print(\"\\nProcessed Schema Links:\")\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "        print(\"Primary Keys:\", relevant_primary_keys)\n",
    "        print(\"Foreign Keys:\", relevant_foreign_keys)\n",
    "        \n",
    "        return schema_dict\n",
    "\n",
    "\n",
    "    def process_question(self, question: str) -> Dict:\n",
    "        # Extract keywords using gpt\n",
    "        extracted_info = self.extract_keywords(question)\n",
    "\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'named_entities', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "\n",
    "        # Preprocess the words (lemmatize, remove stop words)\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "\n",
    "        # Remove duplicates\n",
    "        processed_words = list(set(processed_words))\n",
    "\n",
    "        # Find similar columns for each word\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self.find_similar_words(word)\n",
    "\n",
    "        # Combine the results\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self.primary_keys,\n",
    "                \"foreign_keys\": self.foreign_keys\n",
    "            }\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "global_vr = ValueRetrieval(schema_path='chatgpt_api/schema.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ CLASSIFICATION\n",
    "classification_prompt = '''\n",
    "```\n",
    "Q: \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\"\n",
    "schema_links: [submission.filing_date, submission.sub_type = \"NPORT-P\", submission.accession_number]\n",
    "A: Let’s think step by step. The SQL query for the question \"Find the filing date and submission number of all reports filed for an NPORT-P submission.\" needs these tables = [submission], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Get the names and CIK of registrants who are located in California.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\"]\n",
    "A: Let’s think step by step. The SQL query for the question \"Get the names and CIK of registrants who are located in California.\" needs these tables = [registrant], so we don't need JOIN.\n",
    "Plus, it doesn't require nested queries with (INTERSECT, UNION, EXCEPT, IN, NOT IN), and we need the answer to the questions = [\"\"]. \n",
    "So, we don't need JOIN and don't need nested queries, then the SQL query can be classified as \"EASY\".\n",
    "Label: \"EASY\"\n",
    "\n",
    "Q: \"Find the names and CIK of registrants in California, but only for those whose total assets are above 100 million.\"\n",
    "schema_links: [registrant.registrant_name, registrant.cik, registrant.state = \"US-CA\", fund_reported_info.total_assets > 100000000]\n",
    "A: Let's analyze this. The query involves data from two tables: \"registrant\" for registrant details and \"fund_reported_info\" for total assets. Since we need to check if total assets exceed 100 million, a nested query is necessary to filter based on this condition. This is a nested query. So, the SQL query can be classified as \"NESTED.\"\n",
    "Label: \"NESTED\"\n",
    "```\n",
    "'''\n",
    "\n",
    "def classification_prompt_maker(question, schema_dict):\n",
    "   instruction = \"\"\"```\n",
    "TASK OVERVIEW\n",
    "\n",
    "Given the database schema:\n",
    "{schema_info}\n",
    "\n",
    "Relevant Columns:\n",
    "{table_columns}\n",
    "\n",
    "Relevant Primary Keys:\n",
    "{primary_keys}\n",
    "\n",
    "Relevant Foreign Keys:\n",
    "{foreign_keys}\n",
    "\n",
    "Schema Links:\n",
    "{schema_links}\n",
    "\n",
    "- For the given question, classify it as EASY, NON-NESTED, or NESTED based on nested queries and JOIN\n",
    "- if need nested queries: predict NESTED\n",
    "- elif need JOIN and don't need nested queries: predict NON-NESTED\n",
    "- elif don't need JOIN and don't need nested queries: predict EASY\n",
    "\n",
    "Consider table relationships and what joins would be needed.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "   prompt = instruction.format(\n",
    "       schema_info=schema_info,\n",
    "       table_columns=schema_dict[\"table_columns\"],\n",
    "       primary_keys=schema_dict[\"primary_keys\"],\n",
    "       foreign_keys=schema_dict[\"foreign_keys\"],\n",
    "       schema_links=schema_dict[\"schema_links\"]\n",
    "   ) + classification_prompt + f'Q: \"{question}\"\\nschema_links: {schema_dict[\"schema_links\"]}\\nA: Let\\'s think step by step.'\n",
    "       \n",
    "   return prompt\n",
    "\n",
    "def process_question_classification(question, schema_dict):\n",
    "    def extract_classification(text):\n",
    "        print(f\"Trying to extract classification from: {text}\")\n",
    "        # Common patterns in GPT's response\n",
    "        patterns = [\n",
    "            \"Label:\", \n",
    "            \"Classification:\", \n",
    "            \"The SQL query can be classified as\",\n",
    "            \"can be classified as\"\n",
    "        ]\n",
    "        \n",
    "        text = text.upper()  # Normalize text\n",
    "        # Direct class detection\n",
    "        for class_type in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "            if class_type in text:\n",
    "                return class_type\n",
    "\n",
    "        # Try splitting with different patterns\n",
    "        for pattern in patterns:\n",
    "            if pattern.upper() in text:\n",
    "                parts = text.split(pattern.upper())\n",
    "                if len(parts) > 1:\n",
    "                    # Get the last part and clean it\n",
    "                    result = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    # Extract first word as classification\n",
    "                    classification = result.split()[0].strip()\n",
    "                    if classification in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "                        return classification\n",
    "                        \n",
    "        return \"NESTED\"  # Default fallback\n",
    "\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            print(\"Attempting classification...\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(question, schema_dict) #### ADD SCHEMA LINKS\n",
    "                }],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                temperature=0.0,\n",
    "                #max_tokens=300,\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "            print(\"Raw response:\", raw_response)\n",
    "            classification = extract_classification(raw_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "            if attempts == 3:\n",
    "                raise e\n",
    "    \n",
    "    final_class = classification if classification else \"NESTED\"\n",
    "    return f'\"{final_class}\"', raw_response\n",
    "\n",
    "############################################ SQL GENERATION\n",
    "easy_example = '''\n",
    "```\n",
    "Example with reasoning process:\n",
    "\n",
    "Q: \"Find the issuers with a balance greater than 1 million.\"\n",
    "\n",
    "schema_links: [fund_reported_holding.balance]\n",
    "\n",
    "SQL: SELECT DISTINCT issuer_name \n",
    "      FROM fund_reported_holding \n",
    "      WHERE balance > 1000000;\n",
    "```\n",
    "'''\n",
    "\n",
    "medium_example = '''\n",
    "```\n",
    "Example with reasoning process:\n",
    "\n",
    "Q: \"Find the total upfront payments and receipts for swaps with fixed rate receipts.\"\n",
    "\n",
    "schema_links: [nonforeign_exchange_swap.upfront_payment, nonforeign_exchange_swap.upfront_receipt, nonforeign_exchange_swap.fixed_rate_receipt]\n",
    "\n",
    "A: Let’s think step by step. For creating the SQL for the given question, we need to filter the swaps that have fixed rate receipts. Then, sum up the upfront payments and receipts. First, create an intermediate representation, then use it to construct the SQL query.\n",
    "Intermediate_representation: \n",
    "SELECT SUM(nonforeign_exchange_swap.upfront_payment) + SUM(nonforeign_exchange_swap.upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE nonforeign_exchange_swap.fixed_rate_receipt IS NOT NULL;\n",
    "\n",
    "SQL: \n",
    "SELECT SUM(upfront_payment) + SUM(upfront_receipt) \n",
    "FROM nonforeign_exchange_swap \n",
    "WHERE fixed_rate_receipt IS NOT NULL;\n",
    "```\n",
    "'''\n",
    "\n",
    "hard_example = '''\n",
    "```\n",
    "Example with reasoning process:\n",
    "\n",
    "Q: \"Find the borrowers with aggregate value greater than $1 million and whose interest rate change at 10-year maturity for a 100 basis point change is positive.\"\n",
    "\n",
    "schema_links: [borrower.aggregate_value, borrower.name, interest_rate_risk.intrst_rate_change_10yr_dv100]\n",
    "\n",
    "A: Let's think step by step. First, we need to filter borrowers with aggregate values greater than $1 million. Then, we need to check for interest rate changes at 10-year maturity where the change is positive. \n",
    "The SQL query for the sub-question \"What are the borrowers with aggregate value greater than $1 million and positive interest rate change at 10-year maturity for 100 basis points?\" is:\n",
    "\n",
    "Intermediate_representation: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0;\n",
    "\n",
    "SQL: \n",
    "SELECT borrower.name \n",
    "FROM borrower \n",
    "JOIN interest_rate_risk \n",
    "ON borrower.accession_number = interest_rate_risk.accession_number \n",
    "WHERE borrower.aggregate_value > 1000000 \n",
    "AND interest_rate_risk.intrst_rate_change_10yr_dv100 > 0;\n",
    "```\n",
    "'''\n",
    "\n",
    "def hard_prompt_maker(question, schema_dict, sub_questions=\"\"):\n",
    "   instruction = f\"\"\"```\n",
    "OVERALL TASK:\n",
    "I will provide a database schema, generate an SQL query that retrieves from the database the answer to this question: {question}\n",
    "You might need join statements and nested queries for this.\n",
    "```\n",
    "\"\"\"+\"\"\"```\n",
    "Relevant Columns:\n",
    "{table_columns}\n",
    "\n",
    "Relevant Primary Keys:\n",
    "{primary_keys}\n",
    "\n",
    "Relevant Foreign Keys:\n",
    "{foreign_keys}\n",
    "\n",
    "Schema Links:\n",
    "{schema_links}\n",
    "```\n",
    "\"\"\".format(\n",
    "       table_columns=schema_dict[\"table_columns\"],\n",
    "       primary_keys=schema_dict[\"primary_keys\"],\n",
    "       foreign_keys=schema_dict[\"foreign_keys\"],\n",
    "       schema_links=schema_dict[\"schema_links\"]\n",
    "   )+chat_prompt_revised.common_part_prompt\n",
    "   \n",
    "   #if sub_questions==\"\":\n",
    "       #stepping = \"A: Let's think step by step.\" # {question} can be solved by first solving a sub-question using nested queries.\n",
    "   #else:\n",
    "       #stepping = \"A: Let's think step by step.\"# {question} can be solved by first solving the answer to the following sub-question {sub_questions}.\n",
    "\n",
    "   prompt = f\"\"\"{instruction}{hard_example}\n",
    "```\n",
    "Q: \"{question}\"\n",
    "\n",
    "schema_links: {schema_dict[\"schema_links\"]}\n",
    "```\n",
    "\n",
    "A: Let's think step by step.\"\"\"\n",
    "   return prompt\n",
    "\n",
    "def medium_prompt_maker(question, schema_dict):\n",
    "   instruction = f\"\"\"```\n",
    "OVERALL TASK:\n",
    "I will provide a database schema, generate an SQL query that retrieves from the database the answer to this question: {question}\n",
    "You should not need any nested queries, but you might need join statements for this question.\n",
    "```\n",
    "\"\"\"+\"\"\"```\n",
    "Relevant Columns:\n",
    "{table_columns}\n",
    "\n",
    "Relevant Primary Keys:\n",
    "{primary_keys}\n",
    "\n",
    "Relevant Foreign Keys:\n",
    "{foreign_keys}\n",
    "\n",
    "Schema Links:\n",
    "{schema_links}\n",
    "```\n",
    "\"\"\".format(\n",
    "       table_columns=schema_dict[\"table_columns\"],\n",
    "       primary_keys=schema_dict[\"primary_keys\"],\n",
    "       foreign_keys=schema_dict[\"foreign_keys\"],\n",
    "       schema_links=schema_dict[\"schema_links\"]\n",
    "   )+chat_prompt_revised.common_part_prompt\n",
    "\n",
    "   prompt = f\"\"\"{instruction}{medium_example}\n",
    "```\n",
    "Q: \"{question}\"\n",
    "\n",
    "schema_links: {schema_dict[\"schema_links\"]}\n",
    "```\n",
    "\n",
    "A: Let's think step by step.\"\"\"\n",
    "   return prompt\n",
    "\n",
    "def easy_prompt_maker(question, schema_dict):\n",
    "   instruction = f\"\"\"```\n",
    "OVERALL TASK:\n",
    "I will provide a database schema, generate an SQL query that retrieves from the database the answer to this question: {question}\n",
    "You should not need any nested queries or join statements for this.\n",
    "```\n",
    "\"\"\"+\"\"\"```\n",
    "Relevant Columns:\n",
    "{table_columns}\n",
    "\n",
    "Relevant Primary Keys:\n",
    "{primary_keys}\n",
    "\n",
    "Relevant Foreign Keys:\n",
    "{foreign_keys}\n",
    "\n",
    "Schema Links:\n",
    "{schema_links}\n",
    "```\n",
    "\"\"\".format(\n",
    "       table_columns=schema_dict[\"table_columns\"],\n",
    "       primary_keys=schema_dict[\"primary_keys\"],\n",
    "       foreign_keys=schema_dict[\"foreign_keys\"],\n",
    "       schema_links=schema_dict[\"schema_links\"]\n",
    "   )+chat_prompt_revised.common_part_prompt\n",
    "\n",
    "   prompt = f\"\"\"{instruction}{easy_example} \n",
    "```\n",
    "Q: \"{question}\"\n",
    "\n",
    "schema_links: {schema_dict[\"schema_links\"]}\n",
    "```\n",
    "\n",
    "SQL: \"\"\" #### ADD SCHEMA LINKS\n",
    "   return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict, Any\n",
    "import time\n",
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "thought_instructions = f\"\"\"\n",
    "```\n",
    "Thought Instructions:\n",
    "```\n",
    "\n",
    "```\n",
    "Generate thoughts of increasing complexity.\n",
    "Each thought should build on the previous ones and thoughts \n",
    "should progressively cover the nuances of the problem at hand.\n",
    "```\n",
    "\n",
    "```\n",
    "First set of thoughts should be on whether a the query requires \n",
    "Common Table Expressions (CTEs) to calculate the\n",
    "results for sub queries. \n",
    "\n",
    "Prefer using Common Table Expressions rather than\n",
    "case when statements or nested subqueries.\n",
    "\n",
    "If CTEs are required then for each CTE, an analysis of the purpose of each\n",
    "CTE should be done.\n",
    "An overall structure should be outlined as to what will be calculated in \n",
    "each CTE.\n",
    "```\n",
    "\n",
    "```\n",
    "Next set of thoughts should on \n",
    "extracting out the names of as many of \n",
    "the relevant columns as possible for all CTEs and for all the sql clauses such as the \n",
    "`select`, `where` and `group_by` clauses.\n",
    "There might be additions or deletions from this list based on the \n",
    "following additional thoughts to be generated.\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Generate a thought to figure out the possible phrases in the query \n",
    "which can be used as values of the columns present in the table so as to use them \n",
    "in the `where` clause.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to compare these extracted values with the list of possible values\n",
    "of columns listed in the information for the columns so as to use the exact string\n",
    "in the `where` clause.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to reason whether `IS_TOP_TIER_ENTITY` flag is required or not.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to figure out which time period is being queried.\n",
    "If nothing is specified use `PERIOD_ID = 2023Y`.\n",
    "```\n",
    "\n",
    "```\n",
    "Generate a thought to figure out if a group_by clause is required.\n",
    "```\n",
    "\n",
    "```\n",
    "The above thoughts about \n",
    "1. phrases for values of columns\n",
    "2. query phrase to column value mapping\n",
    "3. filters such as `ASSET_CAT` and others in the where clause\n",
    "4. Period_id value to use\n",
    "5. Group by column\n",
    "\n",
    "should be generated for each of the CTE separately.\n",
    "```\n",
    "\n",
    "```\n",
    "If the input question is similar to any of the examples given above,\n",
    "then a thought should be generated to detect that and then that example \n",
    "should be followed closely to get the SQL for the input question given.\n",
    "```\n",
    "\n",
    "```\n",
    "Closing Thoughts and Observations\n",
    "```\n",
    "These should summarize:\n",
    "1. The structure of the SQL query:\n",
    "    - This states whether the query has any nested query.\n",
    "    If so, the structure of the nested query is also mentioned.\n",
    "    If not, a summary of the function of each of the select`, `where`, `group_by` etc. clauses\n",
    "    should be mentioned.\n",
    "2. An explanation of how the query solves the user question.\n",
    "\"\"\"\n",
    "\n",
    "reasoning_instructions = \"\"\"\n",
    "```\n",
    "1. Reasoning you provide should first focus on why a nested query was chosen or why it wasn't chosen.\n",
    "2. It should give a query plan on how to solve this question - explain \n",
    "the mapping of the columns to the words in the input question.\n",
    "3. It should explain each of the clauses and why they are structured the way they are structured. \n",
    "For example, if there is a `group_by`, an explanation should be given as to why it exists.\n",
    "4. If there's any sum() or any other function used it should be explained as to why it was required.\n",
    "```\n",
    "\n",
    "```\n",
    "Format the generated sql with proper indentation - the columns in the\n",
    "(`select` statement should have more indentation than keyword `select`\n",
    "and so on for each SQL clause.)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Final Output Schema\n",
    "final_output_schema_json = json.dumps({\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"user_nlp_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The original natural language query to be translated into SQL\"\n",
    "        },\n",
    "        \"reasonings\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"thought\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A thought about the user's question\"\n",
    "                    },\n",
    "                    \"helpful\": {\n",
    "                        \"type\": \"boolean\",\n",
    "                        \"description\": \"Whether the thought is helpful to solving the user's question\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"description\": \"Step-by-step reasoning process for query generation\"\n",
    "        },\n",
    "        \"generated_sql_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The final SQL query that answers the natural language question\"\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "class Thought(BaseModel):\n",
    "    \"\"\"A thought about the user's question\"\"\"\n",
    "    thought: str = Field(\n",
    "        description=\"Text of the thought\"\n",
    "    )\n",
    "    helpful: bool = Field(\n",
    "        description=\"Whether the thought is helpful to solving the user's question\"\n",
    "    )\n",
    "\n",
    "class FinalOutput(BaseModel):\n",
    "    \"\"\"Complete output structure containing the query, reasoning, and SQL\"\"\"\n",
    "    user_nlp_query: str = Field(\n",
    "        description=\"The original natural language query to be translated into SQL\"\n",
    "    )\n",
    "    reasonings: List[Thought] = Field(\n",
    "        description=\"Step-by-step reasoning process for query generation\"\n",
    "    )\n",
    "    generated_sql_query: str = Field(\n",
    "        description=\"The final SQL query that answers the natural language question\"\n",
    "    )\n",
    "\n",
    "def make_prompt(question: str, schema_dict: Dict[str, Any], complexity: str) -> str:\n",
    "    \"\"\"\n",
    "    Create prompt with appropriate instructions based on complexity\n",
    "    \"\"\"\n",
    "    example_output = {\n",
    "        \"user_nlp_query\": question,\n",
    "        \"reasonings\": [\n",
    "            {\n",
    "                \"thought\": \"First, we need to identify the main tables required\",\n",
    "                \"helpful\": True\n",
    "            },\n",
    "            {\n",
    "                \"thought\": \"Next, determine if any joins or aggregations are needed\",\n",
    "                \"helpful\": True\n",
    "            },\n",
    "            {\n",
    "                \"thought\": \"Finally, consider how to structure the WHERE clause\",\n",
    "                \"helpful\": True\n",
    "            }\n",
    "        ],\n",
    "        \"generated_sql_query\": \"SELECT column FROM table WHERE condition;\"\n",
    "    }\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "You are an expert SQL developer with deep knowledge of database querying.\n",
    "Your task is to generate a SQL query with clear reasoning steps.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "SCHEMA INFORMATION:\n",
    "{schema_dict}\n",
    "\n",
    "THOUGHT INSTRUCTIONS\n",
    "{thought_instructions}\n",
    "\n",
    "REASONING INSTRUCTIONS\n",
    "{reasoning_instructions}\n",
    "\n",
    "REQUIRED OUTPUT FORMAT:\n",
    "The response must be a valid JSON object exactly matching this schema:\n",
    "{final_output_schema_json}\n",
    "\n",
    "Example of properly formatted response:\n",
    "{json.dumps(example_output, indent=2)}\n",
    "\n",
    "REASONING REQUIREMENTS:\n",
    "1. Provide 3-5 thoughts explaining your strategy\n",
    "2. Each thought should explain WHY you're taking an approach\n",
    "3. Focus on query planning, not implementation details\n",
    "4. Consider table relationships and data types\n",
    "\n",
    "QUERY COMPLEXITY LEVEL: {complexity}\n",
    "\"\"\"\n",
    "\n",
    "    if complexity == \"EASY\":\n",
    "        base_prompt += \"\\nRESTRICTIONS: No JOINs or nested queries allowed.\"\n",
    "    elif complexity == \"NON-NESTED\":\n",
    "        base_prompt += \"\\nRESTRICTIONS: JOINs allowed but no nested queries.\"\n",
    "    else:\n",
    "        base_prompt += \"\\nRESTRICTIONS: Both JOINs and nested queries allowed if needed.\"\n",
    "        \n",
    "    base_prompt += \"\\n\\nIMPORTANT: Return only valid JSON with no additional text.\"\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "def generate_gpt_response(prompt, max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate response from OpenAI API with retries\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are an expert SQL developer. Always return responses as valid JSON matching the specified schema. Include detailed reasoning steps before generating SQL queries.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\", \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                n=1,\n",
    "                stream=False,\n",
    "                temperature=0.0,\n",
    "                top_p=1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            print(f\"Raw GPT response:\\n{content}\")  \n",
    "            return content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def validate_gpt_response(response: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that GPT response contains all required fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "        required_fields = [\"user_nlp_query\", \"reasonings\", \"generated_sql_query\"]\n",
    "        \n",
    "        # Check all required fields exist in the output\n",
    "        if not all(field in data for field in required_fields):\n",
    "            print(\"Missing required fields in response\")\n",
    "            return False\n",
    "            \n",
    "        if not isinstance(data[\"reasonings\"], list) or len(data[\"reasonings\"]) < 1:\n",
    "            print(\"Invalid reasonings array\")\n",
    "            return False\n",
    "            \n",
    "        # Check that each reasoning has required fields\n",
    "        for reason in data[\"reasonings\"]:\n",
    "            if not all(field in reason for field in [\"thought\", \"helpful\"]):\n",
    "                print(\"Invalid reasoning format\")\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Validation error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def process_question_sql(\n",
    "    question: str,\n",
    "    predicted_class: str,\n",
    "    schema_dict: Dict[str, Any],\n",
    "    max_retries: int = 3\n",
    ") -> FinalOutput:\n",
    "    \"\"\"Generate SQL with thoughts and reasoning\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Create appropriate prompt depending on classification module\n",
    "            prompt = make_prompt(\n",
    "                question=question,\n",
    "                schema_dict=schema_dict,\n",
    "                complexity=predicted_class\n",
    "            )\n",
    "            \n",
    "            # Get GPT response\n",
    "            response = generate_gpt_response(prompt)\n",
    "            if response is None:\n",
    "                continue\n",
    "                \n",
    "            # Validate format\n",
    "            if not validate_gpt_response(response):\n",
    "                print(f\"Invalid response format on attempt {attempt + 1}\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                result = json.loads(response)\n",
    "                return FinalOutput(\n",
    "                    user_nlp_query=result[\"user_nlp_query\"],\n",
    "                    reasonings=[\n",
    "                        Thought(**thought) for thought in result[\"reasonings\"]\n",
    "                    ],\n",
    "                    generated_sql_query=result[\"generated_sql_query\"]\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing response: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return FinalOutput(\n",
    "                        user_nlp_query=question,\n",
    "                        reasonings=[\n",
    "                            Thought(\n",
    "                                thought=f\"Failed to parse response: {str(e)}\",\n",
    "                                helpful=False\n",
    "                            )\n",
    "                        ],\n",
    "                        generated_sql_query=\"SELECT 1\"\n",
    "                    )\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Process error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return FinalOutput(\n",
    "                    user_nlp_query=question,\n",
    "                    reasonings=[\n",
    "                        Thought(\n",
    "                            thought=f\"Error in process: {str(e)}\",\n",
    "                            helpful=False\n",
    "                        )\n",
    "                    ],\n",
    "                    generated_sql_query=\"SELECT 1\"\n",
    "                )\n",
    "            continue\n",
    "    \n",
    "    return FinalOutput(\n",
    "        user_nlp_query=question,\n",
    "        reasonings=[\n",
    "            Thought(\n",
    "                thought=\"Maximum retries exceeded\",\n",
    "                helpful=False\n",
    "            )\n",
    "        ],\n",
    "        generated_sql_query=\"SELECT 1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ SELF CORRECTION\n",
    "def debuger(question,sql, predicted_class, schema_dict):\n",
    "\tif '\"EASY\"' in predicted_class:\n",
    "\t\tprompt_used = easy_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_dict=schema_dict\n",
    "                )\n",
    "\telif '\"NON-NESTED\"' in predicted_class:\n",
    "\t\tprompt_used = medium_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_dict=schema_dict\n",
    "                )\n",
    "\telse:\n",
    "\t\tprompt_used = hard_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_dict=schema_dict\n",
    "                )\n",
    "\n",
    "\tinstruction = \"\"\"#### For the given question, use the provided tables, columns, foreign keys, and primary keys to check if the given SQLite SQL QUERY has any issues. If there are any issues, fix them and return the fixed SQLite QUERY in the output. If there are no issues, return the SQLite SQL QUERY as is in the output.\"\n",
    "#### Background Information:\n",
    "Relevant Schema Links:\n",
    "{schema_links}\n",
    "Prompt Used to Generate the Candidate SQLite SQL Query:\n",
    "'''\n",
    "{prompt_used}\n",
    "'''\n",
    "#### Use the following instructions for fixing the SQL QUERY: \n",
    "1) Use the database values that are explicitly mentioned in the question.\n",
    "2) Pay attention to the columns that are used for the JOIN by using the Foreign_keys.\n",
    "3) Use DESC and DISTINCT only when needed.\n",
    "4) Pay attention to the columns that are used for the GROUP BY statement.\n",
    "5) Pay attention to the columns that are used for the SELECT statement.\n",
    "6) Only change the GROUP BY clause when necessary (Avoid redundant columns in GROUP BY).\n",
    "7) Use GROUP BY on one column only.\"\"\"\n",
    "\tprompt = instruction.format(\n",
    "       schema_links=schema_dict[\"schema_links\"],\n",
    "\t   prompt_used=prompt_used\n",
    "   ) + f\"\"\"\n",
    "#### Question: {question}\n",
    "#### SQLite SQL QUERY\n",
    "{sql}\n",
    "#### SQLite FIXED SQL QUERY\n",
    "\"\"\"\n",
    "\treturn prompt\n",
    "\n",
    "\n",
    "\n",
    "def GPT4_debug(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream = False,\n",
    "        temperature=0.0,\n",
    "        #max_tokens=350,\n",
    "        top_p = 1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop = [\"#\", \";\",\"\\n\\n\"]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def refine_query(question, sql, classification, schema_dict):\n",
    "\tdebugged_SQL = None\n",
    "\twhile debugged_SQL is None:\n",
    "\t\ttry:\n",
    "\t\t\tdebugged_SQL = GPT4_debug(debuger(question,sql, classification, schema_dict))\n",
    "\t\texcept:\n",
    "\t\t\ttime.sleep(3)\n",
    "\t\t\tpass\n",
    "\ttry:\n",
    "\t\treturn debugged_SQL.split('```sql', 1)[1]\n",
    "\texcept:\n",
    "\t\traise IndexError\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import io\n",
    "import csv\n",
    "def execute_sql(query: str) -> str:\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect('sqlite/nport.db')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Execute the query with a timeout\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch column names and rows\n",
    "        columns = [description[0] for description in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        # Convert the results to CSV\n",
    "        output = io.StringIO()\n",
    "        writer = csv.writer(output)\n",
    "        writer.writerow(columns)\n",
    "        writer.writerows(rows)\n",
    "        csv_data = output.getvalue()\n",
    "        output.close()\n",
    "\n",
    "        return csv_data\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {str(e)}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_csv_strings(csv_data1: str, csv_data2: str) -> bool:\n",
    "    # Use io.StringIO to read the CSV strings as file-like objects\n",
    "    csv_file1 = io.StringIO(csv_data1)\n",
    "    csv_file2 = io.StringIO(csv_data2)\n",
    "    \n",
    "    # Create CSV readers for each CSV string\n",
    "    reader1 = csv.reader(csv_file1)\n",
    "    reader2 = csv.reader(csv_file2)\n",
    "    \n",
    "    # Compare rows one by one\n",
    "    for row1, row2 in zip(reader1, reader2):\n",
    "        if row1 != row2:\n",
    "            return False  # Rows are different\n",
    "    \n",
    "    # Check if there are extra rows in either file\n",
    "    try:\n",
    "        next(reader1)\n",
    "        return False  # Extra rows in csv_data1\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        next(reader2)\n",
    "        return False  # Extra rows in csv_data2\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    return True  # CSVs are identical\n",
    "\n",
    "\n",
    "def compare_csv_din(ground_truth_query: str, llm_query: str, qnum):\n",
    "    ## let LLM stack query the database\n",
    "    append_to_file(f\"Ground Truth Query: {ground_truth_query}\", qnum)\n",
    "\n",
    "    try: \n",
    "        schema_dict = global_vr.process_schema(llm_query)\n",
    "        append_to_file(f\"Schema Links for Question: {llm_query}\\n{schema_dict}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_schema of Value Retrieval: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    try:\n",
    "        classification, class_reasoning = process_question_classification(llm_query, schema_dict)\n",
    "        append_to_file(f\"classification reasoning: {class_reasoning}\", qnum)\n",
    "        append_to_file(f\"classification: {classification}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_question_classification of Classification: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    try:\n",
    "        process_thesql = process_question_sql(llm_query, classification, schema_dict)\n",
    "        append_to_file(f\"Thoughts: {process_thesql.reasonings}\", qnum)\n",
    "        append_to_file(f\"SQL: {process_thesql.generated_sql_query}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in process_question_sql of SQL Generation: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    try:\n",
    "        final_output = refine_query(llm_query, process_thesql.generated_sql_query, classification, schema_dict)\n",
    "        append_to_file(f\"final_output: {final_output}\", qnum)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error in refine_query of Self-Correction: {e}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    try:\n",
    "        llm_csv = execute_sql(final_output.replace(\"```sql\", \"\").replace(\"```\", \"\").strip())\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error Executing LLM-Generated SQL: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "    try:\n",
    "    ## direct query to the database\n",
    "        ground_truth_csv = execute_sql(ground_truth_query)\n",
    "    except Exception as e:\n",
    "        err_string = (f\"Error Executing Ground Truth SQL: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n",
    "    ## compare results\n",
    "    \n",
    "    try:\n",
    "        diff=compare_csv_strings(ground_truth_csv,llm_csv)\n",
    "        if diff:\n",
    "            print(\"CSV outputs match perfectly.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Mismatch found.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        err_string=(f\"Error comparing CSVs: {str(e)}\")\n",
    "        print(err_string)\n",
    "        append_to_file(err_string,qnum)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Load data from query_summary.csv\n",
    "def load_queries(input_file):\n",
    "    llm_query = []\n",
    "    ground_truth_query = []\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            llm_query.append(row[\"Question\"])\n",
    "            ground_truth_query.append(row[\"SQL\"])\n",
    "    return llm_query, ground_truth_query\n",
    "\n",
    "# Save arrays to file\n",
    "def save_queries_to_file(file_path, llm_query, ground_truth_query):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump({\"llm_query\": llm_query, \"ground_truth_query\": ground_truth_query}, file)\n",
    "\n",
    "# Load arrays from file\n",
    "def load_queries_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        return data[\"llm_query\"], data[\"ground_truth_query\"]\n",
    "\n",
    "# Write comparison results to output file\n",
    "def write_to_output(file_path, ground_truth, llm, result):\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([ground_truth, llm, result])\n",
    "\n",
    "# Main script logic\n",
    "def process_queries(input_file, temp_file, output_file):\n",
    "    # If temp file doesn't exist, load queries from CSV\n",
    "    try:\n",
    "        llm_query, ground_truth_query = load_queries_from_file(temp_file)\n",
    "    except FileNotFoundError:\n",
    "        llm_query, ground_truth_query = load_queries(input_file)\n",
    "        save_queries_to_file(temp_file, llm_query, ground_truth_query)\n",
    "\n",
    "    # Prepare output file with a header if starting fresh\n",
    "    if not os.path.exists(output_file):\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Ground_Truth_Query\", \"LLM_Query\", \"Comparison_Result\"])\n",
    "    i = 0\n",
    "    # Process queries\n",
    "    while llm_query and ground_truth_query:\n",
    "        gt_query = ground_truth_query.pop(0)\n",
    "        llm_query_item = llm_query.pop(0)\n",
    "\n",
    "        # Compare queries and write results\n",
    "        try:\n",
    "            print(\"========================================================================================================\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(f\"Currently processing Question {i}\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(\"========================================================================================================\")\n",
    "            print(\"========================================================================================================\")\n",
    "            result = compare_csv_din(gt_query, llm_query_item, i)\n",
    "            append_to_file(f\"Result: {result}\", i)\n",
    "            write_to_output(output_file, gt_query, llm_query_item, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            write_to_output(output_file, gt_query, llm_query_item, \"Error\")\n",
    "\n",
    "        # Save the remaining queries back to the temp file\n",
    "        save_queries_to_file(temp_file, llm_query, ground_truth_query)\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"query_summary.csv\"\n",
    "temp_file = \"temp_queries.json\"  # Temporary file to store remaining queries\n",
    "output_file = \"din_accuracy_120.csv\"\n",
    "\n",
    "process_queries(input_file, temp_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"query_summary.csv\"\n",
    "temp_file = \"temp_queries.json\"  # Temporary file to store remaining queries\n",
    "output_file = \"din_accuracy_120.csv\"\n",
    "\n",
    "process_queries(input_file, temp_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
