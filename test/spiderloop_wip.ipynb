{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spider Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir_path = \"/Users/hannahzhang/Desktop/spider_data\"\n",
    "SCHEMA_FILE = \"/Users/hannahzhang/Desktop/spider_data/tables.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_databases(dir_path):\n",
    "    \"\"\"\n",
    "    List all SQLite database files in the Spider dataset.\n",
    "    \"\"\"\n",
    "    db_files = []\n",
    "    db_paths = []\n",
    "    for root, _, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".sqlite\"):\n",
    "                db_files.append(file)\n",
    "                db_paths.append(os.path.join(root, file))\n",
    "    return db_files, db_paths\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"\n",
    "    Connect to a specific SQLite database.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def list_tables(conn):\n",
    "    \"\"\"\n",
    "    List all tables in the connected SQLite database.\n",
    "    \"\"\"\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def preview_table(conn, table_name, limit=5):\n",
    "    \"\"\"\n",
    "    Preview data from a specific table.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    columns = [desc[0] for desc in cursor.description]  # Column names\n",
    "    rows = cursor.fetchall()  # Data rows\n",
    "    return columns, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m db_names, db_paths \u001b[38;5;241m=\u001b[39m list_databases(database_dir_path)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(\"Available Databases:\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for idx, db_name in enumerate(db_names):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     print(f\"{idx + 1}: {db_name}\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Select a database to open\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m db_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter the number of the database to open: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m db_path \u001b[38;5;241m=\u001b[39m db_paths[db_index]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Connect to the database\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "db_names, db_paths = list_databases(database_dir_path)\n",
    "# print(\"Available Databases:\")\n",
    "# for idx, db_name in enumerate(db_names):\n",
    "#     print(f\"{idx + 1}: {db_name}\")\n",
    "\n",
    "# Select a database to open\n",
    "db_index = int(input(\"\\nEnter the number of the database to open: \")) - 1\n",
    "db_path = db_paths[db_index]\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "print(f\"\\nConnected to: {db_path}\")\n",
    "\n",
    "# List tables in the database\n",
    "tables = list_tables(conn)\n",
    "print(\"# Tables: \", len(tables))\n",
    "print(\"\\nTables in the database:\")\n",
    "for idx, table in enumerate(tables):\n",
    "    print(f\"{idx + 1}: {table}\")\n",
    "\n",
    "# Select a table to preview\n",
    "table_index = int(input(\"\\nEnter the number of the table to preview: \")) - 1\n",
    "table_name = tables[table_index]\n",
    "\n",
    "# Preview the selected table\n",
    "print(f\"\\nPreviewing table: {table_name}\")\n",
    "columns, rows = preview_table(conn, table_name)\n",
    "print(\"\\nColumns:\", columns)\n",
    "print(\"\\nRows:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"\\nConnection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSLsh:\n",
    "    \"\"\"Locality-Sensitive Hashing implementation for fast approximate nearest neighbor search.\"\"\"\n",
    "    def __init__(self, vectors, n_planes=10, n_tables=5, seed: int = 42):\n",
    "        self.n_planes = n_planes\n",
    "        self.n_tables = n_tables\n",
    "        self.hash_tables = [{} for _ in range(n_tables)]\n",
    "        self.random_planes = []\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Generate random planes for each hash table\n",
    "        for _ in range(n_tables):\n",
    "            planes = np.random.randn(vectors.shape[1], n_planes)\n",
    "            self.random_planes.append(planes)\n",
    "            \n",
    "        self.num_vectors = vectors.shape[0]\n",
    "        self.vectors = vectors\n",
    "        self.build_hash_tables()\n",
    "\n",
    "    def build_hash_tables(self):\n",
    "        \"\"\"Build hash tables from input vectors.\"\"\"\n",
    "        for idx in range(self.num_vectors):\n",
    "            vector = self.vectors[idx].toarray()[0]\n",
    "            hashes = self.hash_vector(vector)\n",
    "            for i, h in enumerate(hashes):\n",
    "                if h not in self.hash_tables[i]:\n",
    "                    self.hash_tables[i][h] = []\n",
    "                self.hash_tables[i][h].append(idx)\n",
    "\n",
    "    def hash_vector(self, vector):\n",
    "        \"\"\"Generate hash codes for a vector.\"\"\"\n",
    "        hashes = []\n",
    "        for planes in self.random_planes:\n",
    "            projections = np.dot(vector, planes)\n",
    "            hash_code = ''.join(['1' if x > 0 else '0' for x in projections])\n",
    "            hashes.append(hash_code)\n",
    "        return hashes\n",
    "\n",
    "    def query(self, vector):\n",
    "        \"\"\"Find candidate nearest neighbors for a query vector.\"\"\"\n",
    "        vector = vector.toarray()[0]  # Convert sparse matrix to 1D array\n",
    "        hashes = self.hash_vector(vector)\n",
    "        candidates = set()\n",
    "        for i, h in enumerate(hashes):\n",
    "            candidates.update(self.hash_tables[i].get(h, []))\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCHEMA_FILE, 'r') as f:\n",
    "        schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderValueRetrieval:\n",
    "    def __init__(self, spider_tables_path: str = SCHEMA_FILE, lsh_seed: int = 42):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # Load all Spider schemas\n",
    "        print(\"DEBUG: Loading Spider schemas from:\", spider_tables_path)\n",
    "        with open(spider_tables_path, 'r') as f:\n",
    "            self.schemas = json.load(f)\n",
    "        \n",
    "        # Create a mapping of db_id to schema\n",
    "        self.db_schemas = {schema['db_id']: schema for schema in self.schemas}\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize database-specific components\n",
    "        self.column_indices = {}\n",
    "        self.vectorizers = {}\n",
    "        self.lsh_indices = {}\n",
    "        \n",
    "        # Build indices for each database\n",
    "        self._build_indices()\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "\n",
    "    def _get_schema_relationships(self, schema: Dict) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Extract primary and foreign keys for a specific database.\"\"\"\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        primary_keys = schema.get('primary_keys', [])\n",
    "        foreign_keys = schema.get('foreign_keys', [])\n",
    "        \n",
    "        # Format primary keys\n",
    "        formatted_pks = []\n",
    "        for pk in primary_keys:\n",
    "            table_idx, col_name = column_names[pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = table_names[table_idx]\n",
    "                formatted_pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        # Format foreign keys\n",
    "        formatted_fks = []\n",
    "        for fk in foreign_keys:\n",
    "            fk_col = column_names[fk[0]]\n",
    "            pk_col = column_names[fk[1]]\n",
    "            fk_table = table_names[fk_col[0]]\n",
    "            pk_table = table_names[pk_col[0]]\n",
    "            formatted_fks.append(\n",
    "                f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\"\n",
    "            )\n",
    "            \n",
    "        return formatted_pks, formatted_fks\n",
    "\n",
    "    def _parse_numeric_value(self, word: str) -> str:\n",
    "        \"\"\"Parse numeric values from words.\"\"\"\n",
    "        if 'billion' in word.lower():\n",
    "            return '1000000000'\n",
    "        elif 'million' in word.lower():\n",
    "            return '1000000'\n",
    "        return word\n",
    "    \n",
    "    def _build_indices(self):\n",
    "        \"\"\"Build all necessary indices for all databases.\"\"\"\n",
    "        for db_id, schema in self.db_schemas.items():\n",
    "            # Build column index\n",
    "            self.column_indices[db_id] = self._build_column_index(schema)\n",
    "            \n",
    "            # Build vectorizer and LSH\n",
    "            terms = self._get_schema_terms(schema)\n",
    "            vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), min_df=1, max_df=0.95)\n",
    "            term_vectors = vectorizer.fit_transform(terms)\n",
    "            \n",
    "            self.vectorizers[db_id] = {\n",
    "                'vectorizer': vectorizer,\n",
    "                'terms': terms\n",
    "            }\n",
    "            \n",
    "            self.lsh_indices[db_id] = {\n",
    "                'lsh': PSLsh(term_vectors, n_planes=10, n_tables=5),\n",
    "                'vectors': term_vectors\n",
    "            }\n",
    "\n",
    "    def _build_column_index(self, schema: Dict) -> Dict:\n",
    "        \"\"\"Build column index for a specific database schema.\"\"\"\n",
    "        column_index = {}\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        column_types = schema.get('column_types', [])\n",
    "        cleaned_column_names = schema.get('column_names', [])\n",
    "        \n",
    "        for (table_idx, col_name), (table_idx, clean_col_name), col_type in zip(column_names[1:], cleaned_column_names[1:], column_types[1:]):  # Skip first row (*) \n",
    "            if table_idx != -1:  # Skip table_idx == -1 which represents '*'\n",
    "                table_name = table_names[table_idx].lower()\n",
    "                qualified_name = f\"{table_name}.{col_name.lower()}\"\n",
    "                \n",
    "                column_index[qualified_name] = {\n",
    "                    'table': table_name,\n",
    "                    'column': col_name.lower(),\n",
    "                    'type': col_type,\n",
    "                    'words': clean_col_name.lower(),\n",
    "                    'synonyms': self._get_column_synonyms(col_name)\n",
    "                }\n",
    "        \n",
    "        return column_index\n",
    "\n",
    "    def _split_column_name(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Split column name into individual words.\"\"\"\n",
    "        words = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', column_name)).split()\n",
    "        words.extend(column_name.split('_'))\n",
    "        return [word.lower() for word in words if word]\n",
    "\n",
    "    def _get_column_synonyms(self, column_name: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for words in column name.\"\"\"\n",
    "        words = self._split_column_name(column_name)\n",
    "        return list(set(words))  # For Spider, we'll just use the words themselves as synonyms\n",
    "\n",
    "    def _get_schema_terms(self, schema: Dict) -> List[str]:\n",
    "        \"\"\"Get all terms from a specific database schema.\"\"\"\n",
    "        terms = []\n",
    "        table_names = schema.get('table_names_original', [])\n",
    "        column_names = schema.get('column_names_original', [])\n",
    "        \n",
    "        for idx, table in enumerate(table_names):\n",
    "            table = table.lower()\n",
    "            terms.append(table)\n",
    "            \n",
    "            # Add column terms\n",
    "            table_columns = [(t_idx, col) for t_idx, col in column_names if t_idx == idx]\n",
    "            for _, column in table_columns:\n",
    "                terms.append(f\"{table}.{column.lower()}\")\n",
    "                \n",
    "        return terms\n",
    "\n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and lemmatize input text, removing stop words.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(str(text).lower())\n",
    "            filtered_tokens = [word for word in tokens if word not in self.stop_words and word.isalnum()]\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "            return lemmatized_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error in preprocessing text '{text}': {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _find_similar_words(self, word: str, db_id: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find similar words in the database-specific schema.\"\"\"\n",
    "        if not word:\n",
    "            return []\n",
    "\n",
    "        word = word.lower()\n",
    "        matches = []\n",
    "        \n",
    "        # Direct matching with column names\n",
    "        column_index = self.column_indices[db_id]\n",
    "        for qualified_name, metadata in column_index.items():\n",
    "            score = 0.0\n",
    "            \n",
    "            # Check exact matches in column words\n",
    "            if word in metadata['words']:\n",
    "                matches.append((qualified_name, 1.0))\n",
    "                continue\n",
    "            \n",
    "            # Fuzzy match with column words\n",
    "            for col_word in metadata['words']:\n",
    "                ratio = fuzz.ratio(word, col_word) / 100.0\n",
    "                if ratio > score:\n",
    "                    score = ratio\n",
    "            \n",
    "            if score > 0.6:\n",
    "                matches.append((qualified_name, score))\n",
    "\n",
    "        # LSH-based matching as backup\n",
    "        if len(matches) < 5:\n",
    "            try:\n",
    "                vectorizer = self.vectorizers[db_id]['vectorizer']\n",
    "                terms = self.vectorizers[db_id]['terms']\n",
    "                word_vector = vectorizer.transform([word])\n",
    "                \n",
    "                lsh = self.lsh_indices[db_id]['lsh']\n",
    "                vectors = self.lsh_indices[db_id]['vectors']\n",
    "                \n",
    "                candidate_indices = lsh.query(word_vector)\n",
    "                \n",
    "                for idx in candidate_indices:\n",
    "                    term = terms[idx]\n",
    "                    if not any(term == match[0] for match in matches):\n",
    "                        candidate_vector = vectors[idx].toarray()[0]\n",
    "                        word_vector_array = word_vector.toarray()[0]\n",
    "                        dist = np.linalg.norm(word_vector_array - candidate_vector)\n",
    "                        sim = 1 / (1 + dist)\n",
    "                        if sim > 0.5:\n",
    "                            matches.append((term, sim * 0.8))\n",
    "            except Exception as e:\n",
    "                print(f\"LSH matching failed: {e}\")\n",
    "\n",
    "        matches.sort(key=lambda x: x[1], reverse=True)\n",
    "        return matches[:5]\n",
    "\n",
    "    def _extract_keywords(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Extract keywords with database-specific context.\"\"\"\n",
    "        schema = self.db_schemas[db_id]\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        system_prompt = f\"\"\"Given this database schema:\n",
    "        Tables: {schema['table_names_original']}\n",
    "        Columns: {schema['column_names_original']}\n",
    "        \n",
    "        Primary Keys: {primary_keys}\n",
    "        Foreign Keys: {foreign_keys}\n",
    "\n",
    "        Extract relevant keywords, keyphrases, and numerical values from the question in JSON format.\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "            ],\n",
    "            functions=[\n",
    "                {\n",
    "                    \"name\": \"extract_components\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"keyphrases\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                            \"numerical_values\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"]\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            function_call={\"name\": \"extract_components\"}\n",
    "        )\n",
    "\n",
    "        # Access the function_call attribute directly\n",
    "        function_call = response.choices[0].message.function_call\n",
    "        arguments = function_call.arguments\n",
    "        extracted_info = json.loads(arguments)\n",
    "\n",
    "        # Debugging statement\n",
    "        # print(\"Extracted Info:\", extracted_info)\n",
    "\n",
    "        return extracted_info\n",
    "\n",
    "\n",
    "    def process_question(self, question: str, db_id: str) -> Dict:\n",
    "        \"\"\"Process question with database context.\"\"\"\n",
    "        # Extract keywords using database-specific schema\n",
    "        extracted_info = self._extract_keywords(question, db_id)\n",
    "        \n",
    "        # Process words\n",
    "        words = []\n",
    "        for key in ['keywords', 'keyphrases', 'numerical_values']:\n",
    "            words.extend(extracted_info.get(key, []))\n",
    "        \n",
    "        # Debugging statement\n",
    "        # print(\"Words Extracted:\", words)\n",
    "\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "            processed_words.extend(self.preprocess_text(word))\n",
    "        \n",
    "        processed_words = list(set(processed_words))\n",
    "        \n",
    "        # Debugging statement\n",
    "        # print(\"Processed Words:\", processed_words)\n",
    "\n",
    "        # Find similar columns using database-specific indices\n",
    "        similar_matches = {}\n",
    "        for word in processed_words:\n",
    "            similar_matches[word] = self._find_similar_words(word, db_id)\n",
    "            # Debugging statement\n",
    "            # print(f\"Similar matches for '{word}': {similar_matches[word]}\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"extracted_info\": extracted_info,\n",
    "            \"processed_words\": processed_words,\n",
    "            \"similar_matches\": similar_matches,\n",
    "            \"schema_relationships\": {\n",
    "                \"primary_keys\": self._get_schema_relationships(self.db_schemas[db_id])[0],\n",
    "                \"foreign_keys\": self._get_schema_relationships(self.db_schemas[db_id])[1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def process_schema(self, question: str, db_id: str) -> str:\n",
    "        \"\"\"Process schema with database context.\"\"\"\n",
    "        if db_id not in self.db_schemas:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "            \n",
    "        # Get schema for this database\n",
    "        schema = self.db_schemas[db_id]\n",
    "        \n",
    "        # Get schema relationships\n",
    "        primary_keys, foreign_keys = self._get_schema_relationships(schema)\n",
    "        \n",
    "        # Process question with database context\n",
    "        results = self.process_question(question, db_id)\n",
    "        \n",
    "        table_columns = []\n",
    "        relevant_primary_keys = []\n",
    "        relevant_foreign_keys = []\n",
    "        \n",
    "        # Use database-specific indices and relationships\n",
    "        for word, matches in results['similar_matches'].items():\n",
    "            if matches:\n",
    "                top_match = matches[0]\n",
    "                if top_match[1] > 0.7:\n",
    "                    if word in results['extracted_info'].get('numerical_values', []):\n",
    "                        # Handle numerical values\n",
    "                        value = self._parse_numeric_value(word)\n",
    "                        table_columns.append(f\"{top_match[0]} > {value}\")\n",
    "                    else:\n",
    "                        table_columns.append(top_match[0])\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Table Columns:\", table_columns)\n",
    "\n",
    "        # Get relevant tables for this database\n",
    "        tables_needed = set()\n",
    "        for link in table_columns:\n",
    "            if '.' in link:\n",
    "                tables_needed.add(link.split('.')[0].lower())\n",
    "        \n",
    "        # Add relevant primary keys\n",
    "        for pk in primary_keys:\n",
    "            table = pk.split('.')[0].lower()\n",
    "            if table in tables_needed:\n",
    "                relevant_primary_keys.append(pk)\n",
    "                \n",
    "        # Add relevant foreign keys\n",
    "        for fk in foreign_keys:\n",
    "            tables_in_fk = set(part.split('.')[0].lower() for part in fk.split(' = '))\n",
    "            if tables_in_fk.intersection(tables_needed):\n",
    "                relevant_foreign_keys.append(fk)\n",
    "        \n",
    "        schema_dict = {\n",
    "            \"table_columns\": table_columns,\n",
    "            \"primary_keys\": relevant_primary_keys,\n",
    "            \"foreign_keys\": relevant_foreign_keys,\n",
    "            \"schema_links\": table_columns  # Added for DIN SQL compatibility\n",
    "        }\n",
    "        \n",
    "        # Debugging statement\n",
    "        print(\"Schema Dict:\", schema_dict)\n",
    "        \n",
    "        return str(schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def classification_prompt_maker(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Create classification prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format schema info for the prompt\n",
    "    schema_info = f\"Tables: {schema['table_names_original']}\\nColumns: {schema['column_names_original']}\"\n",
    "    \n",
    "    instruction = \"\"\"Given the database schema:\n",
    "{schema_info}\n",
    "\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "Classify the question as:\n",
    "- EASY: no JOIN and no nested queries needed\n",
    "- NON-NESTED: needs JOIN but no nested queries\n",
    "- NESTED: needs nested queries\n",
    "\n",
    "Question: \"{question}\"\n",
    "Schema Links: {schema_links}\n",
    "\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "    # Get formatted keys\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "    \n",
    "    return instruction.format(\n",
    "        schema_info=schema_info,\n",
    "        primary_keys=primary_keys,\n",
    "        foreign_keys=foreign_keys,\n",
    "        question=question,\n",
    "        schema_links=schema_links\n",
    "    )\n",
    "\n",
    "def process_question_classification(question, schema_links, db_id, spider_schemas):\n",
    "    \"\"\"Process question classification with Spider database context.\"\"\"\n",
    "    def extract_classification(text):\n",
    "        print(f\"Trying to extract classification from: {text}\")\n",
    "        text = text.upper()\n",
    "        \n",
    "        for class_type in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "            if class_type in text:\n",
    "                return class_type\n",
    "                \n",
    "        patterns = [\"LABEL:\", \"CLASSIFICATION:\", \"CAN BE CLASSIFIED AS\"]\n",
    "        for pattern in patterns:\n",
    "            if pattern in text:\n",
    "                parts = text.split(pattern)\n",
    "                if len(parts) > 1:\n",
    "                    result = parts[1].strip().strip('\"').strip(\"'\")\n",
    "                    classification = result.split()[0].strip()\n",
    "                    if classification in [\"EASY\", \"NON-NESTED\", \"NESTED\"]:\n",
    "                        return classification\n",
    "        \n",
    "        return \"NESTED\"  # Default fallback\n",
    "\n",
    "    classification = None\n",
    "    attempts = 0\n",
    "    while classification is None and attempts < 3:\n",
    "        try:\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": classification_prompt_maker(\n",
    "                        question=question,\n",
    "                        schema_links=schema_links,\n",
    "                        db_id=db_id,\n",
    "                        spider_schemas=spider_schemas\n",
    "                    )\n",
    "                }],\n",
    "                temperature=0.0,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            raw_response = response.choices[0].message.content\n",
    "            # print(\"Raw response:\", raw_response)\n",
    "            classification = extract_classification(raw_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "            attempts += 1\n",
    "    \n",
    "    final_class = classification if classification else \"NESTED\"\n",
    "    return f'\"{final_class}\"'\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        if \"SQL:\" in text:\n",
    "            return text.split(\"SQL:\")[-1].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    def make_spider_prompt(template_type):\n",
    "        schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "        if not schema:\n",
    "            raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "        # Spider-specific examples\n",
    "        examples = {\n",
    "            \"easy\": '''Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club''',\n",
    "            \n",
    "            \"medium\": '''Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id''',\n",
    "            \n",
    "            \"hard\": '''Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)'''\n",
    "        }\n",
    "\n",
    "        template = examples[template_type]\n",
    "        prompt = f\"\"\"Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "{template}\n",
    "\n",
    "Generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if '\"EASY\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"easy\")\n",
    "            elif '\"NON-NESTED\"' in predicted_class:\n",
    "                prompt = make_spider_prompt(\"medium\")\n",
    "            else:\n",
    "                prompt = make_spider_prompt(\"hard\")\n",
    "\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            sql = extract_sql(response.choices[0].message.content)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                sql = \"SELECT\"\n",
    "    \n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_question_sql(question, predicted_class, schema_links, db_id, spider_schemas, max_retries=3):\n",
    "    def extract_sql(text):\n",
    "        print(f\"\\nTrying to extract SQL from: {text}\")  # Debug print\n",
    "        if not text:\n",
    "            return \"SELECT\"\n",
    "            \n",
    "        markers = [\"SQL:\", \"Query:\", \"QUERY:\", \"SQL Query:\", \"Final SQL:\"]\n",
    "        for marker in markers:\n",
    "            if marker in text:\n",
    "                parts = text.split(marker)\n",
    "                if len(parts) > 1:\n",
    "                    sql = parts[-1].strip()\n",
    "                    print(f\"Found SQL after {marker}: {sql}\")  # Debug print\n",
    "                    return sql\n",
    "        print(\"No SQL marker found, returning full text\")  # Debug print\n",
    "        return text.strip()\n",
    "\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "\n",
    "    if '\"EASY\"' in predicted_class:\n",
    "        print(\"EASY\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(easy_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "                    \n",
    "    elif '\"NON-NESTED\"' in predicted_class:\n",
    "        print(\"NON-NESTED\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(medium_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "                    \n",
    "    else:\n",
    "        print(\"NESTED\")\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                SQL = GPT4_generation(hard_prompt_maker(\n",
    "                    question=question,\n",
    "                    schema_links=schema_links,\n",
    "                    schema=schema\n",
    "                ))\n",
    "                if SQL:\n",
    "                    SQL = extract_sql(SQL)\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    SQL = \"SELECT\"\n",
    "\n",
    "    return SQL if SQL else \"SELECT\"\n",
    "\n",
    "def GPT4_generation(prompt, max_retries=3):\n",
    "    client = OpenAI()\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\", \n",
    "                messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a database assistant that generates SQL queries based on questions about any database schema.\"},\n",
    "                    {\"role\": \"user\", \n",
    "                    \"content\": prompt}],\n",
    "                n = 1,\n",
    "                stream = False,\n",
    "                temperature=0.0,\n",
    "                max_tokens=600,\n",
    "                top_p = 1.0,\n",
    "                frequency_penalty=0.0,\n",
    "                presence_penalty=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def easy_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for easy Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"How many clubs are there?\"\n",
    "Schema_links: [club.id]\n",
    "SQL: SELECT COUNT(*) FROM club\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "SQL:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def medium_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for medium (non-nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Show the names of all teams and their leagues.\"\n",
    "Schema_links: [team.name, league.name]\n",
    "A: Let's think step by step. We need to join teams with leagues.\n",
    "SQL: SELECT team.name, league.name \n",
    "FROM team \n",
    "JOIN league ON team.league_id = league.id\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def hard_prompt_maker(question, schema_links, schema):\n",
    "    \"\"\"Create prompt for hard (nested) Spider questions.\"\"\"\n",
    "    prompt = f\"\"\"Database Schema:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "\n",
    "Example:\n",
    "Q: \"Find players who scored more goals than average.\"\n",
    "Schema_links: [player.name, player.goals]\n",
    "A: Let's think step by step:\n",
    "1. Calculate average goals\n",
    "2. Find players above average\n",
    "SQL: SELECT name FROM player \n",
    "WHERE goals > (SELECT AVG(goals) FROM player)\n",
    "\n",
    "Now generate SQL for:\n",
    "Question: {question}\n",
    "Schema_links: {schema_links}\n",
    "A: Let's think step by step.\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def debugger(question: str, sql: str, db_id: str, spider_schemas: list):\n",
    "    \"\"\"Create debug prompt with Spider database context.\"\"\"\n",
    "    # Get specific database schema\n",
    "    schema = next((s for s in spider_schemas if s['db_id'] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Unknown database ID: {db_id}\")\n",
    "    \n",
    "    # Format relationships for better context\n",
    "    def format_keys(schema):\n",
    "        pks = []\n",
    "        fks = []\n",
    "        for pk in schema.get('primary_keys', []):\n",
    "            table_idx, col_name = schema['column_names_original'][pk]\n",
    "            if table_idx != -1:\n",
    "                table_name = schema['table_names_original'][table_idx]\n",
    "                pks.append(f\"{table_name.lower()}.{col_name.lower()}\")\n",
    "        \n",
    "        for fk in schema.get('foreign_keys', []):\n",
    "            fk_col = schema['column_names_original'][fk[0]]\n",
    "            pk_col = schema['column_names_original'][fk[1]]\n",
    "            fk_table = schema['table_names_original'][fk_col[0]]\n",
    "            pk_table = schema['table_names_original'][pk_col[0]]\n",
    "            fks.append(f\"{fk_table.lower()}.{fk_col[1].lower()} = {pk_table.lower()}.{pk_col[1].lower()}\")\n",
    "        \n",
    "        return pks, fks\n",
    "\n",
    "    primary_keys, foreign_keys = format_keys(schema)\n",
    "\n",
    "    instruction = f\"\"\"#### Database Schema for {db_id}:\n",
    "Tables: {schema['table_names_original']}\n",
    "Columns: {schema['column_names_original']}\n",
    "Primary Keys: {primary_keys}\n",
    "Foreign Keys: {foreign_keys}\n",
    "\n",
    "#### For the given question, check and fix the SQL query based on these rules:\n",
    "1) Use the correct table and column names from the schema\n",
    "2) Use proper JOIN conditions based on the foreign key relationships\n",
    "3) Use DESC and DISTINCT when needed based on the question\n",
    "4) Ensure GROUP BY statements include necessary columns\n",
    "5) Verify SELECT statement columns match the question requirements\n",
    "6) Remove redundant columns from GROUP BY\n",
    "7) Use GROUP BY on one column when possible\n",
    "\n",
    "Question: {question}\n",
    "Original SQL: {sql}\n",
    "\n",
    "Return the fixed SQL query:\"\"\"\n",
    "\n",
    "    return instruction\n",
    "\n",
    "def GPT4_debug(prompt):\n",
    "    \"\"\"Debug SQL using GPT-4.\"\"\"\n",
    "    client = OpenAI()\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a SQL expert focusing on fixing and optimizing SQL queries for SQLite.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.0,\n",
    "            max_tokens=350,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=[\"#\", \";\", \"\\n\\n\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in GPT4_debug: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def refine_query(question: str, sql: str, db_id: str, spider_schemas: list) -> str:\n",
    "    \"\"\"Refine and debug the SQL query.\"\"\"\n",
    "    max_attempts = 3\n",
    "    attempt = 0\n",
    "    debugged_SQL = None\n",
    "    \n",
    "    while debugged_SQL is None and attempt < max_attempts:\n",
    "        try:\n",
    "            debug_prompt = debugger(question, sql, db_id, spider_schemas)\n",
    "            debugged_SQL = GPT4_debug(debug_prompt)\n",
    "            \n",
    "            if debugged_SQL:\n",
    "                # Clean up the response\n",
    "                debugged_SQL = debugged_SQL.replace(\"\\n\", \" \").strip()\n",
    "                \n",
    "                # Extract just the SQL if there's additional text\n",
    "                if \"sql\" in debugged_SQL.lower():\n",
    "                    parts = debugged_SQL.lower().split(\"sql\")\n",
    "                    debugged_SQL = parts[-1].strip()\n",
    "                \n",
    "                print(f\"Refined SQL (Attempt {attempt + 1}):\", debugged_SQL)\n",
    "                return debugged_SQL\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in refinement attempt {attempt + 1}: {str(e)}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        attempt += 1\n",
    "    \n",
    "    # If all attempts fail, return original SQL\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available modes:\n",
      "1. dev (1034 questions)\n",
      "2. train_spider (7000 questions)\n",
      "3. train_others (1659 questions)\n",
      "Loading dev.json...\n",
      "Loaded 1034 questions\n",
      "DEBUG: Loading Spider schemas from: /Users/hannahzhang/Desktop/spider_data/tables.json\n",
      "\n",
      "Testing 1 questions\n",
      "\n",
      "Question 1/1\n",
      "Question: How many singers do we have?\n",
      "Database: concert_singer\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m     tester\u001b[38;5;241m.\u001b[39mrun_test(num_questions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(num_questions) \u001b[38;5;28;01mif\u001b[39;00m num_questions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 118\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many questions to process? (press Enter for all): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    117\u001b[0m tester \u001b[38;5;241m=\u001b[39m SpiderTester(mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_questions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 69\u001b[0m, in \u001b[0;36mSpiderTester.run_test\u001b[0;34m(self, num_questions)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Process question\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m schema_links \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_retrieval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema Links: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_links\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Get classification\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 316\u001b[0m, in \u001b[0;36mSpiderValueRetrieval.process_schema\u001b[0;34m(self, question, db_id)\u001b[0m\n\u001b[1;32m    313\u001b[0m primary_keys, foreign_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_schema_relationships(schema)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Process question with database context\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m table_columns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    319\u001b[0m relevant_primary_keys \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[55], line 266\u001b[0m, in \u001b[0;36mSpiderValueRetrieval.process_question\u001b[0;34m(self, question, db_id)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process question with database context.\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Extract keywords using database-specific schema\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m extracted_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Process words\u001b[39;00m\n\u001b[1;32m    269\u001b[0m words \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[55], line 229\u001b[0m, in \u001b[0;36mSpiderValueRetrieval._extract_keywords\u001b[0;34m(self, question, db_id)\u001b[0m\n\u001b[1;32m    218\u001b[0m primary_keys, foreign_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_schema_relationships(schema)\n\u001b[1;32m    220\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven this database schema:\u001b[39m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124mTables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_names_original\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_names_original\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124mExtract relevant keywords, keyphrases, and numerical values from the question in JSON format.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 229\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquestion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextract_components\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeyphrases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumerical_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeywords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextract_components\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Access the function_call attribute directly\u001b[39;00m\n\u001b[1;32m    253\u001b[0m function_call \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mfunction_call\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "class SpiderTester:\n",
    "\n",
    "    def log(self, message: str):\n",
    "        \"\"\"Log message to file and console.\"\"\"\n",
    "        print(message)\n",
    "        with open(self.output_file, 'a') as f:\n",
    "            f.write(message + '\\n')\n",
    "\n",
    "    def __init__(self, spider_path: str = '/Users/hannahzhang/Desktop/spider_data', mode='dev'):\n",
    "        \"\"\"Initialize Spider tester.\n",
    "        Args:\n",
    "            spider_path: Path to Spider dataset\n",
    "            mode: Which dataset to use ('dev', 'train_spider', or 'train_others')\n",
    "        \"\"\"\n",
    "        self.spider_path = spider_path\n",
    "        \n",
    "        # Load schemas\n",
    "        with open(os.path.join(spider_path, 'tables.json'), 'r') as f:\n",
    "            self.spider_schemas = json.load(f)\n",
    "            \n",
    "        # note you must select correct data file based on mode (else it will say database not found)\n",
    "        data_files = {\n",
    "            'dev': 'dev.json',\n",
    "            'train_spider': 'train_spider.json',\n",
    "            'train_others': 'train_others.json'\n",
    "        }\n",
    "        \n",
    "        if mode not in data_files:\n",
    "            raise ValueError(f\"Mode must be one of {list(data_files.keys())}\")\n",
    "            \n",
    "        data_file = data_files[mode]\n",
    "        print(f\"Loading {data_file}...\")\n",
    "        \n",
    "        # Load questions\n",
    "        with open(os.path.join(spider_path, data_file), 'r') as f:\n",
    "            self.test_data = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.test_data)} questions\")\n",
    "            \n",
    "        # Initialize value retrieval\n",
    "        self.value_retrieval = SpiderValueRetrieval(\n",
    "            spider_tables_path=os.path.join(spider_path, 'tables.json')\n",
    "        )\n",
    "        \n",
    "        # Create output file\n",
    "        self.output_file = os.path.join(spider_path, f'results_{mode}_{time.strftime(\"%Y%m%d_%H%M%S\")}.txt')\n",
    "\n",
    "    def run_test(self, num_questions: int = None):\n",
    "        if num_questions is None:\n",
    "            num_questions = len(self.test_data)\n",
    "        else:\n",
    "            num_questions = min(num_questions, len(self.test_data))\n",
    "\n",
    "        processed = 0\n",
    "        successful = 0\n",
    "        \n",
    "        self.log(f\"\\nTesting {num_questions} questions\")\n",
    "        \n",
    "        for idx, test_case in enumerate(self.test_data[:num_questions]):\n",
    "            question = test_case['question']\n",
    "            db_id = test_case['db_id']\n",
    "            \n",
    "            self.log(f\"\\nQuestion {idx + 1}/{num_questions}\")\n",
    "            self.log(f\"Question: {question}\")\n",
    "            self.log(f\"Database: {db_id}\")\n",
    "                \n",
    "            # try:\n",
    "                # Process question\n",
    "            schema_links = self.value_retrieval.process_schema(question, db_id)\n",
    "            self.log(f\"Schema Links: {schema_links}\")\n",
    "            \n",
    "            # Get classification\n",
    "            classification = process_question_classification(\n",
    "                question=question,\n",
    "                schema_links=schema_links,\n",
    "                db_id=db_id,\n",
    "                spider_schemas=self.spider_schemas\n",
    "            )\n",
    "            self.log(f\"Classification: {classification}\")\n",
    "            \n",
    "            # Generate SQL\n",
    "            sql = process_question_sql(\n",
    "                question=question,\n",
    "                predicted_class=classification,\n",
    "                schema_links=schema_links,\n",
    "                db_id=db_id,\n",
    "                spider_schemas=self.spider_schemas\n",
    "            )\n",
    "\n",
    "            print(\"Generated sql: \", sql)\n",
    "            \n",
    "            self.log(f\"Generated SQL: {sql}\")\n",
    "            self.log(f\"Ground Truth: {test_case['query']}\")\n",
    "            \n",
    "            processed += 1\n",
    "                    \n",
    "            # except Exception as e:\n",
    "            #     self.log(f\"Error processing question: {str(e)}\")\n",
    "            #     continue\n",
    "\n",
    "        self.log(\"\\n=== Testing Summary ===\")\n",
    "        self.log(f\"Total Questions Processed: {processed}\")\n",
    "        self.log(f\"Successful: {successful}\")\n",
    "        if processed > 0:\n",
    "            self.log(f\"Success Rate: {(successful/processed)*100:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    # Choose which dataset to use HERE (dev, train_spider, or train_others)\n",
    "    print(\"Available modes:\")\n",
    "    print(\"1. dev (1034 questions)\")\n",
    "    print(\"2. train_spider (7000 questions)\")\n",
    "    print(\"3. train_others (1659 questions)\")\n",
    "    \n",
    "    mode = input(\"Choose mode (dev/train_spider/train_others) [default: dev]: \").strip() or 'dev'\n",
    "    num_questions = input(\"How many questions to process? (press Enter for all): \").strip()\n",
    "    \n",
    "    tester = SpiderTester(mode=mode)\n",
    "    tester.run_test(num_questions=int(num_questions) if num_questions else None)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
